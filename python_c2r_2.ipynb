{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation completed → C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\translated_test_c2e2r_2_微调前.py\n",
      "Translated 35 Chinese comments\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# 初始化模型和tokenizer\n",
    "device = 'cpu'\n",
    "model_name = 'utrobinmv/t5_translate_en_ru_zh_small_1024'\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 自定义术语词典（可在此处修改或扩展）\n",
    "CUSTOM_TERMS = {\n",
    "    \"写入 CSV\": \"Запись в CSV\",\n",
    "    \"CSV 表头\": \"Заголовок таблицы CSV\",\n",
    "}\n",
    "\n",
    "prefix = 'translate to ru: '\n",
    "\n",
    "def translate_text(text, target_lang='ru', term_dict=None):\n",
    "    \"\"\"添加term_dict参数接收自定义术语\"\"\"\n",
    "    # 保护路径中的特殊字符\n",
    "    preserved_paths = re.findall(r'[a-zA-Z]:\\\\[^ \\u4e00-\\u9fff]+', text)\n",
    "    for i, path in enumerate(preserved_paths):\n",
    "        text = text.replace(path, f\"||PATH_{i}||\")\n",
    "    \n",
    "    # 自定义术语替换\n",
    "    if term_dict:\n",
    "        sorted_terms = sorted(term_dict.keys(), \n",
    "                            key=lambda x: len(x), \n",
    "                            reverse=True)\n",
    "        pattern = re.compile('|'.join(map(re.escape, sorted_terms)))\n",
    "        text = pattern.sub(lambda x: term_dict[x.group()], text)\n",
    "    \n",
    "    # 执行翻译\n",
    "    src_text = prefix + text\n",
    "    input_ids = tokenizer(src_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    generated_tokens = model.generate(**input_ids.to(device))\n",
    "    result = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    # 后处理修正\n",
    "    translated = result[0]\n",
    "    \n",
    "    # 恢复被保护的路径\n",
    "    for i, path in enumerate(preserved_paths):\n",
    "        translated = translated.replace(f\"||PATH_{i}||\", path)\n",
    "    \n",
    "    # 修正 .py 前空格问题\n",
    "    translated = re.sub(\n",
    "        r'(\\b[а-яА-ЯёЁ]+)(\\.py\\b)',\n",
    "        lambda m: f\"{m.group(1)} {m.group(2)}\", \n",
    "        translated\n",
    "    )\n",
    "    \n",
    "    # 修正标点格式\n",
    "    translated = re.sub(r'(?<=[а-яА-ЯёЁ])([.,!?])(?=\\S)', r' \\1', translated)\n",
    "    \n",
    "    return translated\n",
    "\n",
    "def translate_comments_in_code(code_lines):\n",
    "    translated_lines = []\n",
    "    translated_count = 0  # 新增注释计数器\n",
    "\n",
    "    for line in code_lines:\n",
    "        # 处理纯中文注释行\n",
    "        pure_comment_match = re.match(r'^(\\s*)#\\s*([\\u4e00-\\u9fff].*)', line)\n",
    "        if pure_comment_match:\n",
    "            indent = pure_comment_match.group(1)\n",
    "            chinese_comment = pure_comment_match.group(2)\n",
    "            translated = translate_text(chinese_comment.strip(), term_dict=CUSTOM_TERMS)\n",
    "            translated_lines.append(f\"{indent}# {translated}\\n\")\n",
    "            translated_count += 1\n",
    "            continue\n",
    "\n",
    "        # 处理行尾中文注释\n",
    "        inline_comment_match = re.search(r'(\\s+#)\\s*([\\u4e00-\\u9fff][^#]*)', line)\n",
    "        if inline_comment_match:\n",
    "            code_part = line[:inline_comment_match.start()]\n",
    "            comment_symbol = inline_comment_match.group(1)\n",
    "            chinese_comment = inline_comment_match.group(2).strip()\n",
    "            translated = translate_text(chinese_comment, term_dict=CUSTOM_TERMS)\n",
    "            translated_lines.append(f\"{code_part}{comment_symbol} {translated}\\n\")\n",
    "            translated_count += 1\n",
    "            continue\n",
    "\n",
    "        # 处理多行注释\n",
    "        multi_match = re.match(r'^(\\s*)([\"\\']{3})(.*?)\\2', line, re.DOTALL)\n",
    "        if multi_match:\n",
    "            indent = multi_match.group(1)\n",
    "            quote_type = multi_match.group(2)\n",
    "            comment_content = multi_match.group(3)\n",
    "            \n",
    "            if re.search(r'[\\u4e00-\\u9fff]', comment_content):\n",
    "                translated = translate_text(comment_content, term_dict=CUSTOM_TERMS)\n",
    "                translated_lines.append(f\"{indent}{quote_type}{translated}{quote_type}\\n\")\n",
    "                translated_count += 1\n",
    "                continue\n",
    "\n",
    "        translated_lines.append(line)\n",
    "    \n",
    "    return translated_lines, translated_count  # 返回两个值\n",
    "\n",
    "def read_code_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8-sig') as file:\n",
    "        return file.readlines()\n",
    "\n",
    "def save_translated_code(file_path, translated_lines):\n",
    "    with open(file_path, 'w', encoding='utf-8-sig') as file:\n",
    "        file.writelines(translated_lines)\n",
    "\n",
    "def main(input_file_path, output_file_path):\n",
    "    code_lines = read_code_file(input_file_path)\n",
    "    translated_lines, comment_count = translate_comments_in_code(code_lines)  # 接收计数\n",
    "    save_translated_code(output_file_path, translated_lines)\n",
    "    print(f\"Translation completed → {output_file}\")\n",
    "    print(f\"Translated {comment_count} Chinese comments\")  # 新增统计输出\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = r'C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\c2e2r_2.py'\n",
    "    output_file = r'C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\translated_test_c2e2r_2_微调前.py'\n",
    "    main(input_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 微调"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 环境准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\gdnjr5233_yolo\\appdata\\roaming\\python\\python310\\site-packages (3.2.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\gdnjr5233_yolo\\appdata\\roaming\\python\\python310\\site-packages (0.2.0)\n",
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: transformers[torch] in c:\\python310\\lib\\site-packages (4.41.2)\n",
      "Requirement already satisfied: filelock in c:\\python310\\lib\\site-packages (from transformers[torch]) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in c:\\users\\gdnjr5233_yolo\\appdata\\roaming\\python\\python310\\site-packages (from transformers[torch]) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\python310\\lib\\site-packages (from transformers[torch]) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\gdnjr5233_yolo\\appdata\\roaming\\python\\python310\\site-packages (from transformers[torch]) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\python310\\lib\\site-packages (from transformers[torch]) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\python310\\lib\\site-packages (from transformers[torch]) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\python310\\lib\\site-packages (from transformers[torch]) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\python310\\lib\\site-packages (from transformers[torch]) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\python310\\lib\\site-packages (from transformers[torch]) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\gdnjr5233_yolo\\appdata\\roaming\\python\\python310\\site-packages (from transformers[torch]) (4.67.1)\n",
      "Requirement already satisfied: torch in c:\\python310\\lib\\site-packages (from transformers[torch]) (2.1.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in c:\\python310\\lib\\site-packages (from transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\gdnjr5233_yolo\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\gdnjr5233_yolo\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\python310\\lib\\site-packages (from datasets) (2.1.3)\n",
      "Requirement already satisfied: xxhash in c:\\python310\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\gdnjr5233_yolo\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in c:\\python310\\lib\\site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2023.12.1)\n",
      "Requirement already satisfied: aiohttp in c:\\python310\\lib\\site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: absl-py in c:\\python310\\lib\\site-packages (from rouge-score) (2.1.0)\n",
      "Requirement already satisfied: nltk in c:\\python310\\lib\\site-packages (from rouge-score) (3.8.1)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\python310\\lib\\site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\gdnjr5233_yolo\\appdata\\roaming\\python\\python310\\site-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\python310\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\python310\\lib\\site-packages (from aiohttp->datasets) (22.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\python310\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\python310\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\python310\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\python310\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python310\\lib\\site-packages (from requests->transformers[torch]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python310\\lib\\site-packages (from requests->transformers[torch]) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gdnjr5233_yolo\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers[torch]) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gdnjr5233_yolo\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers[torch]) (2024.8.30)\n",
      "Requirement already satisfied: sympy in c:\\python310\\lib\\site-packages (from torch->transformers[torch]) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\python310\\lib\\site-packages (from torch->transformers[torch]) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\python310\\lib\\site-packages (from torch->transformers[torch]) (3.0.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\gdnjr5233_yolo\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.27->transformers[torch]) (0.4.6)\n",
      "Requirement already satisfied: click in c:\\python310\\lib\\site-packages (from nltk->rouge-score) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\python310\\lib\\site-packages (from nltk->rouge-score) (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\python310\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python310\\lib\\site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\python310\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python310\\lib\\site-packages (from jinja2->torch->transformers[torch]) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\python310\\lib\\site-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py): started\n",
      "  Building wheel for rouge-score (setup.py): finished with status 'done'\n",
      "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24989 sha256=f0547b1e0bb0b96d7d3308c7d3472e4ca34fade8e14884b64927e8fc23fd54a1\n",
      "  Stored in directory: c:\\users\\gdnjr5233_yolo\\appdata\\local\\pip\\cache\\wheels\\5f\\dd\\89\\461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: rouge-score\n",
      "Successfully installed rouge-score-0.1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -penai (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -penai (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -penai (c:\\python310\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers[torch] datasets sentencepiece rouge-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 导入依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    TrainerCallback\n",
    ")\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['file_path', 'code', 'code_comment_type', 'comment_zh', 'comment_ru', 'comment_en', 'input_text', 'target_text'],\n",
       "        num_rows: 1258\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载数据集\n",
    "# 读取CSV并转换为Dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\datasets\\data.csv\")\n",
    "dataset = load_dataset('csv', data_files={'train': r\"C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\datasets\\data.csv\"})\n",
    "\n",
    "# 添加T5所需前缀\n",
    "def add_prefix(example):\n",
    "    example[\"input_text\"] = f\"translate to ru: {example['comment_zh']}\"\n",
    "    example[\"target_text\"] = example[\"comment_ru\"]\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(add_prefix)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null value check:\n",
      "file_path            0\n",
      "code                 0\n",
      "code_comment_type    0\n",
      "comment_zh           0\n",
      "comment_ru           0\n",
      "comment_en           0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 检查数据集中是否有空值\n",
    "print(\"Null value check:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# 检查俄语注释长度分布\n",
    "import matplotlib.pyplot as plt\n",
    "df['comment_ru'].str.len().hist()\n",
    "plt.title(\"Russian comment length distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['file_path', 'code', 'code_comment_type', 'comment_zh', 'comment_ru', 'comment_en', 'input_text', 'target_text'],\n",
       "        num_rows: 984\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['file_path', 'code', 'code_comment_type', 'comment_zh', 'comment_ru', 'comment_en', 'input_text', 'target_text'],\n",
       "        num_rows: 247\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 数据过滤\n",
    "def filter_invalid_data(example):\n",
    "    return (\n",
    "        example[\"comment_ru\"] is not None and\n",
    "        len(example[\"comment_ru\"]) > 0 and\n",
    "        len(example[\"comment_zh\"]) < 500 and\n",
    "        len(example[\"comment_ru\"]) < 500\n",
    "    )\n",
    "\n",
    "dataset = dataset.filter(filter_invalid_data, num_proc=4)\n",
    "\n",
    "# 数据集划分\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['file_path', 'code', 'code_comment_type', 'comment_zh', 'comment_ru', 'comment_en', 'input_text', 'target_text'],\n",
       "        num_rows: 1006\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['file_path', 'code', 'code_comment_type', 'comment_zh', 'comment_ru', 'comment_en', 'input_text', 'target_text'],\n",
       "        num_rows: 252\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 加载模型和分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "\n",
    "model_name = \"utrobinmv/t5_translate_en_ru_zh_small_1024\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 定义评估指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    # 处理标签中的-100\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # 计算ROUGE-L\n",
    "    scores = [scorer.score(pred, label)['rougeL'].fmeasure \n",
    "             for pred, label in zip(decoded_preds, decoded_labels)]\n",
    "    \n",
    "    return {\"rougeL\": round(np.mean(scores), 4)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. 数据预处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be80e65b63ae4fa3ab0a85946194d40c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "处理数据批次 (num_proc=4):   0%|          | 0/984 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f4d6c06f7414c4e8af8bb3ad6c95561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "处理数据批次 (num_proc=4):   0%|          | 0/247 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eb15fec797f4037b0c22be32b5bcb5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "过滤空样本 (num_proc=2):   0%|          | 0/984 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24325480c18f4b3289240100054eed2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "过滤空样本 (num_proc=2):   0%|          | 0/247 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "预处理结果统计:\n",
      "剩余训练样本: 984\n",
      "剩余验证样本: 247\n",
      "\n",
      "样本验证:\n",
      "原始输入: translate to ru: 计算调整后的 GMV，规则如下：\n",
      "预处理输入: translate to ru: 计算调整后的 GMV,规则如下:</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "预处理标签: Рассчитать скорректированный GMV, правила являются следующими:</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer\n",
    "from functools import partial\n",
    "import torch\n",
    "\n",
    "# 加载分词器\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"utrobinmv/t5_translate_en_ru_zh_small_1024\")\n",
    "\n",
    "def preprocess_function(examples, tokenizer):\n",
    "    \"\"\"增强型预处理函数，包含多重安全验证\"\"\"\n",
    "    try:\n",
    "        # 输入处理（中文）\n",
    "        model_inputs = tokenizer(\n",
    "            examples[\"input_text\"],\n",
    "            max_length=256,\n",
    "            truncation=\"only_first\",  # 优先截断前半部分\n",
    "            padding=\"max_length\",\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # 标签处理（俄语）\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(\n",
    "                text_target=examples[\"target_text\"],\n",
    "                max_length=256,\n",
    "                truncation=\"longest_first\",  # 智能截断\n",
    "                padding=\"max_length\",\n",
    "                add_special_tokens=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "        # 双重有效性验证\n",
    "        input_ids = model_inputs[\"input_ids\"].flatten().tolist()\n",
    "        label_ids = labels[\"input_ids\"].flatten().tolist()\n",
    "        \n",
    "        input_invalid = any(i >= tokenizer.vocab_size for i in input_ids)\n",
    "        label_invalid = any(i >= tokenizer.vocab_size for i in label_ids)\n",
    "        \n",
    "        # 检查特殊token\n",
    "        has_unk = tokenizer.unk_token_id in input_ids + label_ids\n",
    "        \n",
    "        if input_invalid or label_invalid or has_unk:\n",
    "            invalid_count = sum(i >= tokenizer.vocab_size for i in input_ids + label_ids)\n",
    "            print(f\"过滤样本 | 非法token数: {invalid_count} | UNK存在: {has_unk}\")\n",
    "            return {}  # 返回空字典过滤该样本\n",
    "\n",
    "        # 转换为PyTorch张量\n",
    "        model_inputs[\"input_ids\"] = model_inputs[\"input_ids\"].squeeze()\n",
    "        model_inputs[\"attention_mask\"] = model_inputs[\"attention_mask\"].squeeze()\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"].squeeze()\n",
    "        \n",
    "        return model_inputs\n",
    "    except Exception as e:\n",
    "        print(f\"预处理异常: {str(e)}\")\n",
    "        return {}\n",
    "\n",
    "# 创建绑定分词器的预处理函数\n",
    "preprocess_with_tokenizer = partial(preprocess_function, tokenizer=tokenizer)\n",
    "\n",
    "# 应用预处理\n",
    "try:\n",
    "    tokenized_datasets = dataset.map(\n",
    "        preprocess_with_tokenizer,\n",
    "        batched=True,\n",
    "        batch_size=32,\n",
    "        num_proc=4,\n",
    "        remove_columns=dataset[\"train\"].column_names,\n",
    "        load_from_cache_file=False,\n",
    "        desc=\"处理数据批次\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"多进程预处理失败: {str(e)}\")\n",
    "    print(\"尝试单进程模式...\")\n",
    "    tokenized_datasets = dataset.map(\n",
    "        preprocess_with_tokenizer,\n",
    "        batched=True,\n",
    "        batch_size=8,\n",
    "        num_proc=1,\n",
    "        desc=\"安全模式处理\"\n",
    "    )\n",
    "\n",
    "# 过滤空数据（关键步骤）\n",
    "def filter_empty_samples(example):\n",
    "    return len(example[\"input_ids\"]) > 0\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.filter(\n",
    "    filter_empty_samples,\n",
    "    num_proc=2,\n",
    "    desc=\"过滤空样本\"\n",
    ")\n",
    "\n",
    "# 数据验证\n",
    "print(\"\\n预处理结果统计:\")\n",
    "print(f\"剩余训练样本: {len(tokenized_datasets['train'])}\")\n",
    "print(f\"剩余验证样本: {len(tokenized_datasets['test'])}\")\n",
    "\n",
    "# 样本解码验证\n",
    "sample_idx = 42\n",
    "print(\"\\n样本验证:\")\n",
    "print(\"原始输入:\", dataset[\"train\"][\"input_text\"][sample_idx])\n",
    "print(\"预处理输入:\", tokenizer.decode(tokenized_datasets[\"train\"][sample_idx][\"input_ids\"]))\n",
    "print(\"预处理标签:\", tokenizer.decode(tokenized_datasets[\"train\"][sample_idx][\"labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修改预处理函数（关键修复）\n",
    "def preprocess_function(examples, tokenizer):\n",
    "    try:\n",
    "        # 输入处理\n",
    "        model_inputs = tokenizer(\n",
    "            examples[\"input_text\"],\n",
    "            max_length=256,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # 标签处理\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(\n",
    "                text_target=examples[\"target_text\"],\n",
    "                max_length=256,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                add_special_tokens=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "        \n",
    "        # 双重有效性验证\n",
    "        input_invalid = [i for i in model_inputs[\"input_ids\"].flatten() if i >= tokenizer.vocab_size]\n",
    "        label_invalid = [i for i in labels[\"input_ids\"].flatten() if i >= tokenizer.vocab_size]\n",
    "        \n",
    "        if input_invalid or label_invalid:\n",
    "            print(f\"跳过非法样本（输入非法token数: {len(input_invalid)}, 标签非法token数: {len(label_invalid)}）\")\n",
    "            print(\"示例输入:\", examples[\"input_text\"][0][:100])\n",
    "            return {}  # 返回空字典以过滤该样本\n",
    "        \n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "    except Exception as e:\n",
    "        print(f\"预处理异常: {str(e)}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9a504ed9eb5485cb80d881f6f7728e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/984 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "339eda741266483ab39aab82357c9fd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/247 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer\n",
    "from functools import partial\n",
    "\n",
    "# 1. 先加载tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"utrobinmv/t5_translate_en_ru_zh_small_1024\")\n",
    "\n",
    "# 2. 定义带tokenizer参数的预处理函数\n",
    "\n",
    "def preprocess_function(examples, tokenizer):\n",
    "    # 输入处理\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"input_text\"],\n",
    "        max_length=256,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # 标签处理\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            text_target=examples[\"target_text\"],\n",
    "            max_length=256,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "    \n",
    "    # 有效性验证\n",
    "    invalid_tokens = [tid for tid in labels[\"input_ids\"].flatten() \n",
    "                     if tid >= tokenizer.vocab_size]\n",
    "    if invalid_tokens:\n",
    "        print(f\"无效token: {invalid_tokens[:5]}\")\n",
    "        raise ValueError(\"存在非法token\")\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# 3. 创建部分函数绑定tokenizer\n",
    "preprocess_with_tokenizer = partial(\n",
    "    preprocess_function,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# 4. 应用预处理\n",
    "try:\n",
    "    tokenized_datasets = dataset.map(\n",
    "        preprocess_with_tokenizer,\n",
    "        batched=True,\n",
    "        batch_size=32,\n",
    "        num_proc=4,\n",
    "        remove_columns=dataset[\"train\"].column_names,\n",
    "        load_from_cache_file=False\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"预处理失败: {str(e)}\")\n",
    "    # 回退到单进程模式\n",
    "    print(\"尝试单进程模式...\")\n",
    "    tokenized_datasets = dataset.map(\n",
    "        preprocess_with_tokenizer,\n",
    "        batched=True,\n",
    "        batch_size=8,\n",
    "        num_proc=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预处理后数据集结构: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 984\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 247\n",
      "    })\n",
      "})\n",
      "\n",
      "样例输入ID: [21809, 19, 4, 2248, 31, 4, 338, 4606, 29, 18543]\n",
      "样例标签ID: [247, 46635, 707, 2952, 14190, 18543, 9, 12451, 182, 1]\n",
      "\n",
      "解码验证:\n",
      "输入: translate to ru: 使用具体的 API 调用\n",
      "标签: Использовать конкретные API-звонки\n"
     ]
    }
   ],
   "source": [
    "# 验证数据集结构\n",
    "print(\"预处理后数据集结构:\", tokenized_datasets)\n",
    "\n",
    "# 检查样本格式\n",
    "sample = tokenized_datasets[\"train\"][0]\n",
    "print(\"\\n样例输入ID:\", sample[\"input_ids\"][:10])\n",
    "print(\"样例标签ID:\", sample[\"labels\"][:10])\n",
    "\n",
    "# 解码验证\n",
    "input_text = tokenizer.decode(sample[\"input_ids\"], skip_special_tokens=True)\n",
    "label_text = tokenizer.decode(sample[\"labels\"], skip_special_tokens=True)\n",
    "print(f\"\\n解码验证:\\n输入: {input_text}\\n标签: {label_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8cc342c5cab4e7eb35293072f00cd66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/984 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\gdnjr5233_YOLO\\AppData\\Roaming\\Python\\Python310\\site-packages\\multiprocess\\pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"C:\\Users\\gdnjr5233_YOLO\\AppData\\Roaming\\Python\\Python310\\site-packages\\datasets\\utils\\py_utils.py\", line 678, in _write_generator_to_queue\n    for i, result in enumerate(func(**kwargs)):\n  File \"C:\\Users\\gdnjr5233_YOLO\\AppData\\Roaming\\Python\\Python310\\site-packages\\datasets\\arrow_dataset.py\", line 3476, in _map_single\n    batch = apply_function_on_filtered_inputs(\n  File \"C:\\Users\\gdnjr5233_YOLO\\AppData\\Roaming\\Python\\Python310\\site-packages\\datasets\\arrow_dataset.py\", line 3338, in apply_function_on_filtered_inputs\n    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n  File \"C:\\Users\\gdnjr5233_YOLO\\AppData\\Local\\Temp\\ipykernel_16232\\1938666360.py\", line 6, in preprocess_function\nNameError: name 'tokenizer' is not defined\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_inputs\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# 应用预处理\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m tokenized_datasets \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreprocess_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m     45\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\datasets\\dataset_dict.py:886\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    884\u001b[0m     cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[0;32m    885\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[1;32m--> 886\u001b[0m     {\n\u001b[0;32m    887\u001b[0m         k: dataset\u001b[38;5;241m.\u001b[39mmap(\n\u001b[0;32m    888\u001b[0m             function\u001b[38;5;241m=\u001b[39mfunction,\n\u001b[0;32m    889\u001b[0m             with_indices\u001b[38;5;241m=\u001b[39mwith_indices,\n\u001b[0;32m    890\u001b[0m             with_rank\u001b[38;5;241m=\u001b[39mwith_rank,\n\u001b[0;32m    891\u001b[0m             input_columns\u001b[38;5;241m=\u001b[39minput_columns,\n\u001b[0;32m    892\u001b[0m             batched\u001b[38;5;241m=\u001b[39mbatched,\n\u001b[0;32m    893\u001b[0m             batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m    894\u001b[0m             drop_last_batch\u001b[38;5;241m=\u001b[39mdrop_last_batch,\n\u001b[0;32m    895\u001b[0m             remove_columns\u001b[38;5;241m=\u001b[39mremove_columns,\n\u001b[0;32m    896\u001b[0m             keep_in_memory\u001b[38;5;241m=\u001b[39mkeep_in_memory,\n\u001b[0;32m    897\u001b[0m             load_from_cache_file\u001b[38;5;241m=\u001b[39mload_from_cache_file,\n\u001b[0;32m    898\u001b[0m             cache_file_name\u001b[38;5;241m=\u001b[39mcache_file_names[k],\n\u001b[0;32m    899\u001b[0m             writer_batch_size\u001b[38;5;241m=\u001b[39mwriter_batch_size,\n\u001b[0;32m    900\u001b[0m             features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[0;32m    901\u001b[0m             disable_nullable\u001b[38;5;241m=\u001b[39mdisable_nullable,\n\u001b[0;32m    902\u001b[0m             fn_kwargs\u001b[38;5;241m=\u001b[39mfn_kwargs,\n\u001b[0;32m    903\u001b[0m             num_proc\u001b[38;5;241m=\u001b[39mnum_proc,\n\u001b[0;32m    904\u001b[0m             desc\u001b[38;5;241m=\u001b[39mdesc,\n\u001b[0;32m    905\u001b[0m         )\n\u001b[0;32m    906\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    907\u001b[0m     }\n\u001b[0;32m    908\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\datasets\\dataset_dict.py:887\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    884\u001b[0m     cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[0;32m    885\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[0;32m    886\u001b[0m     {\n\u001b[1;32m--> 887\u001b[0m         k: \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    888\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    889\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    892\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    895\u001b[0m \u001b[43m            \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    896\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    897\u001b[0m \u001b[43m            \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    898\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_file_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    899\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    901\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    903\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    904\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    905\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    906\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    907\u001b[0m     }\n\u001b[0;32m    908\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\datasets\\arrow_dataset.py:560\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    553\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[0;32m    558\u001b[0m }\n\u001b[0;32m    559\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 560\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    561\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    562\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\datasets\\arrow_dataset.py:3165\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   3159\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpawning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_proc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m processes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3160\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[0;32m   3161\u001b[0m     unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3162\u001b[0m     total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[0;32m   3163\u001b[0m     desc\u001b[38;5;241m=\u001b[39m(desc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (num_proc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_proc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3164\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m-> 3165\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m iflatmap_unordered(\n\u001b[0;32m   3166\u001b[0m         pool, Dataset\u001b[38;5;241m.\u001b[39m_map_single, kwargs_iterable\u001b[38;5;241m=\u001b[39mkwargs_per_job\n\u001b[0;32m   3167\u001b[0m     ):\n\u001b[0;32m   3168\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m   3169\u001b[0m             shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\datasets\\utils\\py_utils.py:718\u001b[0m, in \u001b[0;36miflatmap_unordered\u001b[1;34m(pool, func, kwargs_iterable)\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    716\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pool_changed:\n\u001b[0;32m    717\u001b[0m         \u001b[38;5;66;03m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[1;32m--> 718\u001b[0m         [async_result\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m async_result \u001b[38;5;129;01min\u001b[39;00m async_results]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\datasets\\utils\\py_utils.py:718\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    716\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pool_changed:\n\u001b[0;32m    717\u001b[0m         \u001b[38;5;66;03m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[1;32m--> 718\u001b[0m         [\u001b[43masync_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m async_result \u001b[38;5;129;01min\u001b[39;00m async_results]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\multiprocess\\pool.py:774\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n\u001b[0;32m    773\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 774\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "max_input_length = 256\n",
    "max_target_length = 256\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # 输入处理\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"input_text\"],\n",
    "        max_length=max_input_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # 标签处理\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            text_target=examples[\"target_text\"],\n",
    "            max_length=max_target_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "    \n",
    "    # 有效性验证\n",
    "    invalid_tokens = [tid for tid in labels[\"input_ids\"].flatten() \n",
    "                     if tid >= tokenizer.vocab_size]\n",
    "    if invalid_tokens:\n",
    "        print(f\"无效token: {invalid_tokens[:5]}\")\n",
    "        print(\"问题样本:\", examples[\"input_text\"][0])\n",
    "        raise ValueError(\"存在非法token\")\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# 应用预处理\n",
    "tokenized_datasets = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    batch_size=32,\n",
    "    num_proc=4,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    load_from_cache_file=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. 训练配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args = Seq2SeqTrainingArguments(\n",
    "#     output_dir=\"./optimized-model\",\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     save_strategy=\"epoch\",\n",
    "#     learning_rate=1e-4,\n",
    "#     per_device_train_batch_size=4,\n",
    "#     gradient_accumulation_steps=4,\n",
    "#     num_train_epochs=3,\n",
    "#     optim=\"adafactor\",\n",
    "#     adafactor_relative_step=False,\n",
    "#     adafactor_scale_parameter=True,\n",
    "#     gradient_checkpointing=True,\n",
    "#     max_grad_norm=1.0,\n",
    "#     lr_scheduler_type=\"cosine\",\n",
    "#     warmup_ratio=0.1,\n",
    "#     weight_decay=0.005,\n",
    "#     fp16=False,\n",
    "#     logging_steps=50,\n",
    "#     dataloader_num_workers=0,\n",
    "#     no_cuda=True,\n",
    "#     load_best_model_at_end=True,\n",
    "#     metric_for_best_model=\"rougeL\",\n",
    "#     greater_is_better=True,\n",
    "#     report_to=\"none\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args = Seq2SeqTrainingArguments(\n",
    "#     output_dir=\"./t5-translate-finetuned\",\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     save_strategy=\"epoch\",\n",
    "#     learning_rate=3e-5,\n",
    "#     per_device_train_batch_size=2,\n",
    "#     per_device_eval_batch_size=2,\n",
    "#     gradient_accumulation_steps=2,\n",
    "#     num_train_epochs=5,\n",
    "#     weight_decay=0.01,\n",
    "#     predict_with_generate=True,\n",
    "#     fp16=False,\n",
    "#     logging_steps=100,\n",
    "#     load_best_model_at_end=True,\n",
    "#     metric_for_best_model=\"rougeL\",\n",
    "#     greater_is_better=True,\n",
    "#     report_to=\"none\",\n",
    "#     dataloader_num_workers=0,\n",
    "#     no_cuda=True  # 明确使用CPU\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\transformers\\training_args.py:1489: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./t5-translate-finetuned\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=4,  # 增大batch_size\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,  # 总batch_size=4*2=8\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rougeL\",\n",
    "    greater_is_better=True,\n",
    "    report_to=\"none\",\n",
    "    dataloader_num_workers=0,\n",
    "    no_cuda=True,\n",
    "    # 新增优化参数\n",
    "    gradient_checkpointing=True,  # 内存优化\n",
    "    optim=\"adafactor\"            # 更高效的优化器\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. 安全回调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SafetyCheckCallback(TrainerCallback):\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        inputs = kwargs.get(\"inputs\")\n",
    "        if inputs and \"labels\" in inputs:\n",
    "            labels = inputs[\"labels\"].cpu().numpy()\n",
    "            invalid_ids = [i for i in labels.flatten() if i >= tokenizer.vocab_size]\n",
    "            if invalid_ids:\n",
    "                print(f\"\\n[安全警报] 检测到无效token: {invalid_ids[:5]}\")\n",
    "                control.should_training_stop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. 创建Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4741566a0430429dbacc2a4cd5a3d29e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/615 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0956, 'grad_norm': 0.26741698384284973, 'learning_rate': 2.5121951219512197e-05, 'epoch': 0.81}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01b5f626efd54b8883886ce7cf41d51b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "piece id is out of range.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 11\u001b[0m\n\u001b[0;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[0;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m      3\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[SafetyCheckCallback()]\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# 开始训练\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\transformers\\trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\transformers\\trainer.py:2311\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2308\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2310\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m-> 2311\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[0;32m   2314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[0;32m   2315\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\transformers\\trainer.py:2721\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2719\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2720\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[1;32m-> 2721\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2722\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[0;32m   2724\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\transformers\\trainer_seq2seq.py:180\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mgather\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gen_kwargs \u001b[38;5;241m=\u001b[39m gen_kwargs\n\u001b[1;32m--> 180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\transformers\\trainer.py:3572\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3569\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   3571\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 3572\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3573\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3574\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3575\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[0;32m   3576\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[0;32m   3577\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   3578\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3579\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3580\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3582\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[0;32m   3583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\transformers\\trainer.py:3854\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3850\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics(\n\u001b[0;32m   3851\u001b[0m             EvalPrediction(predictions\u001b[38;5;241m=\u001b[39mall_preds, label_ids\u001b[38;5;241m=\u001b[39mall_labels, inputs\u001b[38;5;241m=\u001b[39mall_inputs)\n\u001b[0;32m   3852\u001b[0m         )\n\u001b[0;32m   3853\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3854\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEvalPrediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3855\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3856\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[1;32mIn[27], line 5\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[1;34m(eval_pred)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_metrics\u001b[39m(eval_pred):\n\u001b[0;32m      4\u001b[0m     predictions, labels \u001b[38;5;241m=\u001b[39m eval_pred\n\u001b[1;32m----> 5\u001b[0m     decoded_preds \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# 处理标签中的-100\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(labels \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m, labels, tokenizer\u001b[38;5;241m.\u001b[39mpad_token_id)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3796\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_decode\u001b[1;34m(self, sequences, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[0;32m   3772\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbatch_decode\u001b[39m(\n\u001b[0;32m   3773\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   3774\u001b[0m     sequences: Union[List[\u001b[38;5;28mint\u001b[39m], List[List[\u001b[38;5;28mint\u001b[39m]], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.ndarray\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3777\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3778\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m   3779\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3780\u001b[0m \u001b[38;5;124;03m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[0;32m   3781\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3794\u001b[0m \u001b[38;5;124;03m        `List[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[0;32m   3795\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3796\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m   3797\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(\n\u001b[0;32m   3798\u001b[0m             seq,\n\u001b[0;32m   3799\u001b[0m             skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens,\n\u001b[0;32m   3800\u001b[0m             clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[0;32m   3801\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3802\u001b[0m         )\n\u001b[0;32m   3803\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m sequences\n\u001b[0;32m   3804\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3797\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   3772\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbatch_decode\u001b[39m(\n\u001b[0;32m   3773\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   3774\u001b[0m     sequences: Union[List[\u001b[38;5;28mint\u001b[39m], List[List[\u001b[38;5;28mint\u001b[39m]], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.ndarray\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3777\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3778\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m   3779\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3780\u001b[0m \u001b[38;5;124;03m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[0;32m   3781\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3794\u001b[0m \u001b[38;5;124;03m        `List[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[0;32m   3795\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   3796\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m-> 3797\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(\n\u001b[0;32m   3798\u001b[0m             seq,\n\u001b[0;32m   3799\u001b[0m             skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens,\n\u001b[0;32m   3800\u001b[0m             clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[0;32m   3801\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3802\u001b[0m         )\n\u001b[0;32m   3803\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m sequences\n\u001b[0;32m   3804\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3836\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[1;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[0;32m   3833\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[0;32m   3834\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m to_py_obj(token_ids)\n\u001b[1;32m-> 3836\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode(\n\u001b[0;32m   3837\u001b[0m     token_ids\u001b[38;5;241m=\u001b[39mtoken_ids,\n\u001b[0;32m   3838\u001b[0m     skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens,\n\u001b[0;32m   3839\u001b[0m     clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[0;32m   3840\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3841\u001b[0m )\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\transformers\\tokenization_utils.py:1001\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._decode\u001b[1;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens, **kwargs)\u001b[0m\n\u001b[0;32m    991\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decode\u001b[39m(\n\u001b[0;32m    992\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    993\u001b[0m     token_ids: List[\u001b[38;5;28mint\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    997\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    998\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode_use_source_tokenizer \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_source_tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m-> 1001\u001b[0m     filtered_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_ids_to_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1002\u001b[0m     legacy_added_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_added_tokens_encoder\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_special_tokens) \u001b[38;5;241m|\u001b[39m {\n\u001b[0;32m   1003\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madditional_special_tokens \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(token) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[0;32m   1004\u001b[0m     }\n\u001b[0;32m   1005\u001b[0m     \u001b[38;5;66;03m# To avoid mixing byte-level and unicode for byte-level BPT\u001b[39;00m\n\u001b[0;32m   1006\u001b[0m     \u001b[38;5;66;03m# we need to build string separately for added tokens and byte-level tokens\u001b[39;00m\n\u001b[0;32m   1007\u001b[0m     \u001b[38;5;66;03m# cf. https://github.com/huggingface/transformers/issues/1133\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\transformers\\tokenization_utils.py:982\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.convert_ids_to_tokens\u001b[1;34m(self, ids, skip_special_tokens)\u001b[0m\n\u001b[0;32m    980\u001b[0m         tokens\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_added_tokens_decoder[index]\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[0;32m    981\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 982\u001b[0m         tokens\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_id_to_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:408\u001b[0m, in \u001b[0;36mT5Tokenizer._convert_id_to_token\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_convert_id_to_token\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m    407\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 408\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msp_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIdToPiece\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    409\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m token\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sentencepiece\\__init__.py:1179\u001b[0m, in \u001b[0;36m_batchnize.<locals>._batched_func\u001b[1;34m(self, arg)\u001b[0m\n\u001b[0;32m   1177\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [_func(\u001b[38;5;28mself\u001b[39m, n) \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m arg]\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1179\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sentencepiece\\__init__.py:1172\u001b[0m, in \u001b[0;36m_batchnize.<locals>._func\u001b[1;34m(v, n)\u001b[0m\n\u001b[0;32m   1170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_func\u001b[39m(v, n):\n\u001b[0;32m   1171\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(n) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mint\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mpiece_size()):\n\u001b[1;32m-> 1172\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpiece id is out of range.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   1173\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m func(v, n)\n",
      "\u001b[1;31mIndexError\u001b[0m: piece id is out of range."
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[SafetyCheckCallback()]\n",
    ")\n",
    "\n",
    "# 开始训练\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./t5-translate-finetuned-final\")\n",
    "tokenizer.save_pretrained(\"./t5-translate-finetuned-final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. 最终评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\"最终评估结果: {eval_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*5) 数据预处理*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    # 输入文本处理\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"input_text\"],\n",
    "        max_length=256,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        add_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    # 标签处理（关键修复部分）\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            text_target=examples[\"target_text\"],  # 明确指定处理目标文本\n",
    "            max_length=256,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            add_special_tokens=True\n",
    "        )\n",
    "    \n",
    "    # 验证标签有效性\n",
    "    if any(i >= tokenizer.vocab_size for i in labels[\"input_ids\"]):\n",
    "        raise ValueError(\"检测到超出词汇表的token\")\n",
    "        \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b373182785e4ef69fbdf1592eb52048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1006 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239d0456c856413199b416c3d6229890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/252 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_input_length = 256\n",
    "max_target_length = 256\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"input_text\"],\n",
    "        max_length=max_input_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples[\"target_text\"],\n",
    "            max_length=max_target_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"input_text\", \"target_text\"] + dataset[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*6) 训练配置*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\transformers\\training_args.py:1489: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "# training_args = Seq2SeqTrainingArguments(\n",
    "#     output_dir=\"./t5-translate-finetuned\",\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     learning_rate=3e-5,\n",
    "#     per_device_train_batch_size=4,\n",
    "#     per_device_eval_batch_size=4,\n",
    "#     weight_decay=0.01,\n",
    "#     save_total_limit=3,\n",
    "#     num_train_epochs=15,\n",
    "#     predict_with_generate=True,\n",
    "#     fp16=True,  # 如果使用GPU\n",
    "#     logging_steps=50,\n",
    "# )\n",
    "\n",
    "\n",
    "# training_args = Seq2SeqTrainingArguments(\n",
    "#     output_dir=\"./t5-translate-finetuned-cpu\",\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     learning_rate=3e-5,\n",
    "#     per_device_train_batch_size=2,\n",
    "#     per_device_eval_batch_size=2,\n",
    "#     gradient_accumulation_steps=2,\n",
    "#     weight_decay=0.01,\n",
    "#     save_total_limit=2,\n",
    "#     num_train_epochs=1,\n",
    "#     predict_with_generate=True,\n",
    "#     fp16=False,\n",
    "#     logging_steps=100,\n",
    "#     dataloader_num_workers=0,\n",
    "#     no_cuda=True,\n",
    "#     load_best_model_at_end=True,  # 新增：保存最佳模型\n",
    "#     metric_for_best_model=\"rougeL\",  # 根据评估指标选择最佳模型\n",
    "#     greater_is_better=True\n",
    "# )\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./t5-translate-finetuned-cpu\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",  # 必须与评估策略一致\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=2,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=5,  # 恢复原始epoch数\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,\n",
    "    logging_steps=100,\n",
    "    dataloader_num_workers=0,\n",
    "    no_cuda=True,\n",
    "    load_best_model_at_end=True,  # 保存最佳模型\n",
    "    metric_for_best_model=\"rougeL\",  # 根据评估指标选择最佳模型\n",
    "    greater_is_better=True,\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*7) 定义评估指标*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    # 替换-100为pad token id\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # 计算ROUGE-L\n",
    "    scores = [scorer.score(pred, label)['rougeL'].fmeasure \n",
    "             for pred, label in zip(decoded_preds, decoded_labels)]\n",
    "    \n",
    "    return {\"rougeL\": round(np.mean(scores), 4)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*8) 开始训练*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10bb715b487e4e07a5e2203d8416b951",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1255 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.117, 'grad_norm': 1.0339211225509644, 'learning_rate': 2.760956175298805e-05, 'epoch': 0.4}\n",
      "{'loss': 0.1091, 'grad_norm': 1.5766521692276, 'learning_rate': 2.5219123505976097e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7a0124cbf9041fa8739737581b1f7b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/126 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "piece id is out of range.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[0;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m      3\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics\n\u001b[0;32m      7\u001b[0m )\n\u001b[1;32m----> 9\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\transformers\\trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\transformers\\trainer.py:2311\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2308\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2310\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m-> 2311\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[0;32m   2314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[0;32m   2315\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\transformers\\trainer.py:2721\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2719\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2720\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[1;32m-> 2721\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2722\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[0;32m   2724\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\transformers\\trainer_seq2seq.py:180\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mgather\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gen_kwargs \u001b[38;5;241m=\u001b[39m gen_kwargs\n\u001b[1;32m--> 180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\transformers\\trainer.py:3572\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3569\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   3571\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 3572\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3573\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3574\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3575\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[0;32m   3576\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[0;32m   3577\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   3578\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3579\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3580\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3582\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[0;32m   3583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\transformers\\trainer.py:3854\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3850\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics(\n\u001b[0;32m   3851\u001b[0m             EvalPrediction(predictions\u001b[38;5;241m=\u001b[39mall_preds, label_ids\u001b[38;5;241m=\u001b[39mall_labels, inputs\u001b[38;5;241m=\u001b[39mall_inputs)\n\u001b[0;32m   3852\u001b[0m         )\n\u001b[0;32m   3853\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3854\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEvalPrediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3855\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3856\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[1;32mIn[23], line 8\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[1;34m(eval_pred)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_metrics\u001b[39m(eval_pred):\n\u001b[0;32m      7\u001b[0m     predictions, labels \u001b[38;5;241m=\u001b[39m eval_pred\n\u001b[1;32m----> 8\u001b[0m     decoded_preds \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# 替换-100为pad token id\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(labels \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m, labels, tokenizer\u001b[38;5;241m.\u001b[39mpad_token_id)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3796\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_decode\u001b[1;34m(self, sequences, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[0;32m   3772\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbatch_decode\u001b[39m(\n\u001b[0;32m   3773\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   3774\u001b[0m     sequences: Union[List[\u001b[38;5;28mint\u001b[39m], List[List[\u001b[38;5;28mint\u001b[39m]], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.ndarray\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3777\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3778\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m   3779\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3780\u001b[0m \u001b[38;5;124;03m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[0;32m   3781\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3794\u001b[0m \u001b[38;5;124;03m        `List[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[0;32m   3795\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3796\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m   3797\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(\n\u001b[0;32m   3798\u001b[0m             seq,\n\u001b[0;32m   3799\u001b[0m             skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens,\n\u001b[0;32m   3800\u001b[0m             clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[0;32m   3801\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3802\u001b[0m         )\n\u001b[0;32m   3803\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m sequences\n\u001b[0;32m   3804\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3797\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   3772\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbatch_decode\u001b[39m(\n\u001b[0;32m   3773\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   3774\u001b[0m     sequences: Union[List[\u001b[38;5;28mint\u001b[39m], List[List[\u001b[38;5;28mint\u001b[39m]], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.ndarray\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3777\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3778\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m   3779\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3780\u001b[0m \u001b[38;5;124;03m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[0;32m   3781\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3794\u001b[0m \u001b[38;5;124;03m        `List[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[0;32m   3795\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   3796\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m-> 3797\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(\n\u001b[0;32m   3798\u001b[0m             seq,\n\u001b[0;32m   3799\u001b[0m             skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens,\n\u001b[0;32m   3800\u001b[0m             clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[0;32m   3801\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3802\u001b[0m         )\n\u001b[0;32m   3803\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m sequences\n\u001b[0;32m   3804\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3836\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[1;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[0;32m   3833\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[0;32m   3834\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m to_py_obj(token_ids)\n\u001b[1;32m-> 3836\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode(\n\u001b[0;32m   3837\u001b[0m     token_ids\u001b[38;5;241m=\u001b[39mtoken_ids,\n\u001b[0;32m   3838\u001b[0m     skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens,\n\u001b[0;32m   3839\u001b[0m     clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[0;32m   3840\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3841\u001b[0m )\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\transformers\\tokenization_utils.py:1001\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._decode\u001b[1;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens, **kwargs)\u001b[0m\n\u001b[0;32m    991\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decode\u001b[39m(\n\u001b[0;32m    992\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    993\u001b[0m     token_ids: List[\u001b[38;5;28mint\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    997\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    998\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode_use_source_tokenizer \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_source_tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m-> 1001\u001b[0m     filtered_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_ids_to_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1002\u001b[0m     legacy_added_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_added_tokens_encoder\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_special_tokens) \u001b[38;5;241m|\u001b[39m {\n\u001b[0;32m   1003\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madditional_special_tokens \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(token) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[0;32m   1004\u001b[0m     }\n\u001b[0;32m   1005\u001b[0m     \u001b[38;5;66;03m# To avoid mixing byte-level and unicode for byte-level BPT\u001b[39;00m\n\u001b[0;32m   1006\u001b[0m     \u001b[38;5;66;03m# we need to build string separately for added tokens and byte-level tokens\u001b[39;00m\n\u001b[0;32m   1007\u001b[0m     \u001b[38;5;66;03m# cf. https://github.com/huggingface/transformers/issues/1133\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\transformers\\tokenization_utils.py:982\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.convert_ids_to_tokens\u001b[1;34m(self, ids, skip_special_tokens)\u001b[0m\n\u001b[0;32m    980\u001b[0m         tokens\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_added_tokens_decoder[index]\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[0;32m    981\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 982\u001b[0m         tokens\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_id_to_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:408\u001b[0m, in \u001b[0;36mT5Tokenizer._convert_id_to_token\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_convert_id_to_token\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m    407\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 408\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msp_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIdToPiece\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    409\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m token\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sentencepiece\\__init__.py:1179\u001b[0m, in \u001b[0;36m_batchnize.<locals>._batched_func\u001b[1;34m(self, arg)\u001b[0m\n\u001b[0;32m   1177\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [_func(\u001b[38;5;28mself\u001b[39m, n) \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m arg]\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1179\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sentencepiece\\__init__.py:1172\u001b[0m, in \u001b[0;36m_batchnize.<locals>._func\u001b[1;34m(v, n)\u001b[0m\n\u001b[0;32m   1170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_func\u001b[39m(v, n):\n\u001b[0;32m   1171\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(n) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mint\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mpiece_size()):\n\u001b[1;32m-> 1172\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpiece id is out of range.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   1173\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m func(v, n)\n",
      "\u001b[1;31mIndexError\u001b[0m: piece id is out of range."
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*9) 保存微调后的模型*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./t5-translate-finetuned-final\")\n",
    "tokenizer.save_pretrained(\"./t5-translate-finetuned-final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 翻译质量评估\n",
    "*1) 自动评估指标*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在测试集上评估\n",
    "test_results = trainer.evaluate(tokenized_datasets[\"test\"])\n",
    "print(f\"Final ROUGE-L: {test_results['eval_rougeL']}\")\n",
    "\n",
    "# 对比原始模型性能\n",
    "original_model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "original_trainer = Seq2SeqTrainer(\n",
    "    model=original_model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "original_results = original_trainer.evaluate(tokenized_datasets[\"test\"])\n",
    "print(f\"Original ROUGE-L: {original_results['eval_rougeL']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) 人工评估方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成对比样本\n",
    "test_samples = dataset[\"test\"].shuffle(seed=42).select(range(20))\n",
    "\n",
    "def generate_translation(model, text):\n",
    "    inputs = tokenizer(f\"translate to ru: {text}\", return_tensors=\"pt\", max_length=256, truncation=True)\n",
    "    outputs = model.generate(**inputs)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "for sample in test_samples:\n",
    "    original_translation = generate_translation(original_model, sample[\"comment_zh\"])\n",
    "    finetuned_translation = generate_translation(model, sample[\"comment_zh\"])\n",
    "    \n",
    "    print(f\"原文: {sample['comment_zh']}\")\n",
    "    print(f\"参考答案: {sample['comment_ru']}\")\n",
    "    print(f\"原始模型: {original_translation}\")\n",
    "    print(f\"微调模型: {finetuned_translation}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helsinki-NLP/opus-mt-zh-ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Helsinki-NLP/opus-mt-zh-ru is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\utils\\_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 406\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/Helsinki-NLP/opus-mt-zh-ru/resolve/main/tokenizer_config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\transformers\\utils\\hub.py:399\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    398\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 399\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:862\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[0;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[0;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[0;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[0;32m    877\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:969\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[1;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[0;32m    968\u001b[0m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[1;32m--> 969\u001b[0m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:1484\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[1;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[0;32m   1482\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[0;32m   1483\u001b[0m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[1;32m-> 1484\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1486\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:1376\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[1;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1376\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1377\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\n\u001b[0;32m   1378\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1379\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:1296\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[1;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[0;32m   1295\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[1;32m-> 1296\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1298\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1304\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1305\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:277\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[1;32m--> 277\u001b[0m     response \u001b[38;5;241m=\u001b[39m _request_wrapper(\n\u001b[0;32m    278\u001b[0m         method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    279\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    280\u001b[0m         follow_relative_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    282\u001b[0m     )\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:301\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    300\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m--> 301\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\utils\\_http.py:454\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    446\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    447\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    448\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you are authenticated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    453\u001b[0m     )\n\u001b[1;32m--> 454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(RepositoryNotFoundError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n",
      "\u001b[1;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-67b13d5e-3cfba1a251f1666050dc6d24;860b3da3-b57e-4ece-bcb6-51b3bbb828fc)\n\nRepository Not Found for url: https://huggingface.co/Helsinki-NLP/opus-mt-zh-ru/resolve/main/tokenizer_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 初始化专业中俄翻译模型\u001b[39;00m\n\u001b[0;32m      5\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHelsinki-NLP/opus-mt-zh-ru\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 6\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mMarianTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m MarianMTModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2029\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2026\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer_file\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m vocab_files:\n\u001b[0;32m   2027\u001b[0m     \u001b[38;5;66;03m# Try to get the tokenizer config to see if there are versioned tokenizer files.\u001b[39;00m\n\u001b[0;32m   2028\u001b[0m     fast_tokenizer_file \u001b[38;5;241m=\u001b[39m FULL_TOKENIZER_FILE\n\u001b[1;32m-> 2029\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2030\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2031\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2032\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2033\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2034\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2035\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2036\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2037\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2038\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2039\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2040\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2041\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   2042\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   2043\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   2044\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2045\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2046\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[0;32m   2047\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\transformers\\utils\\hub.py:422\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    418\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to have access to it at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    419\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    420\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 422\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    423\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    424\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    425\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    426\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token=<your_token>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    427\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RevisionNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    430\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    431\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    432\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    433\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: Helsinki-NLP/opus-mt-zh-ru is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# 初始化专业中俄翻译模型\n",
    "model_name = \"Helsinki-NLP/opus-mt-zh-ru\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "model.to('cpu')\n",
    "\n",
    "# 术语保护字典 (扩展版)\n",
    "term_dict = {\n",
    "    # 技术术语\n",
    "    \"CSV\": \"CSV\",\n",
    "    \"Python\": \"Python\",\n",
    "    \"UTF-8\": \"UTF-8\",\n",
    "    # 功能术语\n",
    "    \"表头\": \"заголовок таблицы\",\n",
    "    \"写入 CSV\": \"записать в CSV\",\n",
    "    \"单行注释\": \"однострочный комментарий\",\n",
    "    \"多行注释\": \"многострочный комментарий\"\n",
    "}\n",
    "\n",
    "def translate_text(text):\n",
    "    # 保护术语\n",
    "    protected_text = text\n",
    "    for cn, ru in term_dict.items():\n",
    "        protected_text = protected_text.replace(cn, f\"[[{cn}]]\")\n",
    "\n",
    "    # 执行翻译\n",
    "    inputs = tokenizer([protected_text], return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    outputs = model.generate(**inputs)\n",
    "    translated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # 恢复术语\n",
    "    for cn, ru in term_dict.items():\n",
    "        translated = translated.replace(f\"[[{cn}]]\", ru)\n",
    "\n",
    "    # 格式修正\n",
    "    translated = re.sub(r'(?<! )(\\.py\\b)', r' .py', translated)  # 修正.py前空格\n",
    "    translated = re.sub(r'(\\b[а-я]{3,})([A-Z])', r'\\1 \\2', translated)  # 分隔俄语单词和大写字母\n",
    "    \n",
    "    return translated\n",
    "\n",
    "def translate_comments_in_code(code_lines):\n",
    "    translated_lines = []\n",
    "    \n",
    "    for line in code_lines:\n",
    "        # 严格匹配纯中文注释行\n",
    "        if re.match(r'^(\\s*)#\\s*[\\u4e00-\\u9fff]+.*', line):\n",
    "            indent = re.match(r'^(\\s*)', line).group(1)\n",
    "            comment = re.sub(r'^#\\s*', '', line).strip()\n",
    "            translated = translate_text(comment)\n",
    "            translated_lines.append(f\"{indent}# {translated}\\n\")\n",
    "            continue\n",
    "        \n",
    "        # 处理行尾中文注释\n",
    "        if '#' in line and re.search(r'[\\u4e00-\\u9fff]', line):\n",
    "            parts = line.split('#', 1)\n",
    "            code_part, comment_part = parts\n",
    "            if re.search(r'[\\u4e00-\\u9fff]', comment_part):\n",
    "                translated = translate_text(comment_part.strip())\n",
    "                translated_lines.append(f\"{code_part}# {translated}\\n\")\n",
    "                continue\n",
    "        \n",
    "        # 处理多行注释\n",
    "        if '\"\"\"' in line or \"'''\" in line:\n",
    "            match = re.match(r'^(\\s*)([\"\\']{3})(.*?)\\2', line, re.DOTALL)\n",
    "            if match and re.search(r'[\\u4e00-\\u9fff]', match.group(3)):\n",
    "                indent = match.group(1)\n",
    "                quote = match.group(2)\n",
    "                content = match.group(3)\n",
    "                translated = translate_text(content)\n",
    "                translated_lines.append(f\"{indent}{quote}{translated}{quote}\\n\")\n",
    "                continue\n",
    "        \n",
    "        translated_lines.append(line)\n",
    "    \n",
    "    return translated_lines\n",
    "\n",
    "def read_code_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.readlines()\n",
    "\n",
    "def save_translated_code(file_path, translated_lines):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.writelines(translated_lines)\n",
    "\n",
    "def main(input_file_path, output_file_path):\n",
    "    code_lines = read_code_file(input_file_path)\n",
    "    translated_lines = translate_comments_in_code(code_lines)\n",
    "    save_translated_code(output_file_path, translated_lines)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = r'C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\c2e2r.py'\n",
    "    output_file = r'C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\translated_test_final_2.py'\n",
    "    main(input_file, output_file)\n",
    "    print(f\"翻译完成 → {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# 初始化模型和tokenizer\n",
    "device = 'cpu'\n",
    "model_name = 'utrobinmv/t5_translate_en_ru_zh_small_1024'\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 自定义术语词典（可在此处修改或扩展）\n",
    "CUSTOM_TERMS = {\n",
    "    \"写入 CSV\": \"Запись в CSV\",\n",
    "    \"CSV 表头\": \"Заголовок таблицы CSV\",\n",
    "    }\n",
    "\n",
    "prefix = 'translate to ru: '\n",
    "\n",
    "def translate_text(text, target_lang='ru', term_dict=None):\n",
    "    \"\"\"添加term_dict参数接收自定义术语\"\"\"\n",
    "    # 保护路径中的特殊字符\n",
    "    preserved_paths = re.findall(r'[a-zA-Z]:\\\\[^ \\u4e00-\\u9fff]+', text)\n",
    "    for i, path in enumerate(preserved_paths):\n",
    "        text = text.replace(path, f\"||PATH_{i}||\")\n",
    "    \n",
    "    # 自定义术语替换（新增部分）\n",
    "    if term_dict:\n",
    "        # 按术语长度降序排序避免部分匹配\n",
    "        sorted_terms = sorted(term_dict.keys(), \n",
    "                            key=lambda x: len(x), \n",
    "                            reverse=True)\n",
    "        pattern = re.compile('|'.join(map(re.escape, sorted_terms)))\n",
    "        # 直接替换为对应的俄语术语\n",
    "        text = pattern.sub(lambda x: term_dict[x.group()], text)\n",
    "    \n",
    "    # 执行翻译\n",
    "    src_text = prefix + text\n",
    "    input_ids = tokenizer(src_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    generated_tokens = model.generate(**input_ids.to(device))\n",
    "    result = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    # 后处理修正\n",
    "    translated = result[0]\n",
    "    \n",
    "    # 恢复被保护的路径\n",
    "    for i, path in enumerate(preserved_paths):\n",
    "        translated = translated.replace(f\"||PATH_{i}||\", path)\n",
    "    \n",
    "    # 修正 .py 前空格问题\n",
    "    translated = re.sub(\n",
    "        r'(\\b[а-яА-ЯёЁ]+)(\\.py\\b)',\n",
    "        lambda m: f\"{m.group(1)} {m.group(2)}\", \n",
    "        translated\n",
    "    )\n",
    "    \n",
    "    # 修正标点格式\n",
    "    translated = re.sub(r'(?<=[а-яА-ЯёЁ])([.,!?])(?=\\S)', r' \\1', translated)\n",
    "    \n",
    "    return translated\n",
    "\n",
    "def translate_comments_in_code(code_lines):\n",
    "    translated_lines = []\n",
    "    \n",
    "    for line in code_lines:\n",
    "        # 处理纯中文注释行\n",
    "        pure_comment_match = re.match(r'^(\\s*)#\\s*([\\u4e00-\\u9fff].*)', line)\n",
    "        if pure_comment_match:\n",
    "            indent = pure_comment_match.group(1)\n",
    "            chinese_comment = pure_comment_match.group(2)\n",
    "            # 传递术语词典（新增参数）\n",
    "            translated = translate_text(chinese_comment.strip(), term_dict=CUSTOM_TERMS)\n",
    "            translated_lines.append(f\"{indent}# {translated}\\n\")\n",
    "            continue\n",
    "\n",
    "        # 处理行尾中文注释\n",
    "        inline_comment_match = re.search(r'(\\s+#)\\s*([\\u4e00-\\u9fff][^#]*)', line)\n",
    "        if inline_comment_match:\n",
    "            code_part = line[:inline_comment_match.start()]\n",
    "            comment_symbol = inline_comment_match.group(1)\n",
    "            chinese_comment = inline_comment_match.group(2).strip()\n",
    "            # 传递术语词典（新增参数）\n",
    "            translated = translate_text(chinese_comment, term_dict=CUSTOM_TERMS)\n",
    "            translated_lines.append(f\"{code_part}{comment_symbol} {translated}\\n\")\n",
    "            continue\n",
    "\n",
    "        # 处理多行注释\n",
    "        multi_match = re.match(r'^(\\s*)([\"\\']{3})(.*?)\\2', line, re.DOTALL)\n",
    "        if multi_match:\n",
    "            indent = multi_match.group(1)\n",
    "            quote_type = multi_match.group(2)\n",
    "            comment_content = multi_match.group(3)\n",
    "            \n",
    "            if re.search(r'[\\u4e00-\\u9fff]', comment_content):\n",
    "                # 传递术语词典（新增参数）\n",
    "                translated = translate_text(comment_content, term_dict=CUSTOM_TERMS)\n",
    "                translated_lines.append(f\"{indent}{quote_type}{translated}{quote_type}\\n\")\n",
    "                continue\n",
    "\n",
    "        translated_lines.append(line)\n",
    "    \n",
    "    return translated_lines\n",
    "\n",
    "def read_code_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8-sig') as file:\n",
    "        return file.readlines()\n",
    "\n",
    "def save_translated_code(file_path, translated_lines):\n",
    "    with open(file_path, 'w', encoding='utf-8-sig') as file:\n",
    "        file.writelines(translated_lines)\n",
    "\n",
    "def main(input_file_path, output_file_path):\n",
    "    code_lines = read_code_file(input_file_path)\n",
    "    translated_lines = translate_comments_in_code(code_lines)\n",
    "    save_translated_code(output_file_path, translated_lines)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = r'C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\c2e2r_2.py'\n",
    "    output_file = r'C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\translated_test_c2e2r_2.py'\n",
    "    main(input_file, output_file)\n",
    "    print(f\"翻译完成 → {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# 初始化模型和tokenizer\n",
    "device = 'cpu'\n",
    "model_name = 'utrobinmv/t5_translate_en_ru_zh_small_1024'\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 自定义术语词典（按术语长度降序排列）\n",
    "CUSTOM_TERMS = {\n",
    "    \"CSV 表头\": \"Заголовок таблицы CSV\",\n",
    "    \"写入 CSV\": \"Запись в CSV\",\n",
    "    \"PDF 文件\": \"PDF-документ\",\n",
    "    \"CSV\": \"CSV\",\n",
    "    \"PDF\": \"PDF\"\n",
    "}\n",
    "\n",
    "# 安全替换字典（修正模型固有翻译习惯）\n",
    "SAFE_REPLACEMENTS = {\n",
    "    \"КСВ\": \"CSV\",\n",
    "    \"ксв\": \"csv\",\n",
    "    \"ПДФ\": \"PDF\",\n",
    "    \"пдф\": \"pdf\"\n",
    "}\n",
    "\n",
    "prefix = 'translate to ru: '\n",
    "\n",
    "def translate_text(text, target_lang='ru', term_dict=None):\n",
    "    \"\"\"改进后的翻译函数，包含完整术语保护机制\"\"\"\n",
    "    # 第一阶段：保护特殊内容\n",
    "    preserved = {}\n",
    "    \n",
    "    # 1. 保护文件路径\n",
    "    path_pattern = re.compile(r'[a-zA-Z]:\\\\[^ \\u4e00-\\u9fff]+')\n",
    "    for i, match in enumerate(path_pattern.finditer(text)):\n",
    "        placeholder = f\"||PATH_{i}||\"\n",
    "        preserved[placeholder] = match.group()\n",
    "        text = text.replace(match.group(), placeholder)\n",
    "    \n",
    "    # 2. 保护自定义术语\n",
    "    term_placeholders = {}\n",
    "    if term_dict:\n",
    "        sorted_terms = sorted(term_dict.keys(), key=len, reverse=True)\n",
    "        for idx, term in enumerate(sorted_terms):\n",
    "            placeholder = f\"||TERM_{idx}||\"\n",
    "            term_placeholders[placeholder] = term_dict[term]\n",
    "            text = re.sub(re.escape(term), placeholder, text)\n",
    "    \n",
    "    # 执行翻译\n",
    "    src_text = prefix + text\n",
    "    input_ids = tokenizer(src_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    generated_tokens = model.generate(**input_ids.to(device))\n",
    "    translated = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "    \n",
    "    # 第二阶段：恢复和修正\n",
    "    # 1. 恢复术语（优先处理）\n",
    "    for ph, term in term_placeholders.items():\n",
    "        translated = translated.replace(ph, term)\n",
    "    \n",
    "    # 2. 安全替换\n",
    "    for wrong, correct in SAFE_REPLACEMENTS.items():\n",
    "        translated = translated.replace(wrong, correct)\n",
    "    \n",
    "    # 3. 恢复路径\n",
    "    for ph, path in preserved.items():\n",
    "        translated = translated.replace(ph, path)\n",
    "    \n",
    "    # 后处理修正\n",
    "    translated = re.sub(r'(\\b[а-яА-ЯёЁ]+)(\\.py\\b)', r'\\1 \\2', translated)  # .py前空格\n",
    "    translated = re.sub(r'(?<=[а-яА-ЯёЁ])([.,!?])(?=\\S)', r' \\1', translated)  # 标点符号\n",
    "    \n",
    "    return translated\n",
    "\n",
    "def translate_comments_in_code(code_lines):\n",
    "    translated_lines = []\n",
    "    \n",
    "    for line in code_lines:\n",
    "        # 处理单行注释\n",
    "        if pure_comment := re.match(r'^(\\s*)#\\s*([\\u4e00-\\u9fff].*)', line):\n",
    "            indent, comment = pure_comment.groups()\n",
    "            translated = translate_text(comment.strip(), term_dict=CUSTOM_TERMS)\n",
    "            translated_lines.append(f\"{indent}# {translated}\\n\")\n",
    "            continue\n",
    "        \n",
    "        # 处理行尾注释\n",
    "        if inline_comment := re.search(r'(\\s+#)\\s*([\\u4e00-\\u9fff][^#]*)', line):\n",
    "            code_part = line[:inline_comment.start()]\n",
    "            symbol, comment = inline_comment.groups()\n",
    "            translated = translate_text(comment.strip(), term_dict=CUSTOM_TERMS)\n",
    "            translated_lines.append(f\"{code_part}{symbol} {translated}\\n\")\n",
    "            continue\n",
    "        \n",
    "        # 处理多行注释\n",
    "        if multi_comment := re.match(r'^(\\s*)([\"\\']{3})(.*?)\\2', line, re.DOTALL):\n",
    "            indent, quote, content = multi_comment.groups()\n",
    "            if re.search(r'[\\u4e00-\\u9fff]', content):\n",
    "                translated = translate_text(content, term_dict=CUSTOM_TERMS)\n",
    "                translated_lines.append(f\"{indent}{quote}{translated}{quote}\\n\")\n",
    "                continue\n",
    "        \n",
    "        translated_lines.append(line)\n",
    "    \n",
    "    return translated_lines\n",
    "\n",
    "def read_code_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8-sig') as f:\n",
    "        return f.readlines()\n",
    "\n",
    "def save_translated_code(file_path, lines):\n",
    "    with open(file_path, 'w', encoding='utf-8-sig') as f:\n",
    "        f.writelines(lines)\n",
    "\n",
    "def main(input_file, output_file):\n",
    "    code_lines = read_code_file(input_file)\n",
    "    translated = translate_comments_in_code(code_lines)\n",
    "    save_translated_code(output_file, translated)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_path = r'C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\c2e2r.py'\n",
    "    output_path = r'C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\translated_test_18.py'\n",
    "    main(input_path, output_path)\n",
    "    print(f\"翻译完成 → {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# 初始化模型和tokenizer\n",
    "device = 'cpu'  # 使用 CPU\n",
    "model_name = 'utrobinmv/t5_translate_en_ru_zh_small_1024'\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "prefix = 'translate to ru: '  # 更明确的翻译指令\n",
    "\n",
    "def translate_text(text, target_lang='ru'):\n",
    "    # 保护路径中的特殊字符\n",
    "    preserved_paths = re.findall(r'[a-zA-Z]:\\\\[^ \\u4e00-\\u9fff]+', text)\n",
    "    for i, path in enumerate(preserved_paths):\n",
    "        text = text.replace(path, f\"||PATH_{i}||\")\n",
    "    \n",
    "    # 执行翻译\n",
    "    src_text = prefix + text\n",
    "    input_ids = tokenizer(src_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    generated_tokens = model.generate(**input_ids.to(device))\n",
    "    result = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    # 后处理修正\n",
    "    translated = result[0]\n",
    "    \n",
    "    # 恢复被保护的路径\n",
    "    for i, path in enumerate(preserved_paths):\n",
    "        translated = translated.replace(f\"||PATH_{i}||\", path)\n",
    "    \n",
    "    # 修正 .py 前空格问题\n",
    "    translated = re.sub(\n",
    "        r'(\\b[а-яА-ЯёЁ]+)(\\.py\\b)',\n",
    "        lambda m: f\"{m.group(1)} {m.group(2)}\", \n",
    "        translated\n",
    "    )\n",
    "    \n",
    "    # 修正标点格式\n",
    "    translated = re.sub(r'(?<=[а-яА-ЯёЁ])([.,!?])(?=\\S)', r' \\1', translated)\n",
    "    \n",
    "    return translated\n",
    "\n",
    "def read_code_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8-sig') as file:\n",
    "        return file.readlines()\n",
    "\n",
    "def save_translated_code(file_path, translated_lines):\n",
    "    with open(file_path, 'w', encoding='utf-8-sig') as file:\n",
    "        file.writelines(translated_lines)\n",
    "\n",
    "def translate_comments_in_code(code_lines):\n",
    "    translated_lines = []\n",
    "    \n",
    "    for line in code_lines:\n",
    "        # 处理纯中文注释行\n",
    "        pure_comment_match = re.match(r'^(\\s*)#\\s*([\\u4e00-\\u9fff].*)', line)\n",
    "        if pure_comment_match:\n",
    "            indent = pure_comment_match.group(1)\n",
    "            chinese_comment = pure_comment_match.group(2)\n",
    "            translated = translate_text(chinese_comment.strip())\n",
    "            translated_lines.append(f\"{indent}# {translated}\\n\")\n",
    "            continue\n",
    "\n",
    "        # 处理行尾中文注释\n",
    "        inline_comment_match = re.search(r'(\\s+#)\\s*([\\u4e00-\\u9fff][^#]*)', line)\n",
    "        if inline_comment_match:\n",
    "            code_part = line[:inline_comment_match.start()]\n",
    "            comment_symbol = inline_comment_match.group(1)\n",
    "            chinese_comment = inline_comment_match.group(2).strip()\n",
    "            translated = translate_text(chinese_comment)\n",
    "            translated_lines.append(f\"{code_part}{comment_symbol} {translated}\\n\")\n",
    "            continue\n",
    "\n",
    "        # 处理多行注释\n",
    "        multi_match = re.match(r'^(\\s*)([\"\\']{3})(.*?)\\2', line, re.DOTALL)\n",
    "        if multi_match:\n",
    "            indent = multi_match.group(1)\n",
    "            quote_type = multi_match.group(2)\n",
    "            comment_content = multi_match.group(3)\n",
    "            \n",
    "            if re.search(r'[\\u4e00-\\u9fff]', comment_content):\n",
    "                translated = translate_text(comment_content)\n",
    "                translated_lines.append(f\"{indent}{quote_type}{translated}{quote_type}\\n\")\n",
    "                continue\n",
    "\n",
    "        translated_lines.append(line)\n",
    "    \n",
    "    return translated_lines\n",
    "\n",
    "def main(input_file_path, output_file_path):\n",
    "    code_lines = read_code_file(input_file_path)\n",
    "    translated_lines = translate_comments_in_code(code_lines)\n",
    "    save_translated_code(output_file_path, translated_lines)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = r'C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\c2e2r_2.py'\n",
    "    output_file = r'C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\translated_test_c2e2r_2.py'\n",
    "    \n",
    "    # input_file = r'C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\c2e2r.py'\n",
    "    # output_file = r'C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\translated_test_c2e2r.py'\n",
    "    \n",
    "    main(input_file, output_file)\n",
    "    print(f\"翻译完成 → {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# 初始化模型和tokenizer\n",
    "device = 'cpu'\n",
    "model_name = 'utrobinmv/t5_translate_en_ru_zh_small_1024'\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "prefix = 'translate to ru: '  # 翻译指令\n",
    "\n",
    "def translate_text(text, target_lang='ru'):\n",
    "    \n",
    "    # 自定义术语词典\n",
    "    term_dict = {\n",
    "        \"表头\": \"Заголовок таблицы\",\n",
    "        \"写入 CSV\": \"Запись в CSV\",\n",
    "        \"CSV\": \"CSV\"  # 保留CSV不翻译\n",
    "    }\n",
    "    \n",
    "    # 保护术语\n",
    "    for cn, ru in term_dict.items():\n",
    "        text = text.replace(cn, f\"||TERM_{cn}||\")\n",
    "    \n",
    "    # 保护路径\n",
    "    preserved_paths = re.findall(r'[a-zA-Z]:\\\\[^ \\u4e00-\\u9fff]+', text)\n",
    "    for i, path in enumerate(preserved_paths):\n",
    "        text = text.replace(path, f\"||PATH_{i}||\")\n",
    "    \n",
    "    # 执行翻译\n",
    "    src_text = prefix + text\n",
    "    input_ids = tokenizer(src_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    generated_tokens = model.generate(**input_ids.to(device))\n",
    "    result = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    # 后处理\n",
    "    translated = result[0]\n",
    "    \n",
    "    # 恢复路径\n",
    "    for i, path in enumerate(preserved_paths):\n",
    "        translated = translated.replace(f\"||PATH_{i}||\", path)\n",
    "    \n",
    "    # 恢复术语\n",
    "    for cn, ru in term_dict.items():\n",
    "        translated = translated.replace(f\"||TERM_{cn}||\", ru)\n",
    "    \n",
    "    # 格式修正\n",
    "    translated = re.sub(r'(\\b[а-яА-ЯёЁ]+)(\\.py\\b)', r'\\1 \\2', translated)\n",
    "    translated = re.sub(r'(?<=[а-яА-ЯёЁ])([.,!?])(?=\\S)', r' \\1', translated)\n",
    "    \n",
    "    return translated\n",
    "\n",
    "def read_code_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.readlines()\n",
    "\n",
    "def save_translated_code(file_path, translated_lines):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.writelines(translated_lines)\n",
    "\n",
    "def translate_comments_in_code(code_lines):\n",
    "    translated_lines = []\n",
    "    \n",
    "    for line in code_lines:\n",
    "        # 增强注释匹配\n",
    "        comment_match = re.match(r'^(\\s*)#\\s*([\\u4e00-\\u9fff].*?[a-zA-Z]*)', line)\n",
    "        if comment_match:\n",
    "            indent = comment_match.group(1)\n",
    "            comment_content = comment_match.group(2)\n",
    "            translated = translate_text(comment_content.strip())\n",
    "            translated_lines.append(f\"{indent}# {translated}\\n\")\n",
    "            continue\n",
    "\n",
    "        # 增强行尾注释处理\n",
    "        inline_match = re.search(r'(\\s+#)\\s*([\\u4e00-\\u9fff].*?[a-zA-Z]*)', line)\n",
    "        if inline_match:\n",
    "            code_part = line[:inline_match.start()]\n",
    "            symbol = inline_match.group(1)\n",
    "            comment = inline_match.group(2)\n",
    "            translated = translate_text(comment.strip())\n",
    "            translated_lines.append(f\"{code_part}{symbol} {translated}\\n\")\n",
    "            continue\n",
    "\n",
    "        # 处理多行注释\n",
    "        multi_match = re.match(r'^(\\s*)([\"\\']{3})(.*?)\\2', line, re.DOTALL)\n",
    "        if multi_match:\n",
    "            indent = multi_match.group(1)\n",
    "            quote_type = multi_match.group(2)\n",
    "            comment_content = multi_match.group(3)\n",
    "            \n",
    "            if re.search(r'[\\u4e00-\\u9fff]', comment_content):\n",
    "                translated = translate_text(comment_content)\n",
    "                translated_lines.append(f\"{indent}{quote_type}{translated}{quote_type}\\n\")\n",
    "                continue\n",
    "\n",
    "        translated_lines.append(line)\n",
    "    \n",
    "    return translated_lines\n",
    "\n",
    "def main(input_file_path, output_file_path):\n",
    "    code_lines = read_code_file(input_file_path)\n",
    "    translated_lines = translate_comments_in_code(code_lines)\n",
    "    save_translated_code(output_file_path, translated_lines)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = r'C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\c2e2r.py'\n",
    "    output_file = r'C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\translated_test_16.py'\n",
    "    main(input_file, output_file)\n",
    "    print(f\"翻译完成 → {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "翻译后的代码文件已保存至: C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\translated_test_13.py\n"
     ]
    }
   ],
   "source": [
    "# 初始化模型和tokenizer\n",
    "device = 'cpu'  # 使用 CPU\n",
    "model_name = 'utrobinmv/t5_translate_en_ru_zh_small_1024'\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "prefix = 'translate to ru: '  # 指定目标语言为俄语\n",
    "\n",
    "# 用于翻译函数\n",
    "def translate_text(text, target_lang='ru'):\n",
    "    src_text = prefix + text  # 添加翻译前缀\n",
    "    input_ids = tokenizer(src_text, return_tensors=\"pt\")\n",
    "    generated_tokens = model.generate(**input_ids.to(device))\n",
    "    result = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "    # return result[0]\n",
    "    \n",
    "    # 添加后处理修正\n",
    "    translated = result[0]\n",
    "    \n",
    "    # 修正 \".py\" 前的空格问题（匹配俄语单词+标点模式）\n",
    "    translated = re.sub(\n",
    "        r'(\\b[а-яА-ЯёЁ]+)(\\.py\\b)', \n",
    "        lambda m: f\"{m.group(1)} {m.group(2)}\", \n",
    "        translated\n",
    "    )\n",
    "    \n",
    "    # 修正其他常见标点格式（可选）\n",
    "    translated = re.sub(r'(?<=[а-яА-ЯёЁ])([.,!?])', r' \\1', translated)\n",
    "    \n",
    "    return translated\n",
    "\n",
    "# 读取原始代码文件\n",
    "def read_code_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.readlines()\n",
    "\n",
    "# 保存翻译后的代码到新文件\n",
    "def save_translated_code(file_path, translated_lines):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.writelines(translated_lines)\n",
    "\n",
    "# 翻译代码注释\n",
    "def translate_comments_in_code(code_lines):\n",
    "    translated_lines = []\n",
    "    \n",
    "    for line in code_lines:\n",
    "        # 精确匹配纯中文注释行（示例： # 注释内容）\n",
    "        pure_comment_match = re.match(r'^(\\s*)#\\s*([\\u4e00-\\u9fff].*)', line)\n",
    "        if pure_comment_match:\n",
    "            indent = pure_comment_match.group(1)\n",
    "            chinese_comment = pure_comment_match.group(2)\n",
    "            \n",
    "            translated = translate_text(chinese_comment.strip())\n",
    "            translated_lines.append(f\"{indent}# {translated}\\n\")\n",
    "            continue\n",
    "\n",
    "        # 匹配行尾中文注释（示例： code...  # 注释内容）\n",
    "        inline_comment_match = re.search(r'(\\s+#)\\s*([\\u4e00-\\u9fff][^#]*)', line)\n",
    "        if inline_comment_match:\n",
    "            code_part = line[:inline_comment_match.start()]\n",
    "            comment_symbol = inline_comment_match.group(1)\n",
    "            chinese_comment = inline_comment_match.group(2).strip()\n",
    "            \n",
    "            translated = translate_text(chinese_comment)\n",
    "            translated_line = f\"{code_part}{comment_symbol} {translated}\\n\"\n",
    "            translated_lines.append(translated_line)\n",
    "            continue\n",
    "\n",
    "        # 多行注释处理（保持原有逻辑）\n",
    "        multi_match = re.match(r'^(\\s*)([\"\\']{3})(.*?)\\2', line, re.DOTALL)\n",
    "        if multi_match:\n",
    "            indent = multi_match.group(1)\n",
    "            quote_type = multi_match.group(2)\n",
    "            comment_content = multi_match.group(3)\n",
    "            \n",
    "            if re.search(r'[\\u4e00-\\u9fff]', comment_content):\n",
    "                translated = translate_text(comment_content)\n",
    "                translated_lines.append(f\"{indent}{quote_type}{translated}{quote_type}\\n\")\n",
    "                continue\n",
    "\n",
    "        translated_lines.append(line)\n",
    "    \n",
    "    return translated_lines\n",
    "\n",
    "# 主流程\n",
    "def main(input_file_path, output_file_path):\n",
    "    code_lines = read_code_file(input_file_path)  # 读取代码文件\n",
    "    translated_lines = translate_comments_in_code(code_lines)  # 翻译注释\n",
    "    save_translated_code(output_file_path, translated_lines)  # 保存翻译后的文件\n",
    "\n",
    "# 示例调用\n",
    "input_file_path = r'C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\c2e2r.py'  # 输入的 Python 文件路径\n",
    "output_file_path = r'C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\translated_test_13.py'  # 输出的翻译后的文件路径\n",
    "main(input_file_path, output_file_path)\n",
    "\n",
    "print(f\"翻译后的代码文件已保存至: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "翻译后的代码文件已保存至: C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\translated_test_12.py\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# 初始化模型和tokenizer\n",
    "device = 'cpu'  # 使用 CPU\n",
    "model_name = 'utrobinmv/t5_translate_en_ru_zh_small_1024'\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "prefix = 'translate to ru: '  # 指定目标语言为俄语\n",
    "\n",
    "# 用于翻译函数\n",
    "def translate_text(text, target_lang='ru'):\n",
    "    src_text = prefix + text  # 添加翻译前缀\n",
    "    input_ids = tokenizer(src_text, return_tensors=\"pt\")\n",
    "    generated_tokens = model.generate(**input_ids.to(device))\n",
    "    result = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "    return result[0]\n",
    "\n",
    "# 读取原始代码文件\n",
    "def read_code_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.readlines()\n",
    "\n",
    "# 保存翻译后的代码到新文件\n",
    "def save_translated_code(file_path, translated_lines):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.writelines(translated_lines)\n",
    "\n",
    "# 识别并翻译注释\n",
    "# def translate_comments_in_code(code_lines):\n",
    "#     translated_lines = []\n",
    "    \n",
    "#     for line in code_lines:\n",
    "#         # 匹配单行注释\n",
    "#         single_comment_pattern = r'#(.*)'  # 只匹配#后面的内容\n",
    "#         match = re.search(single_comment_pattern, line)\n",
    "        \n",
    "#         if match:\n",
    "#             comment_text = match.group(1).strip()  # 获取注释内容\n",
    "#             # 如果注释是中文，则翻译\n",
    "#             if re.search(r'[\\u4e00-\\u9fff]', comment_text):  # 检查是否包含中文字符\n",
    "#                 translated_comment = translate_text(comment_text, target_lang='ru')\n",
    "#                 # 替换原注释，保持格式\n",
    "#                 line = line.replace(comment_text, ' ' + translated_comment)\n",
    "        \n",
    "#         # 处理多行注释\n",
    "#         multi_comment_pattern = r'\"\"\".*?\"\"\"|\\'\\'\\'.*?\\'\\'\\''  # 处理三引号注释\n",
    "#         multi_match = re.search(multi_comment_pattern, line, re.DOTALL)\n",
    "        \n",
    "#         if multi_match:\n",
    "#             comment_text = multi_match.group(0)\n",
    "#             # 如果多行注释是中文，则翻译\n",
    "#             if re.search(r'[\\u4e00-\\u9fff]', comment_text):  # 检查是否包含中文字符\n",
    "#                 translated_comment = translate_text(comment_text, target_lang='ru')\n",
    "#                 # 保持原来的注释符号，替换注释内容\n",
    "#                 line = line.replace(comment_text, '\"\"\"' + translated_comment + '\"\"\"')\n",
    "        \n",
    "#         translated_lines.append(line)\n",
    "    \n",
    "#     return translated_lines\n",
    "\n",
    "# 修改2：优化注释匹配和替换逻辑\n",
    "# def translate_comments_in_code(code_lines):\n",
    "#     translated_lines = []\n",
    "    \n",
    "#     for line in code_lines:\n",
    "#         # 单行注释处理（优化正则）\n",
    "#         if '#' in line:\n",
    "#             parts = line.split('#', 1)\n",
    "#             code_part, comment_part = parts[0], parts[1]\n",
    "#             if re.search(r'[\\u4e00-\\u9fff]', comment_part):\n",
    "#                 translated = translate_text(comment_part.strip())\n",
    "#                 translated_lines.append(f\"{code_part}# {translated}\\n\")\n",
    "#                 continue\n",
    "\n",
    "#         # 多行注释处理（优化正则和替换逻辑）\n",
    "#         multi_match = re.search(r'(\"\"\"|\\'\\'\\')(.*?)\\1', line, re.DOTALL)\n",
    "#         if multi_match:\n",
    "#             quote_type = multi_match.group(1)\n",
    "#             comment_content = multi_match.group(2)\n",
    "#             if re.search(r'[\\u4e00-\\u9fff]', comment_content):\n",
    "#                 translated = translate_text(comment_content)\n",
    "#                 translated_lines.append(f\"{quote_type}{translated}{quote_type}\\n\")\n",
    "#                 continue\n",
    "\n",
    "#         translated_lines.append(line)\n",
    "    \n",
    "#     return translated_lines\n",
    "\n",
    "# def translate_comments_in_code(code_lines):\n",
    "#     translated_lines = []\n",
    "    \n",
    "#     for line in code_lines:\n",
    "#         # 单行注释处理（已保留缩进）\n",
    "#         if '#' in line:\n",
    "#             parts = line.split('#', 1)\n",
    "#             code_part, comment_part = parts[0], parts[1]\n",
    "#             if re.search(r'[\\u4e00-\\u9fff]', comment_part):\n",
    "#                 translated = translate_text(comment_part.strip())\n",
    "#                 translated_lines.append(f\"{code_part}# {translated}\\n\")\n",
    "#                 continue\n",
    "\n",
    "#         # 多行注释处理（新增缩进捕获）\n",
    "#         multi_match = re.match(r'^(\\s*)([\"\\']{3})(.*?)\\2', line, re.DOTALL)\n",
    "#         if multi_match:\n",
    "#             indent = multi_match.group(1)  # 捕获缩进\n",
    "#             quote_type = multi_match.group(2)\n",
    "#             comment_content = multi_match.group(3)\n",
    "            \n",
    "#             if re.search(r'[\\u4e00-\\u9fff]', comment_content):\n",
    "#                 translated = translate_text(comment_content)\n",
    "#                 # 保留原始缩进和引号结构\n",
    "#                 translated_lines.append(f\"{indent}{quote_type}{translated}{quote_type}\\n\")\n",
    "#                 continue\n",
    "\n",
    "#         translated_lines.append(line)\n",
    "    \n",
    "#     return translated_lines\n",
    "\n",
    "def translate_comments_in_code(code_lines):\n",
    "    translated_lines = []\n",
    "    \n",
    "    for line in code_lines:\n",
    "        # 精确匹配纯中文注释行（示例： # 注释内容）\n",
    "        pure_comment_match = re.match(r'^(\\s*)#\\s*([\\u4e00-\\u9fff].*)', line)\n",
    "        if pure_comment_match:\n",
    "            indent = pure_comment_match.group(1)\n",
    "            chinese_comment = pure_comment_match.group(2)\n",
    "            \n",
    "            translated = translate_text(chinese_comment.strip())\n",
    "            translated_lines.append(f\"{indent}# {translated}\\n\")\n",
    "            continue\n",
    "\n",
    "        # 匹配行尾中文注释（示例： code...  # 注释内容）\n",
    "        inline_comment_match = re.search(r'(\\s+#)\\s*([\\u4e00-\\u9fff][^#]*)', line)\n",
    "        if inline_comment_match:\n",
    "            code_part = line[:inline_comment_match.start()]\n",
    "            comment_symbol = inline_comment_match.group(1)\n",
    "            chinese_comment = inline_comment_match.group(2).strip()\n",
    "            \n",
    "            translated = translate_text(chinese_comment)\n",
    "            translated_line = f\"{code_part}{comment_symbol} {translated}\\n\"\n",
    "            translated_lines.append(translated_line)\n",
    "            continue\n",
    "\n",
    "        # 多行注释处理（保持原有逻辑）\n",
    "        multi_match = re.match(r'^(\\s*)([\"\\']{3})(.*?)\\2', line, re.DOTALL)\n",
    "        if multi_match:\n",
    "            indent = multi_match.group(1)\n",
    "            quote_type = multi_match.group(2)\n",
    "            comment_content = multi_match.group(3)\n",
    "            \n",
    "            if re.search(r'[\\u4e00-\\u9fff]', comment_content):\n",
    "                translated = translate_text(comment_content)\n",
    "                translated_lines.append(f\"{indent}{quote_type}{translated}{quote_type}\\n\")\n",
    "                continue\n",
    "\n",
    "        translated_lines.append(line)\n",
    "    \n",
    "    return translated_lines\n",
    "\n",
    "# 主流程\n",
    "def main(input_file_path, output_file_path):\n",
    "    code_lines = read_code_file(input_file_path)  # 读取代码文件\n",
    "    translated_lines = translate_comments_in_code(code_lines)  # 翻译注释\n",
    "    save_translated_code(output_file_path, translated_lines)  # 保存翻译后的文件\n",
    "\n",
    "# 示例调用\n",
    "input_file_path = r'C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\c2e2r.py'  # 输入的 Python 文件路径\n",
    "output_file_path = r'C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\translated_test_12.py'  # 输出的翻译后的文件路径\n",
    "main(input_file_path, output_file_path)\n",
    "\n",
    "print(f\"翻译后的代码文件已保存至: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "翻译后的代码文件已保存至: C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\translated_test_11.py\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# 初始化模型和tokenizer\n",
    "device = 'cpu'  # 使用 CPU\n",
    "model_name = 'utrobinmv/t5_translate_en_ru_zh_small_1024'\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "prefix = 'translate to ru: '  # 指定目标语言为俄语\n",
    "\n",
    "# 用于翻译函数\n",
    "def translate_text(text, target_lang='ru'):\n",
    "    src_text = prefix + text  # 添加翻译前缀\n",
    "    input_ids = tokenizer(src_text, return_tensors=\"pt\")\n",
    "    generated_tokens = model.generate(**input_ids.to(device))\n",
    "    result = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "    return result[0]\n",
    "\n",
    "# 读取原始代码文件\n",
    "def read_code_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.readlines()\n",
    "\n",
    "# 保存翻译后的代码到新文件\n",
    "def save_translated_code(file_path, translated_lines):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.writelines(translated_lines)\n",
    "\n",
    "# 识别并翻译注释\n",
    "# def translate_comments_in_code(code_lines):\n",
    "#     translated_lines = []\n",
    "    \n",
    "#     for line in code_lines:\n",
    "#         # 匹配单行注释\n",
    "#         single_comment_pattern = r'#(.*)'  # 只匹配#后面的内容\n",
    "#         match = re.search(single_comment_pattern, line)\n",
    "        \n",
    "#         if match:\n",
    "#             comment_text = match.group(1).strip()  # 获取注释内容\n",
    "#             # 如果注释是中文，则翻译\n",
    "#             if re.search(r'[\\u4e00-\\u9fff]', comment_text):  # 检查是否包含中文字符\n",
    "#                 translated_comment = translate_text(comment_text, target_lang='ru')\n",
    "#                 # 替换原注释，保持格式\n",
    "#                 line = line.replace(comment_text, ' ' + translated_comment)\n",
    "        \n",
    "#         # 处理多行注释\n",
    "#         multi_comment_pattern = r'\"\"\".*?\"\"\"|\\'\\'\\'.*?\\'\\'\\''  # 处理三引号注释\n",
    "#         multi_match = re.search(multi_comment_pattern, line, re.DOTALL)\n",
    "        \n",
    "#         if multi_match:\n",
    "#             comment_text = multi_match.group(0)\n",
    "#             # 如果多行注释是中文，则翻译\n",
    "#             if re.search(r'[\\u4e00-\\u9fff]', comment_text):  # 检查是否包含中文字符\n",
    "#                 translated_comment = translate_text(comment_text, target_lang='ru')\n",
    "#                 # 保持原来的注释符号，替换注释内容\n",
    "#                 line = line.replace(comment_text, '\"\"\"' + translated_comment + '\"\"\"')\n",
    "        \n",
    "#         translated_lines.append(line)\n",
    "    \n",
    "#     return translated_lines\n",
    "\n",
    "# 修改2：优化注释匹配和替换逻辑\n",
    "# def translate_comments_in_code(code_lines):\n",
    "#     translated_lines = []\n",
    "    \n",
    "#     for line in code_lines:\n",
    "#         # 单行注释处理（优化正则）\n",
    "#         if '#' in line:\n",
    "#             parts = line.split('#', 1)\n",
    "#             code_part, comment_part = parts[0], parts[1]\n",
    "#             if re.search(r'[\\u4e00-\\u9fff]', comment_part):\n",
    "#                 translated = translate_text(comment_part.strip())\n",
    "#                 translated_lines.append(f\"{code_part}# {translated}\\n\")\n",
    "#                 continue\n",
    "\n",
    "#         # 多行注释处理（优化正则和替换逻辑）\n",
    "#         multi_match = re.search(r'(\"\"\"|\\'\\'\\')(.*?)\\1', line, re.DOTALL)\n",
    "#         if multi_match:\n",
    "#             quote_type = multi_match.group(1)\n",
    "#             comment_content = multi_match.group(2)\n",
    "#             if re.search(r'[\\u4e00-\\u9fff]', comment_content):\n",
    "#                 translated = translate_text(comment_content)\n",
    "#                 translated_lines.append(f\"{quote_type}{translated}{quote_type}\\n\")\n",
    "#                 continue\n",
    "\n",
    "#         translated_lines.append(line)\n",
    "    \n",
    "#     return translated_lines\n",
    "\n",
    "def translate_comments_in_code(code_lines):\n",
    "    translated_lines = []\n",
    "    \n",
    "    for line in code_lines:\n",
    "        # 单行注释处理（已保留缩进）\n",
    "        if '#' in line:\n",
    "            parts = line.split('#', 1)\n",
    "            code_part, comment_part = parts[0], parts[1]\n",
    "            if re.search(r'[\\u4e00-\\u9fff]', comment_part):\n",
    "                translated = translate_text(comment_part.strip())\n",
    "                translated_lines.append(f\"{code_part}# {translated}\\n\")\n",
    "                continue\n",
    "\n",
    "        # 多行注释处理（新增缩进捕获）\n",
    "        multi_match = re.match(r'^(\\s*)([\"\\']{3})(.*?)\\2', line, re.DOTALL)\n",
    "        if multi_match:\n",
    "            indent = multi_match.group(1)  # 捕获缩进\n",
    "            quote_type = multi_match.group(2)\n",
    "            comment_content = multi_match.group(3)\n",
    "            \n",
    "            if re.search(r'[\\u4e00-\\u9fff]', comment_content):\n",
    "                translated = translate_text(comment_content)\n",
    "                # 保留原始缩进和引号结构\n",
    "                translated_lines.append(f\"{indent}{quote_type}{translated}{quote_type}\\n\")\n",
    "                continue\n",
    "\n",
    "        translated_lines.append(line)\n",
    "    \n",
    "    return translated_lines\n",
    "\n",
    "# 主流程\n",
    "def main(input_file_path, output_file_path):\n",
    "    code_lines = read_code_file(input_file_path)  # 读取代码文件\n",
    "    translated_lines = translate_comments_in_code(code_lines)  # 翻译注释\n",
    "    save_translated_code(output_file_path, translated_lines)  # 保存翻译后的文件\n",
    "\n",
    "# 示例调用\n",
    "input_file_path = r'C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\c2e2r.py'  # 输入的 Python 文件路径\n",
    "output_file_path = r'C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\translated_test_11.py'  # 输出的翻译后的文件路径\n",
    "main(input_file_path, output_file_path)\n",
    "\n",
    "print(f\"翻译后的代码文件已保存至: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "翻译后的代码文件已保存至: C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\translated_test_9.py\n"
     ]
    }
   ],
   "source": [
    "# 初始化模型和tokenizer \n",
    "device = 'cpu'  # 使用 CPU\n",
    "model_name = 'utrobinmv/t5_translate_en_ru_zh_small_1024'\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "prefix = 'translate to russian: '  # 指定目标语言为俄语\n",
    "\n",
    "# 用于翻译函数\n",
    "def translate_text(text, target_lang='ru'):\n",
    "    src_text = prefix + text  # 添加翻译前缀\n",
    "    input_ids = tokenizer(src_text, return_tensors=\"pt\")\n",
    "    generated_tokens = model.generate(**input_ids.to(device))\n",
    "    result = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "    return result[0]\n",
    "\n",
    "# 读取原始代码文件\n",
    "def read_code_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.readlines()\n",
    "\n",
    "# 保存翻译后的代码到新文件\n",
    "def save_translated_code(file_path, translated_lines):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.writelines(translated_lines)\n",
    "\n",
    "# 识别并翻译注释\n",
    "def translate_comments_in_code(code_lines):\n",
    "    translated_lines = []\n",
    "    \n",
    "    for line in code_lines:\n",
    "        # 匹配单行注释\n",
    "        single_comment_pattern = r'#(.*)'  # 只匹配#后面的内容\n",
    "        match = re.search(single_comment_pattern, line)\n",
    "        \n",
    "        if match:\n",
    "            comment_text = match.group(1).strip()  # 获取注释内容\n",
    "            # 如果注释是中文，则翻译\n",
    "            if re.search(r'[\\u4e00-\\u9fff]', comment_text):  # 检查是否包含中文字符\n",
    "                translated_comment = translate_text(comment_text, target_lang='ru')\n",
    "                # 替换原注释，保持格式\n",
    "                line = line.replace(comment_text, ' ' + translated_comment)\n",
    "        \n",
    "        # 处理多行注释\n",
    "        multi_comment_pattern = r'\"\"\".*?\"\"\"|\\'\\'\\'.*?\\'\\'\\''  # 处理三引号注释\n",
    "        multi_match = re.search(multi_comment_pattern, line, re.DOTALL)\n",
    "        \n",
    "        if multi_match:\n",
    "            comment_text = multi_match.group(0)\n",
    "            # 如果多行注释是中文，则翻译\n",
    "            if re.search(r'[\\u4e00-\\u9fff]', comment_text):  # 检查是否包含中文字符\n",
    "                translated_comment = translate_text(comment_text, target_lang='ru')\n",
    "                # 保持原来的注释符号，替换注释内容\n",
    "                line = line.replace(comment_text, '\"\"\"' + translated_comment + '\"\"\"')\n",
    "        \n",
    "        translated_lines.append(line)\n",
    "    \n",
    "    return translated_lines\n",
    "\n",
    "# 主流程\n",
    "def main(input_file_path, output_file_path):\n",
    "    code_lines = read_code_file(input_file_path)  # 读取代码文件\n",
    "    translated_lines = translate_comments_in_code(code_lines)  # 翻译注释\n",
    "    save_translated_code(output_file_path, translated_lines)  # 保存翻译后的文件\n",
    "\n",
    "# 示例调用\n",
    "input_file_path = r'C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\c2e2r.py'  # 输入的 Python 文件路径\n",
    "output_file_path = r'C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\translated_test_9.py'  # 输出的翻译后的文件路径\n",
    "main(input_file_path, output_file_path)\n",
    "\n",
    "print(f\"翻译后的代码文件已保存至: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "翻译后的代码文件已保存至: C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\translated_test_8.py\n"
     ]
    }
   ],
   "source": [
    "# 初始化模型和tokenizer\n",
    "device = 'cpu'  # 使用 CPU\n",
    "model_name = 'utrobinmv/t5_translate_en_ru_zh_small_1024'\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "prefix = 'translate to russian: '  # 指定目标语言为俄语\n",
    "\n",
    "# 用于翻译函数\n",
    "def translate_text(text, target_lang='ru'):\n",
    "    src_text = prefix + text  # 添加翻译前缀\n",
    "    input_ids = tokenizer(src_text, return_tensors=\"pt\")\n",
    "    generated_tokens = model.generate(**input_ids.to(device))\n",
    "    result = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "    return result[0]\n",
    "\n",
    "# 读取原始代码文件\n",
    "def read_code_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.readlines()\n",
    "\n",
    "# 保存翻译后的代码到新文件\n",
    "def save_translated_code(file_path, translated_lines):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.writelines(translated_lines)\n",
    "\n",
    "# 识别并翻译注释\n",
    "def translate_comments_in_code(code_lines):\n",
    "    translated_lines = []\n",
    "    \n",
    "    for line in code_lines:\n",
    "        # 匹配单行注释\n",
    "        single_comment_pattern = r'#(.*)'  # 只匹配#后面的内容\n",
    "        match = re.search(single_comment_pattern, line)\n",
    "        \n",
    "        if match:\n",
    "            comment_text = match.group(1).strip()  # 获取注释内容\n",
    "            # 如果注释是中文，则翻译\n",
    "            if re.search(r'[\\u4e00-\\u9fff]', comment_text):  # 检查是否包含中文字符\n",
    "                translated_comment = translate_text(comment_text, target_lang='ru')\n",
    "                # 替换原注释，保持格式\n",
    "                line = line.replace(comment_text, ' ' + translated_comment)\n",
    "        \n",
    "        # 处理多行注释\n",
    "        multi_comment_pattern = r'\"\"\".*?\"\"\"|\\'\\'\\'.*?\\'\\'\\''  # 处理三引号注释\n",
    "        multi_match = re.search(multi_comment_pattern, line, re.DOTALL)\n",
    "        \n",
    "        if multi_match:\n",
    "            comment_text = multi_match.group(0)\n",
    "            # 如果多行注释是中文，则翻译\n",
    "            if re.search(r'[\\u4e00-\\u9fff]', comment_text):  # 检查是否包含中文字符\n",
    "                translated_comment = translate_text(comment_text, target_lang='ru')\n",
    "                # 保持原来的注释符号，替换注释内容\n",
    "                line = line.replace(comment_text, '\"\"' + translated_comment + '\"\"')\n",
    "        \n",
    "        translated_lines.append(line)\n",
    "    \n",
    "    return translated_lines\n",
    "\n",
    "# 主流程\n",
    "def main(input_file_path, output_file_path):\n",
    "    code_lines = read_code_file(input_file_path)  # 读取代码文件\n",
    "    translated_lines = translate_comments_in_code(code_lines)  # 翻译注释\n",
    "    save_translated_code(output_file_path, translated_lines)  # 保存翻译后的文件\n",
    "\n",
    "# 示例调用\n",
    "input_file_path = r'C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\c2e2r.py'  # 输入的 Python 文件路径\n",
    "output_file_path = r'C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\translated_test_8.py'  # 输出的翻译后的文件路径\n",
    "main(input_file_path, output_file_path)\n",
    "\n",
    "print(f\"翻译后的代码文件已保存至: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "翻译后的代码文件已保存至: C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\translated_test_7.py\n"
     ]
    }
   ],
   "source": [
    "# 初始化模型和tokenizer\n",
    "device = 'cpu'  # 使用 CPU\n",
    "model_name = 'utrobinmv/t5_translate_en_ru_zh_small_1024'\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "prefix = 'translate to russian: '\n",
    "\n",
    "# 用于翻译函数\n",
    "def translate_text(text, target_lang='ru'):\n",
    "    src_text = prefix + text\n",
    "    input_ids = tokenizer(src_text, return_tensors=\"pt\")\n",
    "    generated_tokens = model.generate(**input_ids.to(device))\n",
    "    result = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "    return result[0]\n",
    "\n",
    "# 读取原始代码文件\n",
    "def read_code_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.readlines()\n",
    "\n",
    "# 保存翻译后的代码到新文件\n",
    "def save_translated_code(file_path, translated_lines):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.writelines(translated_lines)\n",
    "\n",
    "# 识别并翻译注释\n",
    "def translate_comments_in_code(code_lines):\n",
    "    translated_lines = []\n",
    "    \n",
    "    for line in code_lines:\n",
    "        # 匹配单行注释\n",
    "        single_comment_pattern = r'#(.*)'\n",
    "        match = re.search(single_comment_pattern, line)\n",
    "        \n",
    "        if match:\n",
    "            comment_text = match.group(1).strip()  # 获取注释内容\n",
    "            # 如果注释是中文，则翻译\n",
    "            if re.search(r'[\\u4e00-\\u9fff]', comment_text):  # 中文字符\n",
    "                translated_comment = translate_text(comment_text, target_lang='ru')\n",
    "                # 替换原注释\n",
    "                line = line.replace(comment_text, translated_comment)\n",
    "        \n",
    "        # 处理多行注释\n",
    "        multi_comment_pattern = r'\"\"\".*?\"\"\"|\\'\\'\\'.*?\\'\\'\\''\n",
    "        multi_match = re.search(multi_comment_pattern, line, re.DOTALL)\n",
    "        \n",
    "        if multi_match:\n",
    "            comment_text = multi_match.group(0)\n",
    "            # 如果多行注释是中文，则翻译\n",
    "            if re.search(r'[\\u4e00-\\u9fff]', comment_text):  # 中文字符\n",
    "                translated_comment = translate_text(comment_text, target_lang='ru')\n",
    "                line = line.replace(comment_text, translated_comment)\n",
    "        \n",
    "        translated_lines.append(line)\n",
    "    \n",
    "    return translated_lines\n",
    "\n",
    "# 主流程\n",
    "def main(input_file_path, output_file_path):\n",
    "    code_lines = read_code_file(input_file_path)  # 读取代码文件\n",
    "    translated_lines = translate_comments_in_code(code_lines)  # 翻译注释\n",
    "    save_translated_code(output_file_path, translated_lines)  # 保存翻译后的文件\n",
    "\n",
    "# 示例调用\n",
    "input_file_path = r'C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\c2e2r.py'  # 输入的 Python 文件路径\n",
    "output_file_path = r'C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\translated_test_7.py'  # 输出的翻译后的文件路径\n",
    "main(input_file_path, output_file_path)\n",
    "\n",
    "print(f\"翻译后的代码文件已保存至: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
