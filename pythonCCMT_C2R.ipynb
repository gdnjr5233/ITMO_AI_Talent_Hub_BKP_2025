{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【步骤1】微调模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 加载所需库\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import accelerate\n",
    "import evaluate\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据样本：\n",
      "                                    input_text  \\\n",
      "0    translated into russian: 用于两个样本分位数的自助法t检验   \n",
      "1    translated into russian: 计算控制组和实验组的自助法分位数   \n",
      "2        translated into russian: 对自助法分位数进行t检验   \n",
      "3           translated into russian: 判断是否拒绝原假设   \n",
      "4  translated into russian: 返回使用自助抽样法计算的分位数分布。   \n",
      "\n",
      "                                         target_text  \n",
      "0  t-тест самообслуживания для двух квартилей выб...  \n",
      "1  Рассчитать квантили метода самообслуживания дл...  \n",
      "2       Провести t-тест на квантиль самообслуживания  \n",
      "3  Определение того, отклонять или нет первоначал...  \n",
      "4  Возвращает квантильное распределение, рассчита...  \n"
     ]
    }
   ],
   "source": [
    "# 2. 加载 CSV 数据集\n",
    "\n",
    "csv_path = r\"C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\datasets\\data.csv\"  # 替换为你的 CSV 文件路径\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# 这里以 comment_zh 作为输入、comment_ru 作为目标文本；你也可以调整为结合代码内容的信息\n",
    "# 为确保模型正确接收指令，我们在输入文本前加上翻译提示前缀\n",
    "prefix = \"translated into russian: \"\n",
    "df[\"input_text\"] = prefix + df[\"comment_zh\"].astype(str)\n",
    "df[\"target_text\"] = df[\"comment_ru\"].astype(str)\n",
    "\n",
    "print(\"数据样本：\")\n",
    "print(df[[\"input_text\", \"target_text\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# 3. 将 DataFrame 转换为 Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# 4. 加载预训练的 T5 翻译模型（t5_translate_en_ru_zh_small_1024）\n",
    "model_name = \"utrobinmv/t5_translate_en_ru_zh_small_1024\"  # 请确认该模型存在并可用\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# 强制使用 CPU\n",
    "device = \"cpu\"\n",
    "model.to(device)\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5ba9e2a3f6042b5bb825bbfb50648cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1258 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 5. 定义预处理函数（分词、截断）\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples[\"input_text\"]\n",
    "    targets = examples[\"target_text\"]\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=512, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# 6. 划分训练集和验证集（例如 90% 训练，10% 验证）\n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8305a491d63d48c39de3f03b9f01a1b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/426 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.274, 'grad_norm': 4.341250419616699, 'learning_rate': 4.882629107981221e-05, 'epoch': 0.07}\n",
      "{'loss': 0.3109, 'grad_norm': 3.8970205783843994, 'learning_rate': 4.765258215962441e-05, 'epoch': 0.14}\n",
      "{'loss': 0.3461, 'grad_norm': 2.294842481613159, 'learning_rate': 4.647887323943662e-05, 'epoch': 0.21}\n",
      "{'loss': 0.3331, 'grad_norm': 5.237137794494629, 'learning_rate': 4.530516431924883e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3874, 'grad_norm': 2.663878917694092, 'learning_rate': 4.413145539906103e-05, 'epoch': 0.35}\n",
      "{'loss': 0.308, 'grad_norm': 2.482166290283203, 'learning_rate': 4.295774647887324e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3301, 'grad_norm': 1.9523558616638184, 'learning_rate': 4.178403755868545e-05, 'epoch': 0.49}\n",
      "{'loss': 0.4168, 'grad_norm': 4.1357855796813965, 'learning_rate': 4.0610328638497654e-05, 'epoch': 0.56}\n",
      "{'loss': 0.4076, 'grad_norm': 2.5412566661834717, 'learning_rate': 3.943661971830986e-05, 'epoch': 0.63}\n",
      "{'loss': 0.3844, 'grad_norm': 2.768216133117676, 'learning_rate': 3.826291079812207e-05, 'epoch': 0.7}\n",
      "{'loss': 0.4567, 'grad_norm': 2.7999677658081055, 'learning_rate': 3.7089201877934274e-05, 'epoch': 0.77}\n",
      "{'loss': 0.4725, 'grad_norm': 2.4680044651031494, 'learning_rate': 3.5915492957746486e-05, 'epoch': 0.85}\n",
      "{'loss': 0.3738, 'grad_norm': 4.975489616394043, 'learning_rate': 3.474178403755869e-05, 'epoch': 0.92}\n",
      "{'loss': 0.5688, 'grad_norm': 4.985090732574463, 'learning_rate': 3.3568075117370895e-05, 'epoch': 0.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4bdd8e3ec504f6d830f513afe31c4fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1283717155456543, 'eval_bleu': 0.4703416941636425, 'eval_runtime': 129.3578, 'eval_samples_per_second': 0.974, 'eval_steps_per_second': 0.124, 'epoch': 1.0}\n",
      "{'loss': 0.8059, 'grad_norm': 9.117254257202148, 'learning_rate': 3.23943661971831e-05, 'epoch': 1.06}\n",
      "{'loss': 0.7815, 'grad_norm': 7.098135471343994, 'learning_rate': 3.1220657276995305e-05, 'epoch': 1.13}\n",
      "{'loss': 0.8091, 'grad_norm': 11.68268871307373, 'learning_rate': 3.0046948356807513e-05, 'epoch': 1.2}\n",
      "{'loss': 0.794, 'grad_norm': 10.478001594543457, 'learning_rate': 2.887323943661972e-05, 'epoch': 1.27}\n",
      "{'loss': 0.7261, 'grad_norm': 5.437191486358643, 'learning_rate': 2.7699530516431926e-05, 'epoch': 1.34}\n",
      "{'loss': 0.9052, 'grad_norm': 10.992965698242188, 'learning_rate': 2.6525821596244134e-05, 'epoch': 1.41}\n",
      "{'loss': 0.8765, 'grad_norm': 9.82922649383545, 'learning_rate': 2.535211267605634e-05, 'epoch': 1.48}\n",
      "{'loss': 0.8974, 'grad_norm': 9.882003784179688, 'learning_rate': 2.4178403755868547e-05, 'epoch': 1.55}\n",
      "{'loss': 0.8853, 'grad_norm': 9.145586013793945, 'learning_rate': 2.300469483568075e-05, 'epoch': 1.62}\n",
      "{'loss': 0.9371, 'grad_norm': 5.170496463775635, 'learning_rate': 2.1830985915492956e-05, 'epoch': 1.69}\n",
      "{'loss': 0.809, 'grad_norm': 10.81254768371582, 'learning_rate': 2.0657276995305167e-05, 'epoch': 1.76}\n",
      "{'loss': 0.8015, 'grad_norm': 6.170129299163818, 'learning_rate': 1.9483568075117372e-05, 'epoch': 1.83}\n",
      "{'loss': 0.7326, 'grad_norm': 4.384148120880127, 'learning_rate': 1.830985915492958e-05, 'epoch': 1.9}\n",
      "{'loss': 0.7446, 'grad_norm': 4.855432987213135, 'learning_rate': 1.7136150234741785e-05, 'epoch': 1.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b74bb72824aa4dbd84c25b65260597e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9961930513381958, 'eval_bleu': 0.5014578047365716, 'eval_runtime': 127.3893, 'eval_samples_per_second': 0.989, 'eval_steps_per_second': 0.126, 'epoch': 2.0}\n",
      "{'loss': 0.7831, 'grad_norm': 9.551262855529785, 'learning_rate': 1.5962441314553993e-05, 'epoch': 2.04}\n",
      "{'loss': 0.6167, 'grad_norm': 4.976161479949951, 'learning_rate': 1.4788732394366198e-05, 'epoch': 2.11}\n",
      "{'loss': 0.6306, 'grad_norm': 6.303422927856445, 'learning_rate': 1.3615023474178404e-05, 'epoch': 2.18}\n",
      "{'loss': 0.5847, 'grad_norm': 5.262000560760498, 'learning_rate': 1.2441314553990612e-05, 'epoch': 2.25}\n",
      "{'loss': 0.7252, 'grad_norm': 3.9484033584594727, 'learning_rate': 1.1267605633802817e-05, 'epoch': 2.32}\n",
      "{'loss': 0.7429, 'grad_norm': 9.201313018798828, 'learning_rate': 1.0093896713615023e-05, 'epoch': 2.39}\n",
      "{'loss': 0.6501, 'grad_norm': 7.421340465545654, 'learning_rate': 8.92018779342723e-06, 'epoch': 2.46}\n",
      "{'loss': 0.8193, 'grad_norm': 11.156578063964844, 'learning_rate': 7.746478873239436e-06, 'epoch': 2.54}\n",
      "{'loss': 0.7219, 'grad_norm': 7.442094802856445, 'learning_rate': 6.572769953051644e-06, 'epoch': 2.61}\n",
      "{'loss': 0.6713, 'grad_norm': 3.094036102294922, 'learning_rate': 5.3990610328638506e-06, 'epoch': 2.68}\n",
      "{'loss': 0.798, 'grad_norm': 6.209805965423584, 'learning_rate': 4.225352112676056e-06, 'epoch': 2.75}\n",
      "{'loss': 0.7616, 'grad_norm': 5.283562183380127, 'learning_rate': 3.051643192488263e-06, 'epoch': 2.82}\n",
      "{'loss': 0.7761, 'grad_norm': 7.25398588180542, 'learning_rate': 1.8779342723004696e-06, 'epoch': 2.89}\n",
      "{'loss': 0.7362, 'grad_norm': 7.192032337188721, 'learning_rate': 7.042253521126761e-07, 'epoch': 2.96}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eb98c0e138941479a04d11c35f12a21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.992005467414856, 'eval_bleu': 0.5073454349054234, 'eval_runtime': 128.7216, 'eval_samples_per_second': 0.979, 'eval_steps_per_second': 0.124, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1555.4569, 'train_samples_per_second': 2.183, 'train_steps_per_second': 0.274, 'train_loss': 0.6439104337647488, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=426, training_loss=0.6439104337647488, metrics={'train_runtime': 1555.4569, 'train_samples_per_second': 2.183, 'train_steps_per_second': 0.274, 'total_flos': 104822176628736.0, 'train_loss': 0.6439104337647488, 'epoch': 3.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. 加载 BLEU 评估指标（使用 evaluate 库）\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "def safe_decode(sequences):\n",
    "    \"\"\"\n",
    "    对 token 序列进行安全解码：将超出 tokenizer 词汇表范围的 token 替换为 pad token。\n",
    "    \"\"\"\n",
    "    max_id = tokenizer.vocab_size\n",
    "    safe_sequences = []\n",
    "    for seq in sequences:\n",
    "        safe_seq = [token if (0 <= token < max_id) else tokenizer.pad_token_id for token in seq]\n",
    "        safe_sequences.append(safe_seq)\n",
    "    return tokenizer.batch_decode(safe_sequences, skip_special_tokens=True)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # 这里先对预测值和标签进行安全处理\n",
    "    decoded_preds = safe_decode(predictions)\n",
    "    \n",
    "    # 替换 -100 为 pad token id，并安全处理标签\n",
    "    labels = [[(l if l != -100 else tokenizer.pad_token_id) for l in label] for label in labels]\n",
    "    decoded_labels = safe_decode(labels)\n",
    "    # BLEU 指标要求参考翻译是列表的列表\n",
    "    decoded_labels = [[ref] for ref in decoded_labels]\n",
    "\n",
    "    result = bleu.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"bleu\": result[\"bleu\"]}\n",
    "\n",
    "\n",
    "# def compute_metrics(eval_pred):\n",
    "#     predictions, labels = eval_pred\n",
    "#     decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "#     # 替换 -100 并解码标签\n",
    "#     labels = [[(l if l != -100 else tokenizer.pad_token_id) for l in label] for label in labels]\n",
    "#     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "#     decoded_labels = [[ref] for ref in decoded_labels]\n",
    "#     result = bleu.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "#     return {\"bleu\": result[\"bleu\"]}\n",
    "\n",
    "# def compute_metrics(eval_pred):\n",
    "#     predictions, labels = eval_pred\n",
    "#     print(\"原始预测值：\", predictions)\n",
    "#     print(\"原始标签值：\", labels)\n",
    "\n",
    "#     decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "#     # 确保 labels 形状正确\n",
    "#     labels = [[(l if l != -100 else tokenizer.pad_token_id) for l in label] for label in labels]\n",
    "    \n",
    "#     # 确保 labels 不是空的\n",
    "#     if not labels or not any(labels):\n",
    "#         return {\"bleu\": 0.0}\n",
    "\n",
    "#     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "#     decoded_labels = [[ref] for ref in decoded_labels]\n",
    "\n",
    "#     print(\"解码后的预测值：\", decoded_preds)\n",
    "#     print(\"解码后的标签值：\", decoded_labels)\n",
    "\n",
    "#     result = bleu.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "#     return {\"bleu\": result[\"bleu\"]}\n",
    "\n",
    "\n",
    "# def compute_metrics(eval_pred):\n",
    "#     predictions, labels = eval_pred\n",
    "#     decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "#     # 确保 labels 形状正确\n",
    "#     labels = [[(l if l != -100 else tokenizer.pad_token_id) for l in label] for label in labels]\n",
    "\n",
    "#     # 确保 labels 不是空的\n",
    "#     if not labels or not any(labels):\n",
    "#         return {\"bleu\": 0.0}\n",
    "\n",
    "#     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "#     decoded_labels = [[ref] for ref in decoded_labels]\n",
    "\n",
    "#     result = bleu.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "#     return {\"bleu\": result[\"bleu\"]}\n",
    "\n",
    "# 8. 定义训练参数\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./finetuned_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    predict_with_generate=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"bleu\",\n",
    "    greater_is_better=True,\n",
    ")\n",
    "\n",
    "# data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, label_pad_token_id=-100)\n",
    "\n",
    "# 9. 初始化 Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# print(f\"训练集大小: {len(train_dataset)}\")\n",
    "# print(f\"验证集大小: {len(eval_dataset)}\")\n",
    "\n",
    "# 10. 开始微调训练\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15c13c46027d43b1a9997847c9e32438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "评估结果： {'eval_loss': 0.992005467414856, 'eval_bleu': 0.5073454349054234, 'eval_runtime': 129.0013, 'eval_samples_per_second': 0.977, 'eval_steps_per_second': 0.124, 'epoch': 3.0}\n",
      "微调后的模型已保存到 ./finetuned_model\n"
     ]
    }
   ],
   "source": [
    "# 10. 评估模型效果\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"评估结果：\", eval_results)\n",
    "\n",
    "# 11. 保存微调后的模型\n",
    "trainer.save_model(\"./finetuned_model\")\n",
    "print(\"微调后的模型已保存到 ./finetuned_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【步骤2】实现 Python 文件中文注释翻译并生成新 .py 文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【步骤3】评估翻译质量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 1.0\n"
     ]
    }
   ],
   "source": [
    "# 示例：使用 evaluate 库评估一批翻译结果\n",
    "def evaluate_translations(references, predictions):\n",
    "    \"\"\"\n",
    "    references: 列表，每个元素为参考翻译文本（俄文）\n",
    "    predictions: 列表，每个元素为模型生成的翻译文本（俄文）\n",
    "    \"\"\"\n",
    "    bleu_metric = evaluate.load(\"bleu\")\n",
    "    # BLEU 期望参考文本为列表的列表\n",
    "    ref_lists = [[ref] for ref in references]\n",
    "    results = bleu_metric.compute(predictions=predictions, references=ref_lists)\n",
    "    print(\"BLEU score:\", results[\"bleu\"])\n",
    "\n",
    "# 示例调用：\n",
    "refs = [\"Цель разработки - предоставить персонализированный синхронный перевод для пользователей.\"]\n",
    "preds = [\"Цель разработки - предоставить персонализированный синхронный перевод для пользователей.\"]\n",
    "evaluate_translations(refs, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 加载所需库\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import evaluate\n",
    "from transformers import MarianMTModel, MarianTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>code</th>\n",
       "      <th>code_comment_type</th>\n",
       "      <th>comment_zh</th>\n",
       "      <th>comment_ru</th>\n",
       "      <th>comment_en</th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\datas...</td>\n",
       "      <td>import numpy as np\\nfrom scipy.stats import tt...</td>\n",
       "      <td>Single-line</td>\n",
       "      <td>用于两个样本分位数的自助法t检验</td>\n",
       "      <td>t-тест самообслуживания для двух квартилей выб...</td>\n",
       "      <td>Self-service t-test for two quartiles of the s...</td>\n",
       "      <td>用于两个样本分位数的自助法t检验</td>\n",
       "      <td>t-тест самообслуживания для двух квартилей выб...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\datas...</td>\n",
       "      <td>import numpy as np\\nfrom scipy.stats import tt...</td>\n",
       "      <td>Single-line</td>\n",
       "      <td>计算控制组和实验组的自助法分位数</td>\n",
       "      <td>Рассчитать квантили метода самообслуживания дл...</td>\n",
       "      <td>Calculate the self-help division of the contro...</td>\n",
       "      <td>计算控制组和实验组的自助法分位数</td>\n",
       "      <td>Рассчитать квантили метода самообслуживания дл...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\datas...</td>\n",
       "      <td>import numpy as np\\nfrom scipy.stats import tt...</td>\n",
       "      <td>Single-line</td>\n",
       "      <td>对自助法分位数进行t检验</td>\n",
       "      <td>Провести t-тест на квантиль самообслуживания</td>\n",
       "      <td>t-test self-help splits</td>\n",
       "      <td>对自助法分位数进行t检验</td>\n",
       "      <td>Провести t-тест на квантиль самообслуживания</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\datas...</td>\n",
       "      <td>import numpy as np\\nfrom scipy.stats import tt...</td>\n",
       "      <td>Single-line</td>\n",
       "      <td>判断是否拒绝原假设</td>\n",
       "      <td>Определение того, отклонять или нет первоначал...</td>\n",
       "      <td>Determining whether or not to reject the origi...</td>\n",
       "      <td>判断是否拒绝原假设</td>\n",
       "      <td>Определение того, отклонять или нет первоначал...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\datas...</td>\n",
       "      <td>import numpy as np\\nfrom scipy.stats import tt...</td>\n",
       "      <td>Multi-line</td>\n",
       "      <td>返回使用自助抽样法计算的分位数分布。</td>\n",
       "      <td>Возвращает квантильное распределение, рассчита...</td>\n",
       "      <td>Returns the fractional distribution calculated...</td>\n",
       "      <td>返回使用自助抽样法计算的分位数分布。</td>\n",
       "      <td>Возвращает квантильное распределение, рассчита...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_path  \\\n",
       "0  C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\datas...   \n",
       "1  C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\datas...   \n",
       "2  C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\datas...   \n",
       "3  C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\datas...   \n",
       "4  C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\datas...   \n",
       "\n",
       "                                                code code_comment_type  \\\n",
       "0  import numpy as np\\nfrom scipy.stats import tt...       Single-line   \n",
       "1  import numpy as np\\nfrom scipy.stats import tt...       Single-line   \n",
       "2  import numpy as np\\nfrom scipy.stats import tt...       Single-line   \n",
       "3  import numpy as np\\nfrom scipy.stats import tt...       Single-line   \n",
       "4  import numpy as np\\nfrom scipy.stats import tt...        Multi-line   \n",
       "\n",
       "           comment_zh                                         comment_ru  \\\n",
       "0    用于两个样本分位数的自助法t检验  t-тест самообслуживания для двух квартилей выб...   \n",
       "1    计算控制组和实验组的自助法分位数  Рассчитать квантили метода самообслуживания дл...   \n",
       "2        对自助法分位数进行t检验       Провести t-тест на квантиль самообслуживания   \n",
       "3           判断是否拒绝原假设  Определение того, отклонять или нет первоначал...   \n",
       "4  返回使用自助抽样法计算的分位数分布。  Возвращает квантильное распределение, рассчита...   \n",
       "\n",
       "                                          comment_en          input_text  \\\n",
       "0  Self-service t-test for two quartiles of the s...    用于两个样本分位数的自助法t检验   \n",
       "1  Calculate the self-help division of the contro...    计算控制组和实验组的自助法分位数   \n",
       "2                            t-test self-help splits        对自助法分位数进行t检验   \n",
       "3  Determining whether or not to reject the origi...           判断是否拒绝原假设   \n",
       "4  Returns the fractional distribution calculated...  返回使用自助抽样法计算的分位数分布。   \n",
       "\n",
       "                                         target_text  \n",
       "0  t-тест самообслуживания для двух квартилей выб...  \n",
       "1  Рассчитать квантили метода самообслуживания дл...  \n",
       "2       Провести t-тест на квантиль самообслуживания  \n",
       "3  Определение того, отклонять или нет первоначал...  \n",
       "4  Возвращает квантильное распределение, рассчита...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. 加载 CSV 数据集\n",
    "\n",
    "csv_path = r\"C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\datasets\\data.csv\"  # 替换为你的 CSV 文件路径\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# 假设 CSV 中的列名为： \"file_path\", \"code\", \"code_comment_type\", \"comment_zh\", \"comment_ru\", \"comment_en\"\n",
    "# 本例中我们以 comment_zh 作为输入、comment_ru 作为目标；你也可以考虑加入代码信息作为上下文\n",
    "df[\"input_text\"] = df[\"comment_zh\"]  # 可根据需要扩展，如 \"代码: \" + df[\"code\"] + \"\\n注释: \" + df[\"comment_zh\"]\n",
    "df[\"target_text\"] = df[\"comment_ru\"]\n",
    "\n",
    "# 查看数据样本\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['file_path', 'code', 'code_comment_type', 'comment_zh', 'comment_ru', 'comment_en', 'input_text', 'target_text'],\n",
       "    num_rows: 1258\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3. 将 Pandas DataFrame 转换为 Hugging Face 的 Dataset 格式\n",
    "\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Это испытательное предложение.', 'Мы перевели китайский на русский.']\n"
     ]
    }
   ],
   "source": [
    "# 4. 加载预训练的翻译模型\n",
    "\n",
    "# 中文到英文模型\n",
    "model_name_zh_en = \"Helsinki-NLP/opus-mt-zh-en\"\n",
    "tokenizer_zh_en = MarianTokenizer.from_pretrained(model_name_zh_en)\n",
    "model_zh_en = MarianMTModel.from_pretrained(model_name_zh_en)\n",
    "\n",
    "# 英文到俄文模型\n",
    "model_name_en_ru = \"Helsinki-NLP/opus-mt-en-ru\"\n",
    "tokenizer_en_ru = MarianTokenizer.from_pretrained(model_name_en_ru)\n",
    "model_en_ru = MarianMTModel.from_pretrained(model_name_en_ru)\n",
    "\n",
    "def translate_zh_to_ru(text_zh):\n",
    "    # 第一步：中文翻译成英文\n",
    "    inputs_zh_en = tokenizer_zh_en(text_zh, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    translated_en = model_zh_en.generate(**inputs_zh_en)\n",
    "    text_en = [tokenizer_zh_en.decode(t, skip_special_tokens=True) for t in translated_en]\n",
    "\n",
    "    # 第二步：英文翻译成俄文\n",
    "    inputs_en_ru = tokenizer_en_ru(text_en, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    translated_ru = model_en_ru.generate(**inputs_en_ru)\n",
    "    text_ru = [tokenizer_en_ru.decode(t, skip_special_tokens=True) for t in translated_ru]\n",
    "\n",
    "    return text_ru\n",
    "\n",
    "# 示例\n",
    "text_zh = [\"这是一个测试句子。\", \"我们将中文翻译成俄文。\"]\n",
    "translated_text = translate_zh_to_ru(text_zh)\n",
    "print(translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Helsinki-NLP/opus-mt-zh-ru is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\utils\\_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 406\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/Helsinki-NLP/opus-mt-zh-ru/resolve/main/tokenizer_config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\transformers\\utils\\hub.py:399\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    398\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 399\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:862\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[0;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[0;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[0;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[0;32m    877\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:969\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[1;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[0;32m    968\u001b[0m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[1;32m--> 969\u001b[0m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:1484\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[1;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[0;32m   1482\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[0;32m   1483\u001b[0m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[1;32m-> 1484\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1486\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:1376\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[1;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1376\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1377\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\n\u001b[0;32m   1378\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1379\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:1296\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[1;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[0;32m   1295\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[1;32m-> 1296\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1298\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1304\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1305\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:277\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[1;32m--> 277\u001b[0m     response \u001b[38;5;241m=\u001b[39m _request_wrapper(\n\u001b[0;32m    278\u001b[0m         method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    279\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    280\u001b[0m         follow_relative_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    282\u001b[0m     )\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:301\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    300\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m--> 301\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\utils\\_http.py:454\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    446\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    447\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    448\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you are authenticated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    453\u001b[0m     )\n\u001b[1;32m--> 454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(RepositoryNotFoundError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n",
      "\u001b[1;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-67a6a474-0cde04f71cc8dbbf11a8e1eb;dca148b7-c40d-4e71-b35a-f55fcc961b9e)\n\nRepository Not Found for url: https://huggingface.co/Helsinki-NLP/opus-mt-zh-ru/resolve/main/tokenizer_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 4. 加载预训练的翻译模型（此处使用 Helsinki-NLP/opus-mt-zh-ru，如果没有可考虑其他适合的模型）\u001b[39;00m\n\u001b[0;32m      3\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHelsinki-NLP/opus-mt-zh-ru\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# 请确保此模型存在；如果没有，可以考虑微调现有的中文->俄文翻译模型\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mMarianTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m MarianMTModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2029\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2026\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer_file\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m vocab_files:\n\u001b[0;32m   2027\u001b[0m     \u001b[38;5;66;03m# Try to get the tokenizer config to see if there are versioned tokenizer files.\u001b[39;00m\n\u001b[0;32m   2028\u001b[0m     fast_tokenizer_file \u001b[38;5;241m=\u001b[39m FULL_TOKENIZER_FILE\n\u001b[1;32m-> 2029\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2030\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2031\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2032\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2033\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2034\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2035\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2036\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2037\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2038\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2039\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2040\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2041\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   2042\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   2043\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   2044\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2045\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2046\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[0;32m   2047\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\transformers\\utils\\hub.py:422\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    418\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to have access to it at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    419\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    420\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 422\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    423\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    424\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    425\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    426\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token=<your_token>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    427\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RevisionNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    430\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    431\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    432\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    433\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: Helsinki-NLP/opus-mt-zh-ru is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
     ]
    }
   ],
   "source": [
    "# 4. 加载预训练的翻译模型（此处使用 Helsinki-NLP/opus-mt-zh-ru，如果没有可考虑其他适合的模型）\n",
    "\n",
    "model_name = \"Helsinki-NLP/opus-mt-zh-ru\"  # 请确保此模型存在；如果没有，可以考虑微调现有的中文->俄文翻译模型\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "008d99b0a6a041d79cf0d85ae57a60d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/44.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gdnjr5233_YOLO\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in G:\\huggingface_cache\\hub\\models--Helsinki-NLP--opus-mt-mul-en. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62bc525395084ef7a5421d419e99fb51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source.spm:   0%|          | 0.00/707k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43c3e61b30c741d5a2c6cf2a202bdac2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target.spm:   0%|          | 0.00/791k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d06a0dbfb39495da55e0e75de68e4c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gdnjr5233_YOLO\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08025284dce14631b1d01ffef37c9497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdebba58a70d4a6fa75e99a5db444603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/310M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cafedf45282496a99e9bd89f2147cde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"It's a test sentence.\", 'We translate Chinese into Russian.']\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# 使用多语言模型：支持中文到俄文的翻译\n",
    "model_name = \"Helsinki-NLP/opus-mt-mul-en\"  # 示例多语言模型\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "def translate_zh_to_ru_multilingual(text_zh):\n",
    "    # 给输入文本添加目标语言代码 \">>ru<<\"\n",
    "    text_with_language_code = [\">>ru<< \" + text for text in text_zh]\n",
    "\n",
    "    # 进行翻译\n",
    "    inputs = tokenizer(text_with_language_code, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    translated = model.generate(**inputs)\n",
    "    text_ru = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
    "\n",
    "    return text_ru\n",
    "\n",
    "# 示例\n",
    "text_zh = [\"这是一个测试句子。\", \"我们将中文翻译成俄文。\"]\n",
    "translated_text = translate_zh_to_ru_multilingual(text_zh)\n",
    "print(translated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81b7823354cb495c96496199808aaca3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/44.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gdnjr5233_YOLO\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in G:\\huggingface_cache\\hub\\models--Helsinki-NLP--opus-mt-en-mul. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90461a1450af4e4ab1b6ed6d5ffd8270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source.spm:   0%|          | 0.00/790k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b21942e4743a49889fbe4e57d0d9b5ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target.spm:   0%|          | 0.00/707k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85a6c9a46a0c4f1f91e4ea5d5231f2e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gdnjr5233_YOLO\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b7b1998797047aca2177b10c6c4ab49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['>>ewe<<', '>>sna<<', '>>lin<<', '>>toi_Latn<<', '>>ceb<<', '>>oss<<', '>>run<<', '>>mfe<<', '>>ilo<<', '>>zlm_Latn<<', '>>pes<<', '>>smo<<', '>>hil<<', '>>niu<<', '>>sag<<', '>>fij<<', '>>cmn_Hans<<', '>>nya<<', '>>tso<<', '>>war<<', '>>gil<<', '>>hau_Latn<<', '>>umb<<', '>>glv<<', '>>tvl<<', '>>ton<<', '>>zul<<', '>>kal<<', '>>pag<<', '>>cmn_Hant<<', '>>pus<<', '>>abk<<', '>>pap<<', '>>hat<<', '>>mkd<<', '>>tuk_Latn<<', '>>yor<<', '>>tuk<<', '>>sqi<<', '>>tir<<', '>>mlg<<', '>>tur<<', '>>ido_Latn<<', '>>mai<<', '>>ibo<<', '>>srp_Cyrl<<', '>>srp_Latn<<', '>>kir_Cyrl<<', '>>heb<<', '>>bos_Latn<<', '>>bak<<', '>>ast<<', '>>som<<', '>>tah<<', '>>chv<<', '>>kek_Latn<<', '>>lug<<', '>>vie<<', '>>wln<<', '>>isl<<', '>>hye<<', '>>mah<<', '>>yue_Hant<<', '>>crh_Latn<<', '>>amh<<', '>>nds<<', '>>pan_Guru<<', '>>xho<<', '>>ukr<<', '>>cat<<', '>>afr<<', '>>tat<<', '>>guj<<', '>>jpn<<', '>>mon<<', '>>eus<<', '>>nob<<', '>>glg<<', '>>ind<<', '>>sin<<', '>>cym<<', '>>zho_Hant<<', '>>zho_Hans<<', '>>tgk_Cyrl<<', '>>aze_Latn<<', '>>ltz<<', '>>bod<<', '>>asm<<', '>>tel<<', '>>urd<<', '>>kaz_Cyrl<<', '>>lat_Latn<<', '>>gla<<', '>>kan<<', '>>bul<<', '>>kin<<', '>>ina_Latn<<', '>>ron<<', '>>spa<<', '>>csb_Latn<<', '>>iba<<', '>>tha<<', '>>nno<<', '>>hrv<<', '>>fry<<', '>>bre<<', '>>mar<<', '>>sme<<', '>>swe<<', '>>deu<<', '>>jav<<', '>>snd_Arab<<', '>>ben<<', '>>cmn<<', '>>ces<<', '>>ita<<', '>>fin<<', '>>por<<', '>>hin<<', '>>hun<<', '>>mal<<', '>>pol<<', '>>fra<<', '>>nld<<', '>>epo<<', '>>slv<<', '>>hsb<<', '>>kur_Latn<<', '>>ori<<', '>>tam<<', '>>bel<<', '>>dan<<', '>>ara<<', '>>mya<<', '>>rus<<', '>>mri<<', '>>est<<', '>>uzb_Latn<<', '>>lao<<', '>>yid<<', '>>uzb_Cyrl<<', '>>uig_Arab<<', '>>lit<<', '>>zho<<', '>>lav<<', '>>ell<<', '>>kat<<', '>>gle<<', '>>mlt<<', '>>khm<<', '>>oci<<', '>>kur_Arab<<', '>>ang_Latn<<', '>>kaz_Latn<<', '>>wol<<', '>>sun<<', '>>chr<<', '>>tat_Latn<<', '>>mhr<<', '>>tyv<<', '>>rom<<', '>>cha<<', '>>kab<<', '>>nav<<', '>>arg<<', '>>khm_Latn<<', '>>bul_Latn<<', '>>udm<<', '>>quc<<', '>>cor<<', '>>san_Deva<<', '>>fao<<', '>>bel_Latn<<', '>>jbo_Latn<<', '>>yue<<', '>>grn<<', '>>sco<<', '>>arq<<', '>>ltg<<', '>>yue_Hans<<', '>>min<<', '>>nan<<', '>>bam_Latn<<', '>>ido<<', '>>ile_Latn<<', '>>wuu<<', '>>crh<<', '>>tlh_Latn<<', '>>lzh<<', '>>jbo<<', '>>lzh_Hans<<', '>>vol_Latn<<', '>>lfn_Latn<<', '>>arz<<']\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianTokenizer\n",
    "\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-mul\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 打印支持的语言代码\n",
    "print(tokenizer.supported_language_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 提取 Python 文件中的注释，并保留代码的原始行结构\n",
    "def extract_comments(file_path):\n",
    "    \"\"\"从文件中提取所有单行和多行注释，返回原始代码按行分割的列表，以及注释信息\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()  # 逐行读取，确保保留空行\n",
    "\n",
    "    comments = []\n",
    "    code_lines = []\n",
    "\n",
    "    multi_line_comment = False  # 标记是否处于多行注释块中\n",
    "    multi_line_content = []  # 存储多行注释内容\n",
    "    multi_line_indent = \"\"  # 存储多行注释的缩进\n",
    "\n",
    "    for line in lines:\n",
    "        code_lines.append(line)  # 记录完整代码（包括换行符）\n",
    "\n",
    "        # 处理单行注释（包含在代码行后的情况）\n",
    "        single_comment_match = re.match(r\"^(.*?)\\s*(#.*)\", line)\n",
    "        if single_comment_match:\n",
    "            code_part = single_comment_match.group(1)\n",
    "            comment_part = single_comment_match.group(2)\n",
    "            if re.search(\"[\\u4e00-\\u9fff]\", comment_part):  # 只翻译包含中文的注释\n",
    "                comments.append((comment_part, \"Single-line\", single_comment_match.group(1)))  # 返回3元组\n",
    "            continue\n",
    "\n",
    "        # 处理多行注释\n",
    "        multi_match = re.match(r\"(\\s*)(['\\\"]{3})\", line)  # 识别多行注释的开始\n",
    "        if multi_match:\n",
    "            if not multi_line_comment:  # 进入多行注释\n",
    "                multi_line_comment = True\n",
    "                multi_line_indent = multi_match.group(1)  # 记录缩进\n",
    "                multi_line_content = [line]\n",
    "            else:  # 结束多行注释\n",
    "                multi_line_comment = False\n",
    "                multi_line_content.append(line)\n",
    "                comments.append((\"\".join(multi_line_content), \"Multi-line\", multi_line_indent))  # 返回3元组\n",
    "            continue\n",
    "        \n",
    "        if multi_line_comment:\n",
    "            multi_line_content.append(line)\n",
    "\n",
    "    return code_lines, comments\n",
    "\n",
    "# 翻译注释（中文 -> 俄文）\n",
    "def translate_comment(text, prefix=\"translate to ru: \"):\n",
    "    \"\"\"使用微调模型翻译中文注释\"\"\"\n",
    "    input_text = prefix + text\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    generated_tokens = model.generate(**inputs, max_length=512)\n",
    "    translated = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "    return translated[0]\n",
    "\n",
    "# 替换代码中的中文注释，保持原有格式（缩进 & 空行）\n",
    "def replace_comments_in_code(code_lines, comments_translations):\n",
    "    \"\"\"\n",
    "    code_lines: 原始代码（按行分割）\n",
    "    comments_translations: 列表，包含 (原始注释, 翻译后注释, 注释类型, 缩进)\n",
    "    \"\"\"\n",
    "    new_code_lines = code_lines[:]  # 复制代码列表，避免修改原始数据\n",
    "    \n",
    "    for orig, trans, ctype, indent in comments_translations:\n",
    "        if ctype == \"Single-line\":\n",
    "            # 确保保留缩进并替换\n",
    "            for i, line in enumerate(new_code_lines):\n",
    "                if orig in line:\n",
    "                    # 替换时去除重复的“#”\n",
    "                    new_code_lines[i] = line.replace(orig, indent + \"# \" + trans, 1)\n",
    "                    break\n",
    "        \n",
    "        elif ctype == \"Multi-line\":\n",
    "            orig_lines = orig.split(\"\\n\")\n",
    "            trans_lines = trans.split(\"\\n\")\n",
    "            trans_lines = [indent + line for line in trans_lines]  # 保持缩进\n",
    "            for i in range(len(new_code_lines) - len(orig_lines) + 1):\n",
    "                if new_code_lines[i:i+len(orig_lines)] == orig_lines:\n",
    "                    new_code_lines[i:i+len(orig_lines)] = trans_lines\n",
    "                    break\n",
    "\n",
    "    return new_code_lines\n",
    "\n",
    "# 处理 Python 文件，翻译中文注释为俄语\n",
    "def process_python_file(input_file_path, output_file_path):\n",
    "    # 提取代码及注释\n",
    "    code_lines, comments = extract_comments(input_file_path)\n",
    "    \n",
    "    # 翻译注释\n",
    "    comments_translations = []\n",
    "    for comment, ctype, indent in comments:\n",
    "        if re.search(\"[\\u4e00-\\u9fff]\", comment):  # 只翻译包含中文的注释\n",
    "            trans = translate_comment(comment)\n",
    "            comments_translations.append((comment, trans, ctype, indent))  # 保证返回4元组\n",
    "        else:\n",
    "            comments_translations.append((comment, comment, ctype, indent))  # 保持非中文注释不变\n",
    "    \n",
    "    # 替换代码中的注释\n",
    "    new_code_lines = replace_comments_in_code(code_lines, comments_translations)\n",
    "    \n",
    "    # 保存新文件，确保换行符保持不变\n",
    "    with open(output_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.writelines(new_code_lines)\n",
    "    \n",
    "    print(f\"新文件已生成：{output_file_path}\")\n",
    "\n",
    "# 示例：处理单个 Python 文件\n",
    "input_file = r\"C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\c2e2r.py\"  # 替换为待翻译的文件路径\n",
    "output_file = r\"C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\translated_test_6.py\"  # 替换为生成的输出文件路径\n",
    "process_python_file(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "# 提取 Python 文件中的注释，并保留代码的原始行结构\n",
    "def extract_comments(file_path):\n",
    "    \"\"\"从文件中提取所有单行和多行注释，返回原始代码按行分割的列表，以及注释信息\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "        lines = f.readlines()  # 逐行读取，确保保留空行\n",
    "\n",
    "    comments = []\n",
    "    code_lines = []\n",
    "\n",
    "    multi_line_comment = False  # 标记是否处于多行注释块中\n",
    "    multi_line_content = []  # 存储多行注释内容\n",
    "    multi_line_delimiter = \"\"  # 存储多行注释的分隔符\n",
    "    multi_line_indent = \"\"  # 存储多行注释的缩进\n",
    "\n",
    "    for line in lines:\n",
    "        code_lines.append(line)  # 记录完整代码（包括换行符）\n",
    "\n",
    "        # 处理单行注释（包含在代码行后的情况）\n",
    "        single_comment_match = re.match(r\"^(.*?)\\s*(#.*)\", line)\n",
    "        if single_comment_match:\n",
    "            code_part = single_comment_match.group(1)\n",
    "            comment_part = single_comment_match.group(2)\n",
    "            if re.search(\"[\\u4e00-\\u9fff]\", comment_part):  # 只翻译包含中文的注释\n",
    "                comments.append((comment_part, \"Single-line\", single_comment_match.group(1)))\n",
    "            continue\n",
    "\n",
    "        # 处理多行注释\n",
    "        multi_match = re.match(r\"(\\s*)(['\\\"]{3})\", line)  # 识别多行注释的开始\n",
    "        if multi_match:\n",
    "            if not multi_line_comment:  # 进入多行注释\n",
    "                multi_line_comment = True\n",
    "                multi_line_delimiter = multi_match.group(2)\n",
    "                multi_line_indent = multi_match.group(1)  # 记录缩进\n",
    "                multi_line_content = [line]\n",
    "            else:  # 结束多行注释\n",
    "                multi_line_comment = False\n",
    "                multi_line_content.append(line)\n",
    "                comments.append((\"\".join(multi_line_content), \"Multi-line\", multi_line_indent))\n",
    "            continue\n",
    "        \n",
    "        if multi_line_comment:\n",
    "            multi_line_content.append(line)\n",
    "\n",
    "    return code_lines, comments\n",
    "\n",
    "# 翻译注释（中文 -> 俄文）\n",
    "def translate_comment(text, prefix=\"translate to ru: \"):\n",
    "    \"\"\"使用微调模型翻译中文注释\"\"\"\n",
    "    input_text = prefix + text\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    generated_tokens = model.generate(**inputs, max_length=512)\n",
    "    translated = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "    return translated[0]\n",
    "\n",
    "# 替换代码中的中文注释，保持原有格式（缩进 & 空行）\n",
    "def replace_comments_in_code(code_lines, comments_translations):\n",
    "    \"\"\"\n",
    "    code_lines: 原始代码（按行分割）\n",
    "    comments_translations: 列表，包含 (原始注释, 翻译后注释, 注释类型, 缩进)\n",
    "    \"\"\"\n",
    "    new_code_lines = code_lines[:]  # 复制代码列表，避免修改原始数据\n",
    "    \n",
    "    for orig, trans, ctype, indent in comments_translations:\n",
    "        if ctype == \"Single-line\":\n",
    "            # 确保保留缩进并替换\n",
    "            for i, line in enumerate(new_code_lines):\n",
    "                if orig in line:\n",
    "                    new_code_lines[i] = line.replace(orig, indent + \"# \" + trans, 1)\n",
    "                    break\n",
    "        \n",
    "        elif ctype == \"Multi-line\":\n",
    "            orig_lines = orig.split(\"\\n\")\n",
    "            trans_lines = trans.split(\"\\n\")\n",
    "            trans_lines = [indent + line for line in trans_lines]  # 保持缩进\n",
    "            for i in range(len(new_code_lines) - len(orig_lines) + 1):\n",
    "                if new_code_lines[i:i+len(orig_lines)] == orig_lines:\n",
    "                    new_code_lines[i:i+len(orig_lines)] = trans_lines\n",
    "                    break\n",
    "\n",
    "    return new_code_lines\n",
    "\n",
    "# 处理 Python 文件，翻译中文注释为俄语\n",
    "def process_python_file(input_file_path, output_file_path):\n",
    "    # 提取代码及注释\n",
    "    code_lines, comments = extract_comments(input_file_path)\n",
    "    \n",
    "    # 翻译注释\n",
    "    comments_translations = []\n",
    "    for comment, ctype, indent in comments:\n",
    "        if re.search(\"[\\u4e00-\\u9fff]\", comment):  # 只翻译包含中文的注释\n",
    "            trans = translate_comment(comment)\n",
    "            comments_translations.append((comment, trans, ctype, indent))\n",
    "        else:\n",
    "            comments_translations.append((comment, comment, ctype, indent))  # 保持非中文注释不变\n",
    "    \n",
    "    # 替换代码中的注释\n",
    "    new_code_lines = replace_comments_in_code(code_lines, comments_translations)\n",
    "    \n",
    "    # 保存新文件，确保换行符保持不变\n",
    "    with open(output_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.writelines(new_code_lines)\n",
    "    \n",
    "    print(f\"新文件已生成：{output_file_path}\")\n",
    "\n",
    "# 示例：处理单个 Python 文件\n",
    "input_file = r\"C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\c2e2r.py\"  # 替换为待翻译的文件路径\n",
    "output_file = r\"C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\translated_test_5.py\"  # 替换为生成的输出文件路径\n",
    "process_python_file(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 提取 Python 文件中的注释，并保留代码的原始行结构\n",
    "def extract_comments(file_path):\n",
    "    \"\"\"从文件中提取所有单行和多行注释，返回原始代码按行分割的列表，以及注释信息\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "        lines = f.readlines()  # 逐行读取，确保保留空行\n",
    "    \n",
    "    comments = []\n",
    "    code_lines = []\n",
    "    \n",
    "    multi_line_comment = False  # 标记是否处于多行注释块中\n",
    "    multi_line_content = []  # 存储多行注释内容\n",
    "    multi_line_delimiter = \"\"  # 存储多行注释的分隔符\n",
    "    multi_line_indent = \"\"  # 存储多行注释的缩进\n",
    "\n",
    "    for line in lines:\n",
    "        code_lines.append(line)  # 记录完整代码（包括换行符）\n",
    "\n",
    "        # 处理单行注释\n",
    "        single_comment_match = re.match(r\"(\\s*#.*)\", line)\n",
    "        if single_comment_match:\n",
    "            comments.append((single_comment_match.group(1), \"Single-line\", single_comment_match.group(1).lstrip()))\n",
    "            continue\n",
    "\n",
    "        # 处理多行注释\n",
    "        multi_match = re.match(r\"(\\s*)(['\\\"]{3})\", line)  # 识别多行注释的开始\n",
    "        if multi_match:\n",
    "            if not multi_line_comment:  # 进入多行注释\n",
    "                multi_line_comment = True\n",
    "                multi_line_delimiter = multi_match.group(2)\n",
    "                multi_line_indent = multi_match.group(1)  # 记录缩进\n",
    "                multi_line_content = [line]\n",
    "            else:  # 结束多行注释\n",
    "                multi_line_comment = False\n",
    "                multi_line_content.append(line)\n",
    "                comments.append((\"\".join(multi_line_content), \"Multi-line\", multi_line_indent))\n",
    "            continue\n",
    "        \n",
    "        if multi_line_comment:\n",
    "            multi_line_content.append(line)\n",
    "\n",
    "    return code_lines, comments\n",
    "\n",
    "# 翻译注释（中文 -> 俄文）\n",
    "def translate_comment(text, prefix=\"translate to ru: \"):\n",
    "    \"\"\"使用微调模型翻译中文注释\"\"\"\n",
    "    input_text = prefix + text\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    generated_tokens = model.generate(**inputs, max_length=512)\n",
    "    translated = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "    return translated[0]\n",
    "\n",
    "# 替换代码中的中文注释，保持原有格式（缩进 & 空行）\n",
    "def replace_comments_in_code(code_lines, comments_translations):\n",
    "    \"\"\"\n",
    "    code_lines: 原始代码（按行分割）\n",
    "    comments_translations: 列表，包含 (原始注释, 翻译后注释, 注释类型, 缩进)\n",
    "    \"\"\"\n",
    "    new_code_lines = code_lines[:]  # 复制代码列表，避免修改原始数据\n",
    "    \n",
    "    for orig, trans, ctype, indent in comments_translations:\n",
    "        if ctype == \"Single-line\":\n",
    "            # 确保保留缩进\n",
    "            for i, line in enumerate(new_code_lines):\n",
    "                if orig in line:\n",
    "                    new_code_lines[i] = line.replace(orig, indent + trans, 1)\n",
    "                    break\n",
    "        \n",
    "        elif ctype == \"Multi-line\":\n",
    "            orig_lines = orig.split(\"\\n\")\n",
    "            trans_lines = trans.split(\"\\n\")\n",
    "            trans_lines = [indent + line for line in trans_lines]  # 保持缩进\n",
    "            for i in range(len(new_code_lines) - len(orig_lines) + 1):\n",
    "                if new_code_lines[i:i+len(orig_lines)] == orig_lines:\n",
    "                    new_code_lines[i:i+len(orig_lines)] = trans_lines\n",
    "                    break\n",
    "\n",
    "    return new_code_lines\n",
    "\n",
    "# 处理 Python 文件，翻译中文注释为俄语\n",
    "def process_python_file(input_file_path, output_file_path):\n",
    "    # 提取代码及注释\n",
    "    code_lines, comments = extract_comments(input_file_path)\n",
    "    \n",
    "    # 翻译注释\n",
    "    comments_translations = []\n",
    "    for comment, ctype, indent in comments:\n",
    "        if re.search(\"[\\u4e00-\\u9fff]\", comment):  # 只翻译包含中文的注释\n",
    "            trans = translate_comment(comment)\n",
    "            comments_translations.append((comment, trans, ctype, indent))\n",
    "        else:\n",
    "            comments_translations.append((comment, comment, ctype, indent))  # 保持非中文注释不变\n",
    "    \n",
    "    # 替换代码中的注释\n",
    "    new_code_lines = replace_comments_in_code(code_lines, comments_translations)\n",
    "    \n",
    "    # 保存新文件，确保换行符保持不变\n",
    "    with open(output_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.writelines(new_code_lines)\n",
    "    \n",
    "    print(f\"新文件已生成：{output_file_path}\")\n",
    "\n",
    "# 示例：处理单个 Python 文件\n",
    "input_file = r\"C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\c2e2r.py\"  # 替换为待翻译的文件路径\n",
    "output_file = r\"C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\translated_test_4.py\"  # 替换为生成的输出文件路径\n",
    "process_python_file(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 提取 Python 文件中的注释，并保留代码的原始行结构\n",
    "def extract_comments(file_path):\n",
    "    \"\"\"从文件中提取所有单行和多行注释，返回原始代码按行分割的列表，以及注释信息\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "        lines = f.readlines()  # 逐行读取，确保保留空行\n",
    "    \n",
    "    comments = []\n",
    "    code_lines = []\n",
    "    \n",
    "    multi_line_comment = False  # 标记是否处于多行注释块中\n",
    "    multi_line_content = []  # 存储多行注释内容\n",
    "    multi_line_delimiter = \"\"  # 存储多行注释的分隔符\n",
    "    \n",
    "    for line in lines:\n",
    "        code_lines.append(line)  # 记录完整代码（包括换行符）\n",
    "\n",
    "        # 处理单行注释\n",
    "        single_comment_match = re.match(r\"(\\s*#.*)\", line)\n",
    "        if single_comment_match:\n",
    "            comments.append((single_comment_match.group(1), \"Single-line\"))\n",
    "            continue\n",
    "\n",
    "        # 处理多行注释\n",
    "        multi_match = re.match(r\"(\\s*)(['\\\"]{3})\", line)  # 识别多行注释的开始\n",
    "        if multi_match:\n",
    "            if not multi_line_comment:  # 进入多行注释\n",
    "                multi_line_comment = True\n",
    "                multi_line_delimiter = multi_match.group(2)\n",
    "                multi_line_content = [line]\n",
    "            else:  # 结束多行注释\n",
    "                multi_line_comment = False\n",
    "                multi_line_content.append(line)\n",
    "                comments.append((\"\".join(multi_line_content), \"Multi-line\"))\n",
    "            continue\n",
    "        \n",
    "        if multi_line_comment:\n",
    "            multi_line_content.append(line)\n",
    "\n",
    "    return code_lines, comments\n",
    "\n",
    "# 翻译注释（中文 -> 俄文）\n",
    "def translate_comment(text, prefix=\"translate to ru: \"):\n",
    "    \"\"\"使用微调模型翻译中文注释\"\"\"\n",
    "    input_text = prefix + text\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    generated_tokens = model.generate(**inputs, max_length=512)\n",
    "    translated = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "    return translated[0]\n",
    "\n",
    "# 替换代码中的中文注释，保持原有格式（缩进 & 空行）\n",
    "def replace_comments_in_code(code_lines, comments_translations):\n",
    "    \"\"\"\n",
    "    code_lines: 原始代码（按行分割）\n",
    "    comments_translations: 列表，包含 (原始注释, 翻译后注释, 注释类型)\n",
    "    \"\"\"\n",
    "    new_code_lines = code_lines[:]  # 复制代码列表，避免修改原始数据\n",
    "    \n",
    "    for orig, trans, ctype in comments_translations:\n",
    "        if ctype == \"Single-line\":\n",
    "            # 确保保留缩进\n",
    "            for i, line in enumerate(new_code_lines):\n",
    "                if orig in line:\n",
    "                    new_code_lines[i] = line.replace(orig, trans, 1)\n",
    "                    break\n",
    "        \n",
    "        elif ctype == \"Multi-line\":\n",
    "            orig_lines = orig.split(\"\\n\")\n",
    "            trans_lines = trans.split(\"\\n\")\n",
    "            for i in range(len(new_code_lines) - len(orig_lines) + 1):\n",
    "                if new_code_lines[i:i+len(orig_lines)] == orig_lines:\n",
    "                    new_code_lines[i:i+len(orig_lines)] = trans_lines\n",
    "                    break\n",
    "\n",
    "    return new_code_lines\n",
    "\n",
    "# 处理 Python 文件，翻译中文注释为俄语\n",
    "def process_python_file(input_file_path, output_file_path):\n",
    "    # 提取代码及注释\n",
    "    code_lines, comments = extract_comments(input_file_path)\n",
    "    \n",
    "    # 翻译注释\n",
    "    comments_translations = []\n",
    "    for comment, ctype in comments:\n",
    "        if re.search(\"[\\u4e00-\\u9fff]\", comment):  # 只翻译包含中文的注释\n",
    "            trans = translate_comment(comment)\n",
    "            comments_translations.append((comment, trans, ctype))\n",
    "        else:\n",
    "            comments_translations.append((comment, comment, ctype))  # 保持非中文注释不变\n",
    "    \n",
    "    # 替换代码中的注释\n",
    "    new_code_lines = replace_comments_in_code(code_lines, comments_translations)\n",
    "    \n",
    "    # 保存新文件，确保换行符保持不变\n",
    "    with open(output_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.writelines(new_code_lines)\n",
    "    \n",
    "    print(f\"新文件已生成：{output_file_path}\")\n",
    "\n",
    "# 示例：处理单个 Python 文件\n",
    "input_file = r\"C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\c2e2r.py\"  # 替换为待翻译的文件路径\n",
    "output_file = r\"C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\translated_test_3.py\"  # 替换为生成的输出文件路径\n",
    "process_python_file(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 提取 Python 文件中的注释，并保留代码的原始行结构\n",
    "def extract_comments(file_path):\n",
    "    \"\"\"从文件中提取所有单行和多行注释，返回原始代码按行分割的列表，以及注释信息\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "        lines = f.readlines()  # 逐行读取，确保保留空行\n",
    "    \n",
    "    comments = []\n",
    "    code_lines = []\n",
    "    \n",
    "    multi_line_comment = False  # 标记是否处于多行注释块中\n",
    "    multi_line_content = []  # 存储多行注释内容\n",
    "    multi_line_delimiter = \"\"  # 存储多行注释的分隔符\n",
    "    \n",
    "    for line in lines:\n",
    "        code_lines.append(line)  # 记录完整代码（包括换行符）\n",
    "\n",
    "        # 处理单行注释\n",
    "        single_comment_match = re.match(r\"(\\s*#.*)\", line)\n",
    "        if single_comment_match:\n",
    "            comments.append((single_comment_match.group(1), \"Single-line\"))\n",
    "            continue\n",
    "\n",
    "        # 处理多行注释\n",
    "        multi_match = re.match(r\"(\\s*)(['\\\"]{3})\", line)  # 识别多行注释的开始\n",
    "        if multi_match:\n",
    "            if not multi_line_comment:  # 进入多行注释\n",
    "                multi_line_comment = True\n",
    "                multi_line_delimiter = multi_match.group(2)\n",
    "                multi_line_content = [line]\n",
    "            else:  # 结束多行注释\n",
    "                multi_line_comment = False\n",
    "                multi_line_content.append(line)\n",
    "                comments.append((\"\".join(multi_line_content), \"Multi-line\"))\n",
    "            continue\n",
    "        \n",
    "        if multi_line_comment:\n",
    "            multi_line_content.append(line)\n",
    "\n",
    "    return code_lines, comments\n",
    "\n",
    "# 翻译注释（中文 -> 俄文）\n",
    "def translate_comment(text, prefix=\"translate to ru: \"):\n",
    "    \"\"\"使用微调模型翻译中文注释\"\"\"\n",
    "    input_text = prefix + text\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    generated_tokens = model.generate(**inputs, max_length=512)\n",
    "    translated = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "    return translated[0]\n",
    "\n",
    "# 替换代码中的中文注释，保持原有格式（缩进 & 空行）\n",
    "def replace_comments_in_code(code_lines, comments_translations):\n",
    "    \"\"\"\n",
    "    code_lines: 原始代码（按行分割）\n",
    "    comments_translations: 列表，包含 (原始注释, 翻译后注释, 注释类型)\n",
    "    \"\"\"\n",
    "    new_code_lines = code_lines[:]  # 复制代码列表，避免修改原始数据\n",
    "    \n",
    "    for orig, trans, ctype in comments_translations:\n",
    "        if ctype == \"Single-line\":\n",
    "            # 确保保留缩进\n",
    "            for i, line in enumerate(new_code_lines):\n",
    "                if orig in line:\n",
    "                    new_code_lines[i] = line.replace(orig, trans, 1)\n",
    "                    break\n",
    "        \n",
    "        elif ctype == \"Multi-line\":\n",
    "            orig_lines = orig.split(\"\\n\")\n",
    "            trans_lines = trans.split(\"\\n\")\n",
    "            for i in range(len(new_code_lines) - len(orig_lines) + 1):\n",
    "                if new_code_lines[i:i+len(orig_lines)] == orig_lines:\n",
    "                    new_code_lines[i:i+len(orig_lines)] = trans_lines\n",
    "                    break\n",
    "\n",
    "    return new_code_lines\n",
    "\n",
    "# 处理 Python 文件，翻译中文注释为俄语\n",
    "def process_python_file(input_file_path, output_file_path):\n",
    "    # 提取代码及注释\n",
    "    code_lines, comments = extract_comments(input_file_path)\n",
    "    \n",
    "    # 翻译注释\n",
    "    comments_translations = []\n",
    "    for comment, ctype in comments:\n",
    "        if re.search(\"[\\u4e00-\\u9fff]\", comment):  # 只翻译包含中文的注释\n",
    "            trans = translate_comment(comment)\n",
    "            comments_translations.append((comment, trans, ctype))\n",
    "        else:\n",
    "            comments_translations.append((comment, comment, ctype))  # 保持非中文注释不变\n",
    "    \n",
    "    # 替换代码中的注释\n",
    "    new_code_lines = replace_comments_in_code(code_lines, comments_translations)\n",
    "    \n",
    "    # 保存新文件，确保换行符保持不变\n",
    "    with open(output_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.writelines(new_code_lines)\n",
    "    \n",
    "    print(f\"新文件已生成：{output_file_path}\")\n",
    "\n",
    "# 示例：处理单个 Python 文件\n",
    "input_file = r\"C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\c2e2r_2.py\"  # 替换为待翻译的文件路径\n",
    "output_file = r\"C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\translated_test_2.py\"  # 替换为生成的输出文件路径\n",
    "process_python_file(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 提取 Python 文件中的注释\n",
    "def extract_comments(file_path):\n",
    "    \"\"\"从文件中提取所有单行和多行注释，返回原始代码和注释信息列表（包含注释内容及类型）\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "        code = f.read()\n",
    "    \n",
    "    # 匹配单行注释：以 # 开头\n",
    "    single_line_pattern = r\"(^\\s*#.*$)\"\n",
    "    single_line_comments = re.findall(single_line_pattern, code, flags=re.MULTILINE)\n",
    "    \n",
    "    # 匹配多行注释：''' 或 \"\"\" 包裹的内容\n",
    "    multi_line_pattern = r\"(['\\\"]{3})([\\s\\S]*?)(\\1)\"\n",
    "    multi_line_comments = re.findall(multi_line_pattern, code)\n",
    "    multi_line_comments = [match[1] for match in multi_line_comments]\n",
    "    \n",
    "    # 构造注释列表，标记类型\n",
    "    comments = []\n",
    "    for comment in single_line_comments:\n",
    "        comments.append((comment, \"Single-line\"))\n",
    "    for comment in multi_line_comments:\n",
    "        comments.append((comment.strip(), \"Multi-line\"))\n",
    "    \n",
    "    return code, comments\n",
    "\n",
    "# 使用微调后的模型进行翻译（从中文到俄文）\n",
    "def translate_comment(text, prefix=\"translate to ru: \"):\n",
    "    # 构造输入文本：添加前缀以提示模型翻译方向\n",
    "    input_text = prefix + text\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    generated_tokens = model.generate(**inputs, max_length=512)\n",
    "    translated = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "    return translated[0]\n",
    "\n",
    "# 根据原代码内容替换注释为翻译后的俄文注释\n",
    "def replace_comments_in_code(code, comments_translations):\n",
    "    \"\"\"\n",
    "    comments_translations: 列表，元素为 (原始注释, 翻译后注释, 注释类型)\n",
    "    \"\"\"\n",
    "    new_code = code\n",
    "    # 逐条替换，注意替换时可能需要额外处理格式问题\n",
    "    for orig, trans, ctype in comments_translations:\n",
    "        # 为了尽量精确替换，使用原始注释文本进行替换\n",
    "        new_code = new_code.replace(orig, trans)\n",
    "    return new_code\n",
    "\n",
    "# 主流程：上传文件 -> 翻译注释 -> 生成新 .py 文件\n",
    "def process_python_file(input_file_path, output_file_path):\n",
    "    # 提取代码和注释\n",
    "    code, comments = extract_comments(input_file_path)\n",
    "    \n",
    "    # 翻译每条注释（这里只处理中文注释，可添加判断，如只处理包含中文字符的注释）\n",
    "    comments_translations = []\n",
    "    for comment, ctype in comments:\n",
    "        # 这里可以添加一个简单的判断：如果注释中包含中文，则进行翻译\n",
    "        if re.search(\"[\\u4e00-\\u9fff]\", comment):\n",
    "            trans = translate_comment(comment)\n",
    "            comments_translations.append((comment, trans, ctype))\n",
    "        else:\n",
    "            comments_translations.append((comment, comment, ctype))\n",
    "    \n",
    "    # 替换代码中的注释\n",
    "    new_code = replace_comments_in_code(code, comments_translations)\n",
    "    \n",
    "    # 保存生成的新文件\n",
    "    with open(output_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(new_code)\n",
    "    print(f\"新文件已生成：{output_file_path}\")\n",
    "\n",
    "# 示例：处理某个 Python 文件\n",
    "input_file = r\"C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\c2e2r_2.py\"   # 替换为待翻译的 Python 文件路径\n",
    "output_file = r\"C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\c2r_test.py\"  # 替换为输出文件路径\n",
    "process_python_file(input_file, output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
