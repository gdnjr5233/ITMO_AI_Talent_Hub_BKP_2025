{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\python310\\lib\\site-packages (4.41.2)\n",
      "Requirement already satisfied: datasets in c:\\users\\gdnjr5233_yolo\\appdata\\roaming\\python\\python310\\site-packages (3.2.0)\n",
      "Collecting sacrebleu\n",
      "  Using cached sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\gdnjr5233_yolo\\appdata\\roaming\\python\\python310\\site-packages (0.2.0)\n",
      "Requirement already satisfied: filelock in c:\\python310\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in c:\\users\\gdnjr5233_yolo\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\python310\\lib\\site-packages (from transformers) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\gdnjr5233_yolo\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\python310\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\python310\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\python310\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\python310\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\python310\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\gdnjr5233_yolo\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\gdnjr5233_yolo\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\gdnjr5233_yolo\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\python310\\lib\\site-packages (from datasets) (2.1.3)\n",
      "Requirement already satisfied: xxhash in c:\\python310\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\gdnjr5233_yolo\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in c:\\python310\\lib\\site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2023.12.1)\n",
      "Requirement already satisfied: aiohttp in c:\\python310\\lib\\site-packages (from datasets) (3.9.3)\n",
      "Collecting portalocker (from sacrebleu)\n",
      "  Using cached portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in c:\\python310\\lib\\site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\gdnjr5233_yolo\\appdata\\roaming\\python\\python310\\site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in c:\\python310\\lib\\site-packages (from sacrebleu) (5.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\python310\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\python310\\lib\\site-packages (from aiohttp->datasets) (22.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\python310\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\python310\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\python310\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\python310\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python310\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python310\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gdnjr5233_yolo\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gdnjr5233_yolo\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\python310\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python310\\lib\\site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\python310\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\gdnjr5233_yolo\\appdata\\roaming\\python\\python310\\site-packages (from portalocker->sacrebleu) (306)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python310\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Using cached sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
      "Using cached portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: portalocker, sacrebleu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -penai (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -penai (c:\\python310\\lib\\site-packages)\n",
      "  WARNING: Failed to write executable - trying to use .deleteme logic\n",
      "ERROR: Could not install packages due to an OSError: [WinError 2] 系统找不到指定的文件。: 'C:\\\\Python310\\\\Scripts\\\\sacrebleu.exe' -> 'C:\\\\Python310\\\\Scripts\\\\sacrebleu.exe.deleteme'\n",
      "\n",
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# 安装必要库\n",
    "\n",
    "!pip install transformers datasets sacrebleu sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers.trainer_callback import TrainerCallback\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "# 配置日志记录\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 增强的错误处理回调\n",
    "class EnhancedErrorHandlingCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.error_count = 0\n",
    "        self.max_errors = 10  # 最大允许错误数\n",
    "    \n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if \"loss\" not in state.log_history[-1]:\n",
    "            self.error_count += 1\n",
    "            logger.warning(f\"跳过问题批次 (累计跳过: {self.error_count})\")\n",
    "            if self.error_count >= self.max_errors:\n",
    "                logger.error(\"达到最大错误次数，终止训练\")\n",
    "                control.should_training_stop = True\n",
    "            return control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 增强的数据预处理函数\n",
    "def enhanced_preprocess(example):\n",
    "    try:\n",
    "        # 空值检查\n",
    "        if not example[\"comment_zh\"] or not example[\"comment_ru\"]:\n",
    "            raise ValueError(\"空值样本\")\n",
    "        \n",
    "        # 长度检查\n",
    "        input_text = f\"translate to ru: {example['comment_zh']}\"\n",
    "        if len(input_text) > 512 or len(example['comment_ru']) > 512:\n",
    "            raise ValueError(\"文本过长\")\n",
    "        \n",
    "        # 编码处理\n",
    "        inputs = tokenizer(\n",
    "            input_text,\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            padding=False,\n",
    "            return_tensors=None\n",
    "        )\n",
    "        \n",
    "        targets = tokenizer(\n",
    "            text_target=example[\"comment_ru\"],\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            padding=False,\n",
    "            return_tensors=None\n",
    "        )\n",
    "        \n",
    "        # 有效性检查\n",
    "        if not inputs[\"input_ids\"] or not targets[\"input_ids\"]:\n",
    "            raise ValueError(\"无效编码\")\n",
    "            \n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"],\n",
    "            \"attention_mask\": inputs[\"attention_mask\"],\n",
    "            \"labels\": targets[\"input_ids\"]\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.debug(f\"过滤样本: {e} - 原文: {example['comment_zh'][:50]}...\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载并严格过滤数据\n",
    "def load_and_filter_data(path):\n",
    "    df = pd.read_csv(path)\n",
    "    logger.info(f\"原始数据量: {len(df)}\")\n",
    "    \n",
    "    # 空值过滤\n",
    "    df = df[[\"comment_zh\", \"comment_ru\"]].dropna()\n",
    "    df = df[(df[\"comment_zh\"].str.len() > 0) & (df[\"comment_ru\"].str.len() > 0)]\n",
    "    logger.info(f\"空值过滤后: {len(df)}\")\n",
    "    \n",
    "    # 长度过滤\n",
    "    df = df[df[\"comment_zh\"].apply(lambda x: len(x) <= 500)]\n",
    "    df = df[df[\"comment_ru\"].apply(lambda x: len(x) <= 500)]\n",
    "    logger.info(f\"长度过滤后: {len(df)}\")\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        raise ValueError(\"有效数据量为零，请检查数据文件！\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 改进的数据整理函数\n",
    "def robust_data_collator(features):\n",
    "    try:\n",
    "        # 过滤无效特征\n",
    "        valid_features = []\n",
    "        for f in features:\n",
    "            if f and len(f[\"input_ids\"]) > 0 and len(f[\"labels\"]) > 0:\n",
    "                valid_features.append(f)\n",
    "        \n",
    "        if not valid_features:\n",
    "            logger.warning(\"收到空批次，生成虚拟样本\")\n",
    "            return {\n",
    "                \"input_ids\": torch.zeros((1, 10), dtype=torch.long),\n",
    "                \"attention_mask\": torch.ones((1, 10), dtype=torch.long),\n",
    "                \"labels\": torch.zeros((1, 10), dtype=torch.long)\n",
    "            }\n",
    "        \n",
    "        # 动态最大长度计算\n",
    "        max_len = max(len(f[\"input_ids\"]) for f in valid_features)\n",
    "        max_len = min(max_len, 512)\n",
    "        \n",
    "        batch = tokenizer.pad(\n",
    "            {\"input_ids\": [f[\"input_ids\"] for f in valid_features]},\n",
    "            padding=\"longest\",\n",
    "            max_length=max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        labels = tokenizer.pad(\n",
    "            {\"input_ids\": [f[\"labels\"] for f in valid_features]},\n",
    "            padding=\"longest\",\n",
    "            max_length=max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )[\"input_ids\"]\n",
    "        \n",
    "        # 替换pad_token_id\n",
    "        labels[labels == tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": batch[\"input_ids\"],\n",
    "            \"attention_mask\": batch[\"attention_mask\"],\n",
    "            \"labels\": labels\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"数据整理错误: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:原始数据量: 1258\n",
      "INFO:__main__:空值过滤后: 1258\n",
      "INFO:__main__:长度过滤后: 1231\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "334530ea44e344708141aa7bac27d1db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/1107 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:流程错误: name 'logger' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'logger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\gdnjr5233_YOLO\\AppData\\Local\\Temp\\ipykernel_11172\\589650714.py\", line 14, in enhanced_preprocess\nNameError: name 'tokenizer' is not defined\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\gdnjr5233_YOLO\\AppData\\Roaming\\Python\\Python310\\site-packages\\multiprocess\\pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"C:\\Users\\gdnjr5233_YOLO\\AppData\\Roaming\\Python\\Python310\\site-packages\\datasets\\utils\\py_utils.py\", line 678, in _write_generator_to_queue\n    for i, result in enumerate(func(**kwargs)):\n  File \"C:\\Users\\gdnjr5233_YOLO\\AppData\\Roaming\\Python\\Python310\\site-packages\\datasets\\arrow_dataset.py\", line 3446, in _map_single\n    example = apply_function_on_filtered_inputs(example, i, offset=offset)\n  File \"C:\\Users\\gdnjr5233_YOLO\\AppData\\Roaming\\Python\\Python310\\site-packages\\datasets\\arrow_dataset.py\", line 3338, in apply_function_on_filtered_inputs\n    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n  File \"C:\\Users\\gdnjr5233_YOLO\\AppData\\Local\\Temp\\ipykernel_11172\\589650714.py\", line 40, in enhanced_preprocess\nNameError: name 'logger' is not defined\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m train_df, val_df \u001b[38;5;241m=\u001b[39m train_test_split(df, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 转换数据集格式\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43menhanced_preprocess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcomment_zh\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcomment_ru\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m     13\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;28;01mlambda\u001b[39;00m x: x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     15\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_pandas(val_df)\u001b[38;5;241m.\u001b[39mmap(\n\u001b[0;32m     16\u001b[0m     enhanced_preprocess,\n\u001b[0;32m     17\u001b[0m     remove_columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomment_zh\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomment_ru\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     18\u001b[0m     num_proc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     19\u001b[0m     load_from_cache_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     20\u001b[0m )\u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;28;01mlambda\u001b[39;00m x: x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     22\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m训练样本数: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\datasets\\arrow_dataset.py:560\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    553\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[0;32m    558\u001b[0m }\n\u001b[0;32m    559\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 560\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    561\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    562\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\datasets\\arrow_dataset.py:3165\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   3159\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpawning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_proc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m processes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3160\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[0;32m   3161\u001b[0m     unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3162\u001b[0m     total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[0;32m   3163\u001b[0m     desc\u001b[38;5;241m=\u001b[39m(desc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (num_proc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_proc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3164\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m-> 3165\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m iflatmap_unordered(\n\u001b[0;32m   3166\u001b[0m         pool, Dataset\u001b[38;5;241m.\u001b[39m_map_single, kwargs_iterable\u001b[38;5;241m=\u001b[39mkwargs_per_job\n\u001b[0;32m   3167\u001b[0m     ):\n\u001b[0;32m   3168\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m   3169\u001b[0m             shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\datasets\\utils\\py_utils.py:718\u001b[0m, in \u001b[0;36miflatmap_unordered\u001b[1;34m(pool, func, kwargs_iterable)\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    716\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pool_changed:\n\u001b[0;32m    717\u001b[0m         \u001b[38;5;66;03m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[1;32m--> 718\u001b[0m         [async_result\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m async_result \u001b[38;5;129;01min\u001b[39;00m async_results]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\datasets\\utils\\py_utils.py:718\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    716\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pool_changed:\n\u001b[0;32m    717\u001b[0m         \u001b[38;5;66;03m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[1;32m--> 718\u001b[0m         [\u001b[43masync_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m async_result \u001b[38;5;129;01min\u001b[39;00m async_results]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\multiprocess\\pool.py:774\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n\u001b[0;32m    773\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 774\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "\u001b[1;31mNameError\u001b[0m: name 'logger' is not defined"
     ]
    }
   ],
   "source": [
    "# 主流程\n",
    "try:\n",
    "    # 加载并预处理数据\n",
    "    df = load_and_filter_data(r\"C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\datasets\\data.csv\")  # 替换实际路径\n",
    "    train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "    \n",
    "    # 转换数据集格式\n",
    "    train_dataset = Dataset.from_pandas(train_df).map(\n",
    "        enhanced_preprocess,\n",
    "        remove_columns=[\"comment_zh\", \"comment_ru\"],\n",
    "        num_proc=2,\n",
    "        load_from_cache_file=False\n",
    "    ).filter(lambda x: x is not None)\n",
    "    \n",
    "    val_dataset = Dataset.from_pandas(val_df).map(\n",
    "        enhanced_preprocess,\n",
    "        remove_columns=[\"comment_zh\", \"comment_ru\"],\n",
    "        num_proc=2,\n",
    "        load_from_cache_file=False\n",
    "    ).filter(lambda x: x is not None)\n",
    "    \n",
    "    logger.info(f\"训练样本数: {len(train_dataset)}\")\n",
    "    logger.info(f\"验证样本数: {len(val_dataset)}\")\n",
    "    \n",
    "    # 模型初始化\n",
    "    model = T5ForConditionalGeneration.from_pretrained(\"utrobinmv/t5_translate_en_ru_zh_small_1024\")\n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"utrobinmv/t5_translate_en_ru_zh_small_1024\")\n",
    "    \n",
    "    # 训练参数配置\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=\"./enhanced-model\",\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=500,\n",
    "        save_steps=500,\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        weight_decay=0.01,\n",
    "        num_train_epochs=5,\n",
    "        logging_dir=\"./logs\",\n",
    "        report_to=\"none\",\n",
    "        gradient_accumulation_steps=2,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        dataloader_num_workers=2,\n",
    "        remove_unused_columns=False,\n",
    "        predict_with_generate=True,\n",
    "        generation_max_length=512\n",
    "    )\n",
    "    \n",
    "    # 初始化Trainer\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=robust_data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        callbacks=[EnhancedErrorHandlingCallback()]\n",
    "    )\n",
    "    \n",
    "    # 训练流程\n",
    "    try:\n",
    "        logger.info(\"开始训练...\")\n",
    "        trainer.train()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"训练中断: {e}\")\n",
    "        trainer.save_model(\"./interrupted-model\")\n",
    "        logger.info(\"中间模型已保存到 ./interrupted-model\")\n",
    "    \n",
    "    # 最终保存\n",
    "    trainer.save_model(\"./enhanced-model\")\n",
    "    tokenizer.save_pretrained(\"./enhanced-model\")\n",
    "    logger.info(\"训练完成，模型已保存到 ./enhanced-model\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"流程错误: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义回调函数处理错误\n",
    "from transformers.trainer_callback import TrainerCallback\n",
    "\n",
    "class ErrorHandlingCallback(TrainerCallback):\n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        print(\"Starting training...\")\n",
    "        self.skipped_batches = 0\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if \"loss\" not in state.log_history[-1]:\n",
    "            print(f\"Skipping problematic batch (total skipped: {self.skipped_batches})\")\n",
    "            self.skipped_batches += 1\n",
    "            return control\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理函数\n",
    "def preprocess_data(example):\n",
    "    try:\n",
    "        # 添加目标语言前缀\n",
    "        input_text = f\"translate to ru: {example['comment_zh']}\"\n",
    "        target_text = example['comment_ru']\n",
    "        \n",
    "        # 编码处理\n",
    "        inputs = tokenizer(\n",
    "            input_text,\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            padding=False,\n",
    "            return_tensors=None\n",
    "        )\n",
    "        \n",
    "        targets = tokenizer(\n",
    "            text_target=target_text,\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            padding=False,\n",
    "            return_tensors=None\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"],\n",
    "            \"attention_mask\": inputs[\"attention_mask\"],\n",
    "            \"labels\": targets[\"input_ids\"]\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing sample: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_zh</th>\n",
       "      <th>comment_ru</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>用于两个样本分位数的自助法t检验</td>\n",
       "      <td>t-тест самообслуживания для двух квартилей выб...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>计算控制组和实验组的自助法分位数</td>\n",
       "      <td>Рассчитать квантили метода самообслуживания дл...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>对自助法分位数进行t检验</td>\n",
       "      <td>Провести t-тест на квантиль самообслуживания</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>判断是否拒绝原假设</td>\n",
       "      <td>Определение того, отклонять или нет первоначал...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>返回使用自助抽样法计算的分位数分布。</td>\n",
       "      <td>Возвращает квантильное распределение, рассчита...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253</th>\n",
       "      <td>拟合模型。\\n\\n        参数\\n        -------------\\n  ...</td>\n",
       "      <td>Комбинированные модели.\\n\\nПараметры\\n--------...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>将数据集减少到选择的特征。\\n\\n        参数\\n        ---------...</td>\n",
       "      <td>Снизить наборы данных до отдельных характерист...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>返回选择的特征数量。</td>\n",
       "      <td>Возвращает количество выбранных характеристик.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>生成数据集。\\n\\n    参数\\n    -------------\\n    n_sam...</td>\n",
       "      <td>Создаёт наборы данных.\\n\\nПараметры\\n---------...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>运行程序。</td>\n",
       "      <td>Запускай программу.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1258 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment_zh  \\\n",
       "0                                      用于两个样本分位数的自助法t检验   \n",
       "1                                      计算控制组和实验组的自助法分位数   \n",
       "2                                          对自助法分位数进行t检验   \n",
       "3                                             判断是否拒绝原假设   \n",
       "4                                    返回使用自助抽样法计算的分位数分布。   \n",
       "...                                                 ...   \n",
       "1253  拟合模型。\\n\\n        参数\\n        -------------\\n  ...   \n",
       "1254  将数据集减少到选择的特征。\\n\\n        参数\\n        ---------...   \n",
       "1255                                         返回选择的特征数量。   \n",
       "1256  生成数据集。\\n\\n    参数\\n    -------------\\n    n_sam...   \n",
       "1257                                              运行程序。   \n",
       "\n",
       "                                             comment_ru  \n",
       "0     t-тест самообслуживания для двух квартилей выб...  \n",
       "1     Рассчитать квантили метода самообслуживания дл...  \n",
       "2          Провести t-тест на квантиль самообслуживания  \n",
       "3     Определение того, отклонять или нет первоначал...  \n",
       "4     Возвращает квантильное распределение, рассчита...  \n",
       "...                                                 ...  \n",
       "1253  Комбинированные модели.\\n\\nПараметры\\n--------...  \n",
       "1254  Снизить наборы данных до отдельных характерист...  \n",
       "1255     Возвращает количество выбранных характеристик.  \n",
       "1256  Создаёт наборы данных.\\n\\nПараметры\\n---------...  \n",
       "1257                                Запускай программу.  \n",
       "\n",
       "[1258 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载数据集\n",
    "df = pd.read_csv(r\"C:\\Users\\gdnjr5233_YOLO\\Desktop\\ВКР_2025\\datasets\\data.csv\")  # 替换为实际路径\n",
    "df = df[[\"comment_zh\", \"comment_ru\"]].dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分训练集和验证集\n",
    "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "\n",
    "# 转换为HuggingFace数据集格式\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# 加载模型和tokenizer\n",
    "model_name = \"utrobinmv/t5_translate_en_ru_zh_small_1024\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ac438646f864590bb50884a034cd763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/1132 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8efcf1655d1455f954cc6304c27d50f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1132 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45590dd01a0c41c3a7426f5102e4bf3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/126 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9d003d68ff04f7bbe16eb209eeb211e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/126 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 应用预处理\n",
    "train_dataset = train_dataset.map(\n",
    "    preprocess_data,\n",
    "    remove_columns=[\"comment_zh\", \"comment_ru\"],\n",
    "    num_proc=4\n",
    ").filter(lambda x: x is not None)\n",
    "\n",
    "val_dataset = val_dataset.map(\n",
    "    preprocess_data,\n",
    "    remove_columns=[\"comment_zh\", \"comment_ru\"],\n",
    "    num_proc=4\n",
    ").filter(lambda x: x is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据整理函数（添加错误处理）\n",
    "def data_collator(features):\n",
    "    filtered_features = [f for f in features if f is not None]\n",
    "    if not filtered_features:\n",
    "        return None  # 跳过空batch\n",
    "    \n",
    "    batch = tokenizer.pad(\n",
    "        filtered_features,\n",
    "        padding=\"longest\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # 检查是否包含无效的token id\n",
    "    if (batch[\"labels\"] >= tokenizer.vocab_size).any():\n",
    "        print(\"Detected invalid token IDs in labels, skipping batch\")\n",
    "        return None\n",
    "        \n",
    "    return {\n",
    "        \"input_ids\": batch[\"input_ids\"],\n",
    "        \"attention_mask\": batch[\"attention_mask\"],\n",
    "        \"labels\": batch[\"labels\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 训练参数配置\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./fine-tuned-model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=5,\n",
    "    predict_with_generate=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\",\n",
    "    gradient_accumulation_steps=2,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    dataloader_drop_last=True,  # 防止最后不完整的batch\n",
    "    load_best_model_at_end=True,  # 保存最佳模型\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76c3ba82e6834fbfb0da7b245fabd657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/705 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training interrupted by error: list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gdnjr5233_YOLO\\AppData\\Roaming\\Python\\Python310\\site-packages\\datasets\\formatting\\formatting.py:101: RuntimeWarning: divide by zero encountered in remainder\n",
      "  return table.fast_gather(key % table.num_rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial model saved at ./partial-model\n"
     ]
    }
   ],
   "source": [
    "# 初始化Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[ErrorHandlingCallback()]\n",
    ")\n",
    "\n",
    "# 开始训练\n",
    "try:\n",
    "    trainer.train()\n",
    "except Exception as e:\n",
    "    print(f\"Training interrupted by error: {e}\")\n",
    "    # 自动保存检查点\n",
    "    trainer.save_model(\"./partial-model\")\n",
    "    print(\"Partial model saved at ./partial-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存最终模型\n",
    "trainer.save_model(\"./fine-tuned-model\")\n",
    "tokenizer.save_pretrained(\"./fine-tuned-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
