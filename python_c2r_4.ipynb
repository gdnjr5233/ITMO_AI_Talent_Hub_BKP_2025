{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\python310\\lib\\site-packages (4.41.2)\n",
      "Requirement already satisfied: datasets in c:\\users\\gdnjr5233_yolo\\appdata\\roaming\\python\\python310\\site-packages (3.2.0)\n",
      "Collecting sacrebleu\n",
      "  Using cached sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\gdnjr5233_yolo\\appdata\\roaming\\python\\python310\\site-packages (0.2.0)\n",
      "Requirement already satisfied: filelock in c:\\python310\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in c:\\users\\gdnjr5233_yolo\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\python310\\lib\\site-packages (from transformers) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\gdnjr5233_yolo\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\python310\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\python310\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\python310\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\python310\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\python310\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\gdnjr5233_yolo\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\gdnjr5233_yolo\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\gdnjr5233_yolo\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\python310\\lib\\site-packages (from datasets) (2.1.3)\n",
      "Requirement already satisfied: xxhash in c:\\python310\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\gdnjr5233_yolo\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in c:\\python310\\lib\\site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2023.12.1)\n",
      "Requirement already satisfied: aiohttp in c:\\python310\\lib\\site-packages (from datasets) (3.9.3)\n",
      "Collecting portalocker (from sacrebleu)\n",
      "  Using cached portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in c:\\python310\\lib\\site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\gdnjr5233_yolo\\appdata\\roaming\\python\\python310\\site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in c:\\python310\\lib\\site-packages (from sacrebleu) (5.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\python310\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\python310\\lib\\site-packages (from aiohttp->datasets) (22.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\python310\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\python310\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\python310\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\python310\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python310\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python310\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gdnjr5233_yolo\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gdnjr5233_yolo\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\python310\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python310\\lib\\site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\python310\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\gdnjr5233_yolo\\appdata\\roaming\\python\\python310\\site-packages (from portalocker->sacrebleu) (306)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python310\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Using cached sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
      "Using cached portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: portalocker, sacrebleu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -penai (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -penai (c:\\python310\\lib\\site-packages)\n",
      "  WARNING: Failed to write executable - trying to use .deleteme logic\n",
      "ERROR: Could not install packages due to an OSError: [WinError 2] ç³»ç»Ÿæ‰¾ä¸åˆ°æŒ‡å®šçš„æ–‡ä»¶ã€‚: 'C:\\\\Python310\\\\Scripts\\\\sacrebleu.exe' -> 'C:\\\\Python310\\\\Scripts\\\\sacrebleu.exe.deleteme'\n",
      "\n",
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# å®‰è£…å¿…è¦åº“\n",
    "\n",
    "!pip install transformers datasets sacrebleu sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers.trainer_callback import TrainerCallback\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "# é…ç½®æ—¥å¿—è®°å½•\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¢å¼ºçš„é”™è¯¯å¤„ç†å›è°ƒ\n",
    "class EnhancedErrorHandlingCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.error_count = 0\n",
    "        self.max_errors = 10  # æœ€å¤§å…è®¸é”™è¯¯æ•°\n",
    "    \n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if \"loss\" not in state.log_history[-1]:\n",
    "            self.error_count += 1\n",
    "            logger.warning(f\"è·³è¿‡é—®é¢˜æ‰¹æ¬¡ (ç´¯è®¡è·³è¿‡: {self.error_count})\")\n",
    "            if self.error_count >= self.max_errors:\n",
    "                logger.error(\"è¾¾åˆ°æœ€å¤§é”™è¯¯æ¬¡æ•°ï¼Œç»ˆæ­¢è®­ç»ƒ\")\n",
    "                control.should_training_stop = True\n",
    "            return control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¢å¼ºçš„æ•°æ®é¢„å¤„ç†å‡½æ•°\n",
    "def enhanced_preprocess(example):\n",
    "    try:\n",
    "        # ç©ºå€¼æ£€æŸ¥\n",
    "        if not example[\"comment_zh\"] or not example[\"comment_ru\"]:\n",
    "            raise ValueError(\"ç©ºå€¼æ ·æœ¬\")\n",
    "        \n",
    "        # é•¿åº¦æ£€æŸ¥\n",
    "        input_text = f\"translate to ru: {example['comment_zh']}\"\n",
    "        if len(input_text) > 512 or len(example['comment_ru']) > 512:\n",
    "            raise ValueError(\"æ–‡æœ¬è¿‡é•¿\")\n",
    "        \n",
    "        # ç¼–ç å¤„ç†\n",
    "        inputs = tokenizer(\n",
    "            input_text,\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            padding=False,\n",
    "            return_tensors=None\n",
    "        )\n",
    "        \n",
    "        targets = tokenizer(\n",
    "            text_target=example[\"comment_ru\"],\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            padding=False,\n",
    "            return_tensors=None\n",
    "        )\n",
    "        \n",
    "        # æœ‰æ•ˆæ€§æ£€æŸ¥\n",
    "        if not inputs[\"input_ids\"] or not targets[\"input_ids\"]:\n",
    "            raise ValueError(\"æ— æ•ˆç¼–ç \")\n",
    "            \n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"],\n",
    "            \"attention_mask\": inputs[\"attention_mask\"],\n",
    "            \"labels\": targets[\"input_ids\"]\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.debug(f\"è¿‡æ»¤æ ·æœ¬: {e} - åŸæ–‡: {example['comment_zh'][:50]}...\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½å¹¶ä¸¥æ ¼è¿‡æ»¤æ•°æ®\n",
    "def load_and_filter_data(path):\n",
    "    df = pd.read_csv(path)\n",
    "    logger.info(f\"åŸå§‹æ•°æ®é‡: {len(df)}\")\n",
    "    \n",
    "    # ç©ºå€¼è¿‡æ»¤\n",
    "    df = df[[\"comment_zh\", \"comment_ru\"]].dropna()\n",
    "    df = df[(df[\"comment_zh\"].str.len() > 0) & (df[\"comment_ru\"].str.len() > 0)]\n",
    "    logger.info(f\"ç©ºå€¼è¿‡æ»¤å: {len(df)}\")\n",
    "    \n",
    "    # é•¿åº¦è¿‡æ»¤\n",
    "    df = df[df[\"comment_zh\"].apply(lambda x: len(x) <= 500)]\n",
    "    df = df[df[\"comment_ru\"].apply(lambda x: len(x) <= 500)]\n",
    "    logger.info(f\"é•¿åº¦è¿‡æ»¤å: {len(df)}\")\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        raise ValueError(\"æœ‰æ•ˆæ•°æ®é‡ä¸ºé›¶ï¼Œè¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶ï¼\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ”¹è¿›çš„æ•°æ®æ•´ç†å‡½æ•°\n",
    "def robust_data_collator(features):\n",
    "    try:\n",
    "        # è¿‡æ»¤æ— æ•ˆç‰¹å¾\n",
    "        valid_features = []\n",
    "        for f in features:\n",
    "            if f and len(f[\"input_ids\"]) > 0 and len(f[\"labels\"]) > 0:\n",
    "                valid_features.append(f)\n",
    "        \n",
    "        if not valid_features:\n",
    "            logger.warning(\"æ”¶åˆ°ç©ºæ‰¹æ¬¡ï¼Œç”Ÿæˆè™šæ‹Ÿæ ·æœ¬\")\n",
    "            return {\n",
    "                \"input_ids\": torch.zeros((1, 10), dtype=torch.long),\n",
    "                \"attention_mask\": torch.ones((1, 10), dtype=torch.long),\n",
    "                \"labels\": torch.zeros((1, 10), dtype=torch.long)\n",
    "            }\n",
    "        \n",
    "        # åŠ¨æ€æœ€å¤§é•¿åº¦è®¡ç®—\n",
    "        max_len = max(len(f[\"input_ids\"]) for f in valid_features)\n",
    "        max_len = min(max_len, 512)\n",
    "        \n",
    "        batch = tokenizer.pad(\n",
    "            {\"input_ids\": [f[\"input_ids\"] for f in valid_features]},\n",
    "            padding=\"longest\",\n",
    "            max_length=max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        labels = tokenizer.pad(\n",
    "            {\"input_ids\": [f[\"labels\"] for f in valid_features]},\n",
    "            padding=\"longest\",\n",
    "            max_length=max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )[\"input_ids\"]\n",
    "        \n",
    "        # æ›¿æ¢pad_token_id\n",
    "        labels[labels == tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": batch[\"input_ids\"],\n",
    "            \"attention_mask\": batch[\"attention_mask\"],\n",
    "            \"labels\": labels\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"æ•°æ®æ•´ç†é”™è¯¯: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:åŸå§‹æ•°æ®é‡: 1258\n",
      "INFO:__main__:ç©ºå€¼è¿‡æ»¤å: 1258\n",
      "INFO:__main__:é•¿åº¦è¿‡æ»¤å: 1231\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "334530ea44e344708141aa7bac27d1db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/1107 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:æµç¨‹é”™è¯¯: name 'logger' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'logger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\gdnjr5233_YOLO\\AppData\\Local\\Temp\\ipykernel_11172\\589650714.py\", line 14, in enhanced_preprocess\nNameError: name 'tokenizer' is not defined\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\gdnjr5233_YOLO\\AppData\\Roaming\\Python\\Python310\\site-packages\\multiprocess\\pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"C:\\Users\\gdnjr5233_YOLO\\AppData\\Roaming\\Python\\Python310\\site-packages\\datasets\\utils\\py_utils.py\", line 678, in _write_generator_to_queue\n    for i, result in enumerate(func(**kwargs)):\n  File \"C:\\Users\\gdnjr5233_YOLO\\AppData\\Roaming\\Python\\Python310\\site-packages\\datasets\\arrow_dataset.py\", line 3446, in _map_single\n    example = apply_function_on_filtered_inputs(example, i, offset=offset)\n  File \"C:\\Users\\gdnjr5233_YOLO\\AppData\\Roaming\\Python\\Python310\\site-packages\\datasets\\arrow_dataset.py\", line 3338, in apply_function_on_filtered_inputs\n    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n  File \"C:\\Users\\gdnjr5233_YOLO\\AppData\\Local\\Temp\\ipykernel_11172\\589650714.py\", line 40, in enhanced_preprocess\nNameError: name 'logger' is not defined\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m train_df, val_df \u001b[38;5;241m=\u001b[39m train_test_split(df, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# è½¬æ¢æ•°æ®é›†æ ¼å¼\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43menhanced_preprocess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcomment_zh\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcomment_ru\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m     13\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;28;01mlambda\u001b[39;00m x: x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     15\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_pandas(val_df)\u001b[38;5;241m.\u001b[39mmap(\n\u001b[0;32m     16\u001b[0m     enhanced_preprocess,\n\u001b[0;32m     17\u001b[0m     remove_columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomment_zh\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomment_ru\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     18\u001b[0m     num_proc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     19\u001b[0m     load_from_cache_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     20\u001b[0m )\u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;28;01mlambda\u001b[39;00m x: x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     22\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mè®­ç»ƒæ ·æœ¬æ•°: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\datasets\\arrow_dataset.py:560\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    553\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[0;32m    558\u001b[0m }\n\u001b[0;32m    559\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 560\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    561\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    562\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\datasets\\arrow_dataset.py:3165\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   3159\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpawning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_proc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m processes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3160\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[0;32m   3161\u001b[0m     unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3162\u001b[0m     total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[0;32m   3163\u001b[0m     desc\u001b[38;5;241m=\u001b[39m(desc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (num_proc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_proc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3164\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m-> 3165\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m iflatmap_unordered(\n\u001b[0;32m   3166\u001b[0m         pool, Dataset\u001b[38;5;241m.\u001b[39m_map_single, kwargs_iterable\u001b[38;5;241m=\u001b[39mkwargs_per_job\n\u001b[0;32m   3167\u001b[0m     ):\n\u001b[0;32m   3168\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m   3169\u001b[0m             shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\datasets\\utils\\py_utils.py:718\u001b[0m, in \u001b[0;36miflatmap_unordered\u001b[1;34m(pool, func, kwargs_iterable)\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    716\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pool_changed:\n\u001b[0;32m    717\u001b[0m         \u001b[38;5;66;03m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[1;32m--> 718\u001b[0m         [async_result\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m async_result \u001b[38;5;129;01min\u001b[39;00m async_results]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\datasets\\utils\\py_utils.py:718\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    716\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pool_changed:\n\u001b[0;32m    717\u001b[0m         \u001b[38;5;66;03m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[1;32m--> 718\u001b[0m         [\u001b[43masync_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m async_result \u001b[38;5;129;01min\u001b[39;00m async_results]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\multiprocess\\pool.py:774\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n\u001b[0;32m    773\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 774\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "\u001b[1;31mNameError\u001b[0m: name 'logger' is not defined"
     ]
    }
   ],
   "source": [
    "# ä¸»æµç¨‹\n",
    "try:\n",
    "    # åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®\n",
    "    df = load_and_filter_data(r\"C:\\Users\\gdnjr5233_YOLO\\Desktop\\Ğ’ĞšĞ _2025\\datasets\\data.csv\")  # æ›¿æ¢å®é™…è·¯å¾„\n",
    "    train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "    \n",
    "    # è½¬æ¢æ•°æ®é›†æ ¼å¼\n",
    "    train_dataset = Dataset.from_pandas(train_df).map(\n",
    "        enhanced_preprocess,\n",
    "        remove_columns=[\"comment_zh\", \"comment_ru\"],\n",
    "        num_proc=2,\n",
    "        load_from_cache_file=False\n",
    "    ).filter(lambda x: x is not None)\n",
    "    \n",
    "    val_dataset = Dataset.from_pandas(val_df).map(\n",
    "        enhanced_preprocess,\n",
    "        remove_columns=[\"comment_zh\", \"comment_ru\"],\n",
    "        num_proc=2,\n",
    "        load_from_cache_file=False\n",
    "    ).filter(lambda x: x is not None)\n",
    "    \n",
    "    logger.info(f\"è®­ç»ƒæ ·æœ¬æ•°: {len(train_dataset)}\")\n",
    "    logger.info(f\"éªŒè¯æ ·æœ¬æ•°: {len(val_dataset)}\")\n",
    "    \n",
    "    # æ¨¡å‹åˆå§‹åŒ–\n",
    "    model = T5ForConditionalGeneration.from_pretrained(\"utrobinmv/t5_translate_en_ru_zh_small_1024\")\n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"utrobinmv/t5_translate_en_ru_zh_small_1024\")\n",
    "    \n",
    "    # è®­ç»ƒå‚æ•°é…ç½®\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=\"./enhanced-model\",\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=500,\n",
    "        save_steps=500,\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        weight_decay=0.01,\n",
    "        num_train_epochs=5,\n",
    "        logging_dir=\"./logs\",\n",
    "        report_to=\"none\",\n",
    "        gradient_accumulation_steps=2,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        dataloader_num_workers=2,\n",
    "        remove_unused_columns=False,\n",
    "        predict_with_generate=True,\n",
    "        generation_max_length=512\n",
    "    )\n",
    "    \n",
    "    # åˆå§‹åŒ–Trainer\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=robust_data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        callbacks=[EnhancedErrorHandlingCallback()]\n",
    "    )\n",
    "    \n",
    "    # è®­ç»ƒæµç¨‹\n",
    "    try:\n",
    "        logger.info(\"å¼€å§‹è®­ç»ƒ...\")\n",
    "        trainer.train()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"è®­ç»ƒä¸­æ–­: {e}\")\n",
    "        trainer.save_model(\"./interrupted-model\")\n",
    "        logger.info(\"ä¸­é—´æ¨¡å‹å·²ä¿å­˜åˆ° ./interrupted-model\")\n",
    "    \n",
    "    # æœ€ç»ˆä¿å­˜\n",
    "    trainer.save_model(\"./enhanced-model\")\n",
    "    tokenizer.save_pretrained(\"./enhanced-model\")\n",
    "    logger.info(\"è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜åˆ° ./enhanced-model\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"æµç¨‹é”™è¯¯: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è‡ªå®šä¹‰å›è°ƒå‡½æ•°å¤„ç†é”™è¯¯\n",
    "from transformers.trainer_callback import TrainerCallback\n",
    "\n",
    "class ErrorHandlingCallback(TrainerCallback):\n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        print(\"Starting training...\")\n",
    "        self.skipped_batches = 0\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if \"loss\" not in state.log_history[-1]:\n",
    "            print(f\"Skipping problematic batch (total skipped: {self.skipped_batches})\")\n",
    "            self.skipped_batches += 1\n",
    "            return control\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•°æ®é¢„å¤„ç†å‡½æ•°\n",
    "def preprocess_data(example):\n",
    "    try:\n",
    "        # æ·»åŠ ç›®æ ‡è¯­è¨€å‰ç¼€\n",
    "        input_text = f\"translate to ru: {example['comment_zh']}\"\n",
    "        target_text = example['comment_ru']\n",
    "        \n",
    "        # ç¼–ç å¤„ç†\n",
    "        inputs = tokenizer(\n",
    "            input_text,\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            padding=False,\n",
    "            return_tensors=None\n",
    "        )\n",
    "        \n",
    "        targets = tokenizer(\n",
    "            text_target=target_text,\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            padding=False,\n",
    "            return_tensors=None\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"],\n",
    "            \"attention_mask\": inputs[\"attention_mask\"],\n",
    "            \"labels\": targets[\"input_ids\"]\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing sample: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_zh</th>\n",
       "      <th>comment_ru</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ç”¨äºä¸¤ä¸ªæ ·æœ¬åˆ†ä½æ•°çš„è‡ªåŠ©æ³•tæ£€éªŒ</td>\n",
       "      <td>t-Ñ‚ĞµÑÑ‚ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑĞ»ÑƒĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ´Ğ²ÑƒÑ… ĞºĞ²Ğ°Ñ€Ñ‚Ğ¸Ğ»ĞµĞ¹ Ğ²Ñ‹Ğ±...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>è®¡ç®—æ§åˆ¶ç»„å’Œå®éªŒç»„çš„è‡ªåŠ©æ³•åˆ†ä½æ•°</td>\n",
       "      <td>Ğ Ğ°ÑÑÑ‡Ğ¸Ñ‚Ğ°Ñ‚ÑŒ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑĞ»ÑƒĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>å¯¹è‡ªåŠ©æ³•åˆ†ä½æ•°è¿›è¡Œtæ£€éªŒ</td>\n",
       "      <td>ĞŸÑ€Ğ¾Ğ²ĞµÑÑ‚Ğ¸ t-Ñ‚ĞµÑÑ‚ Ğ½Ğ° ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ»ÑŒ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑĞ»ÑƒĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>åˆ¤æ–­æ˜¯å¦æ‹’ç»åŸå‡è®¾</td>\n",
       "      <td>ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½ÑÑ‚ÑŒ Ğ¸Ğ»Ğ¸ Ğ½ĞµÑ‚ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ½Ğ°Ñ‡Ğ°Ğ»...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>è¿”å›ä½¿ç”¨è‡ªåŠ©æŠ½æ ·æ³•è®¡ç®—çš„åˆ†ä½æ•°åˆ†å¸ƒã€‚</td>\n",
       "      <td>Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ, Ñ€Ğ°ÑÑÑ‡Ğ¸Ñ‚Ğ°...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253</th>\n",
       "      <td>æ‹Ÿåˆæ¨¡å‹ã€‚\\n\\n        å‚æ•°\\n        -------------\\n  ...</td>\n",
       "      <td>ĞšĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.\\n\\nĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹\\n--------...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>å°†æ•°æ®é›†å‡å°‘åˆ°é€‰æ‹©çš„ç‰¹å¾ã€‚\\n\\n        å‚æ•°\\n        ---------...</td>\n",
       "      <td>Ğ¡Ğ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¾ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>è¿”å›é€‰æ‹©çš„ç‰¹å¾æ•°é‡ã€‚</td>\n",
       "      <td>Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>ç”Ÿæˆæ•°æ®é›†ã€‚\\n\\n    å‚æ•°\\n    -------------\\n    n_sam...</td>\n",
       "      <td>Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….\\n\\nĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹\\n---------...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>è¿è¡Œç¨‹åºã€‚</td>\n",
       "      <td>Ğ—Ğ°Ğ¿ÑƒÑĞºĞ°Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñƒ.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1258 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment_zh  \\\n",
       "0                                      ç”¨äºä¸¤ä¸ªæ ·æœ¬åˆ†ä½æ•°çš„è‡ªåŠ©æ³•tæ£€éªŒ   \n",
       "1                                      è®¡ç®—æ§åˆ¶ç»„å’Œå®éªŒç»„çš„è‡ªåŠ©æ³•åˆ†ä½æ•°   \n",
       "2                                          å¯¹è‡ªåŠ©æ³•åˆ†ä½æ•°è¿›è¡Œtæ£€éªŒ   \n",
       "3                                             åˆ¤æ–­æ˜¯å¦æ‹’ç»åŸå‡è®¾   \n",
       "4                                    è¿”å›ä½¿ç”¨è‡ªåŠ©æŠ½æ ·æ³•è®¡ç®—çš„åˆ†ä½æ•°åˆ†å¸ƒã€‚   \n",
       "...                                                 ...   \n",
       "1253  æ‹Ÿåˆæ¨¡å‹ã€‚\\n\\n        å‚æ•°\\n        -------------\\n  ...   \n",
       "1254  å°†æ•°æ®é›†å‡å°‘åˆ°é€‰æ‹©çš„ç‰¹å¾ã€‚\\n\\n        å‚æ•°\\n        ---------...   \n",
       "1255                                         è¿”å›é€‰æ‹©çš„ç‰¹å¾æ•°é‡ã€‚   \n",
       "1256  ç”Ÿæˆæ•°æ®é›†ã€‚\\n\\n    å‚æ•°\\n    -------------\\n    n_sam...   \n",
       "1257                                              è¿è¡Œç¨‹åºã€‚   \n",
       "\n",
       "                                             comment_ru  \n",
       "0     t-Ñ‚ĞµÑÑ‚ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑĞ»ÑƒĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ´Ğ²ÑƒÑ… ĞºĞ²Ğ°Ñ€Ñ‚Ğ¸Ğ»ĞµĞ¹ Ğ²Ñ‹Ğ±...  \n",
       "1     Ğ Ğ°ÑÑÑ‡Ğ¸Ñ‚Ğ°Ñ‚ÑŒ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑĞ»ÑƒĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»...  \n",
       "2          ĞŸÑ€Ğ¾Ğ²ĞµÑÑ‚Ğ¸ t-Ñ‚ĞµÑÑ‚ Ğ½Ğ° ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ»ÑŒ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑĞ»ÑƒĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ  \n",
       "3     ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½ÑÑ‚ÑŒ Ğ¸Ğ»Ğ¸ Ğ½ĞµÑ‚ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ½Ğ°Ñ‡Ğ°Ğ»...  \n",
       "4     Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ, Ñ€Ğ°ÑÑÑ‡Ğ¸Ñ‚Ğ°...  \n",
       "...                                                 ...  \n",
       "1253  ĞšĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.\\n\\nĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹\\n--------...  \n",
       "1254  Ğ¡Ğ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¾ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚...  \n",
       "1255     Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº.  \n",
       "1256  Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….\\n\\nĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹\\n---------...  \n",
       "1257                                Ğ—Ğ°Ğ¿ÑƒÑĞºĞ°Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñƒ.  \n",
       "\n",
       "[1258 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# åŠ è½½æ•°æ®é›†\n",
    "df = pd.read_csv(r\"C:\\Users\\gdnjr5233_YOLO\\Desktop\\Ğ’ĞšĞ _2025\\datasets\\data.csv\")  # æ›¿æ¢ä¸ºå®é™…è·¯å¾„\n",
    "df = df[[\"comment_zh\", \"comment_ru\"]].dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†\n",
    "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "\n",
    "# è½¬æ¢ä¸ºHuggingFaceæ•°æ®é›†æ ¼å¼\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# åŠ è½½æ¨¡å‹å’Œtokenizer\n",
    "model_name = \"utrobinmv/t5_translate_en_ru_zh_small_1024\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ac438646f864590bb50884a034cd763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/1132 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8efcf1655d1455f954cc6304c27d50f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1132 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45590dd01a0c41c3a7426f5102e4bf3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/126 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9d003d68ff04f7bbe16eb209eeb211e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/126 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# åº”ç”¨é¢„å¤„ç†\n",
    "train_dataset = train_dataset.map(\n",
    "    preprocess_data,\n",
    "    remove_columns=[\"comment_zh\", \"comment_ru\"],\n",
    "    num_proc=4\n",
    ").filter(lambda x: x is not None)\n",
    "\n",
    "val_dataset = val_dataset.map(\n",
    "    preprocess_data,\n",
    "    remove_columns=[\"comment_zh\", \"comment_ru\"],\n",
    "    num_proc=4\n",
    ").filter(lambda x: x is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•°æ®æ•´ç†å‡½æ•°ï¼ˆæ·»åŠ é”™è¯¯å¤„ç†ï¼‰\n",
    "def data_collator(features):\n",
    "    filtered_features = [f for f in features if f is not None]\n",
    "    if not filtered_features:\n",
    "        return None  # è·³è¿‡ç©ºbatch\n",
    "    \n",
    "    batch = tokenizer.pad(\n",
    "        filtered_features,\n",
    "        padding=\"longest\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # æ£€æŸ¥æ˜¯å¦åŒ…å«æ— æ•ˆçš„token id\n",
    "    if (batch[\"labels\"] >= tokenizer.vocab_size).any():\n",
    "        print(\"Detected invalid token IDs in labels, skipping batch\")\n",
    "        return None\n",
    "        \n",
    "    return {\n",
    "        \"input_ids\": batch[\"input_ids\"],\n",
    "        \"attention_mask\": batch[\"attention_mask\"],\n",
    "        \"labels\": batch[\"labels\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# è®­ç»ƒå‚æ•°é…ç½®\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./fine-tuned-model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=5,\n",
    "    predict_with_generate=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\",\n",
    "    gradient_accumulation_steps=2,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    dataloader_drop_last=True,  # é˜²æ­¢æœ€åä¸å®Œæ•´çš„batch\n",
    "    load_best_model_at_end=True,  # ä¿å­˜æœ€ä½³æ¨¡å‹\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76c3ba82e6834fbfb0da7b245fabd657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/705 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training interrupted by error: list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gdnjr5233_YOLO\\AppData\\Roaming\\Python\\Python310\\site-packages\\datasets\\formatting\\formatting.py:101: RuntimeWarning: divide by zero encountered in remainder\n",
      "  return table.fast_gather(key % table.num_rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial model saved at ./partial-model\n"
     ]
    }
   ],
   "source": [
    "# åˆå§‹åŒ–Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[ErrorHandlingCallback()]\n",
    ")\n",
    "\n",
    "# å¼€å§‹è®­ç»ƒ\n",
    "try:\n",
    "    trainer.train()\n",
    "except Exception as e:\n",
    "    print(f\"Training interrupted by error: {e}\")\n",
    "    # è‡ªåŠ¨ä¿å­˜æ£€æŸ¥ç‚¹\n",
    "    trainer.save_model(\"./partial-model\")\n",
    "    print(\"Partial model saved at ./partial-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜æœ€ç»ˆæ¨¡å‹\n",
    "trainer.save_model(\"./fine-tuned-model\")\n",
    "tokenizer.save_pretrained(\"./fine-tuned-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
